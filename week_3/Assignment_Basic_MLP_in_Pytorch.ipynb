{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Cwq-uPYvpa"
      },
      "source": [
        "# Build basic 2-Layer MLP to solve the xor-Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "K4FDsqgaYvps"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs #for data generatio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2x1wijYZYvpu"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=200, n_features=2, cluster_std=.1\n",
        "                  ,centers= [(1,1), (1,0), (0,0),(0,1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "aoAh4a4KYvpv"
      },
      "outputs": [],
      "source": [
        "#make blobs into binary problem\n",
        "y[y==2]=0\n",
        "y[y==3]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "RMPaCrKBYvpw",
        "outputId": "f8a5bef3-5877-42ed-d3b9-98bf5f278b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ed3747f5590>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.scatter(X[:,0],X[:,1],c=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q9Y4g0NYvpy"
      },
      "source": [
        "## Steps:\n",
        "* build train and test sets\n",
        "* write MLP class in Pytorch with two layers with adjustable number of perceptrons\n",
        "* use nn.linear and nn.Sigmoid() units\n",
        "* train your model\n",
        "* test your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n",
        "\n",
        "\n",
        "x_train = torch.FloatTensor(X_train)\n",
        "x_test = torch.FloatTensor(X_test)\n",
        "Y_train = torch.FloatTensor(y_train)\n",
        "Y_test = torch.FloatTensor(y_test)"
      ],
      "metadata": {
        "id": "rdx4UbnsMFof"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Model(torch.nn.Module): #all nets inherit from nn.Module\n",
        "    def __init__(self, dims): #define layer types\n",
        "        super(Model, self).__init__()\n",
        "        self.yes = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2,dims, bias=False),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(dims,1, bias=False),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "        #self.fc = torch.nn.Linear(2,1,bias=False) # Perceptron is single neuron \"fully connected\" (fc) -> linear unit with 2 inputs and 1 output\n",
        "        #self.non_linear = torch.nn.Sigmoid() #non-linear activation\n",
        "    def forward(self, x): #build network\n",
        "        output = self.yes(x) #w*X\n",
        "        #output = self.fc(output) # activation\n",
        "        return output"
      ],
      "metadata": {
        "id": "piXsTVTrZssQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Model(5)\n",
        "criterion1 = torch.nn.BCELoss()\n",
        "optimizer1 = torch.optim.SGD(model1.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "yiOe6mVjIJzA"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.eval() #set to eval mode\n",
        "model1(x_test)"
      ],
      "metadata": {
        "id": "g4VyT5XGxprC",
        "outputId": "de26c57e-961e-49a6-e2a1-3ca512583407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5568],\n",
              "        [0.5361],\n",
              "        [0.5568],\n",
              "        [0.5502],\n",
              "        [0.5492],\n",
              "        [0.5360],\n",
              "        [0.5436],\n",
              "        [0.5578],\n",
              "        [0.5510],\n",
              "        [0.5438],\n",
              "        [0.5472],\n",
              "        [0.5565],\n",
              "        [0.5463],\n",
              "        [0.5550],\n",
              "        [0.5453],\n",
              "        [0.5437],\n",
              "        [0.5551],\n",
              "        [0.5350],\n",
              "        [0.5488],\n",
              "        [0.5580],\n",
              "        [0.5389],\n",
              "        [0.5470],\n",
              "        [0.5580],\n",
              "        [0.5462],\n",
              "        [0.5372],\n",
              "        [0.5575],\n",
              "        [0.5569],\n",
              "        [0.5590],\n",
              "        [0.5581],\n",
              "        [0.5371],\n",
              "        [0.5441],\n",
              "        [0.5496],\n",
              "        [0.5573],\n",
              "        [0.5495],\n",
              "        [0.5381],\n",
              "        [0.5400],\n",
              "        [0.5589],\n",
              "        [0.5453],\n",
              "        [0.5437],\n",
              "        [0.5472],\n",
              "        [0.5365],\n",
              "        [0.5418],\n",
              "        [0.5459],\n",
              "        [0.5480],\n",
              "        [0.5378],\n",
              "        [0.5501],\n",
              "        [0.5469],\n",
              "        [0.5590],\n",
              "        [0.5488],\n",
              "        [0.5471],\n",
              "        [0.5373],\n",
              "        [0.5369],\n",
              "        [0.5487],\n",
              "        [0.5375],\n",
              "        [0.5421],\n",
              "        [0.5513],\n",
              "        [0.5479],\n",
              "        [0.5587],\n",
              "        [0.5450],\n",
              "        [0.5448]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.train() #set to train mode\n",
        "epoch = 90000\n",
        "for epoch in range(epoch):\n",
        "    optimizer1.zero_grad()\n",
        "    # Forward pass\n",
        "    y_pred = model1(x_train)\n",
        "    # Compute Loss\n",
        "    loss = criterion1(y_pred.squeeze(), Y_train)\n",
        "\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    #make gradient update\n",
        "    optimizer1.step()"
      ],
      "metadata": {
        "id": "RnYQYosO3naa",
        "outputId": "336e50ce-f143-4926-a56e-c785469ac638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 85000: train loss: 0.06894513219594955\n",
            "Epoch 85001: train loss: 0.06894288957118988\n",
            "Epoch 85002: train loss: 0.0689406767487526\n",
            "Epoch 85003: train loss: 0.06893844157457352\n",
            "Epoch 85004: train loss: 0.06893621385097504\n",
            "Epoch 85005: train loss: 0.06893400102853775\n",
            "Epoch 85006: train loss: 0.06893177330493927\n",
            "Epoch 85007: train loss: 0.06892955303192139\n",
            "Epoch 85008: train loss: 0.0689273253083229\n",
            "Epoch 85009: train loss: 0.06892510503530502\n",
            "Epoch 85010: train loss: 0.06892287731170654\n",
            "Epoch 85011: train loss: 0.06892064958810806\n",
            "Epoch 85012: train loss: 0.06891842186450958\n",
            "Epoch 85013: train loss: 0.0689162015914917\n",
            "Epoch 85014: train loss: 0.06891398131847382\n",
            "Epoch 85015: train loss: 0.06891175359487534\n",
            "Epoch 85016: train loss: 0.06890954077243805\n",
            "Epoch 85017: train loss: 0.06890730559825897\n",
            "Epoch 85018: train loss: 0.06890508532524109\n",
            "Epoch 85019: train loss: 0.0689028725028038\n",
            "Epoch 85020: train loss: 0.06890063732862473\n",
            "Epoch 85021: train loss: 0.06889840960502625\n",
            "Epoch 85022: train loss: 0.06889619678258896\n",
            "Epoch 85023: train loss: 0.06889397650957108\n",
            "Epoch 85024: train loss: 0.068891741335392\n",
            "Epoch 85025: train loss: 0.06888952851295471\n",
            "Epoch 85026: train loss: 0.06888729333877563\n",
            "Epoch 85027: train loss: 0.06888508051633835\n",
            "Epoch 85028: train loss: 0.06888287514448166\n",
            "Epoch 85029: train loss: 0.06888063997030258\n",
            "Epoch 85030: train loss: 0.0688784196972847\n",
            "Epoch 85031: train loss: 0.06887619197368622\n",
            "Epoch 85032: train loss: 0.06887396425008774\n",
            "Epoch 85033: train loss: 0.06887175142765045\n",
            "Epoch 85034: train loss: 0.06886953860521317\n",
            "Epoch 85035: train loss: 0.06886730343103409\n",
            "Epoch 85036: train loss: 0.0688650980591774\n",
            "Epoch 85037: train loss: 0.06886287033557892\n",
            "Epoch 85038: train loss: 0.06886065006256104\n",
            "Epoch 85039: train loss: 0.06885842978954315\n",
            "Epoch 85040: train loss: 0.06885620951652527\n",
            "Epoch 85041: train loss: 0.06885398179292679\n",
            "Epoch 85042: train loss: 0.0688517838716507\n",
            "Epoch 85043: train loss: 0.06884955614805222\n",
            "Epoch 85044: train loss: 0.06884735077619553\n",
            "Epoch 85045: train loss: 0.06884513050317764\n",
            "Epoch 85046: train loss: 0.06884290277957916\n",
            "Epoch 85047: train loss: 0.06884069740772247\n",
            "Epoch 85048: train loss: 0.0688384622335434\n",
            "Epoch 85049: train loss: 0.06883624941110611\n",
            "Epoch 85050: train loss: 0.06883404403924942\n",
            "Epoch 85051: train loss: 0.06883182376623154\n",
            "Epoch 85052: train loss: 0.06882960349321365\n",
            "Epoch 85053: train loss: 0.06882739067077637\n",
            "Epoch 85054: train loss: 0.06882517784833908\n",
            "Epoch 85055: train loss: 0.0688229650259018\n",
            "Epoch 85056: train loss: 0.06882074475288391\n",
            "Epoch 85057: train loss: 0.06881853193044662\n",
            "Epoch 85058: train loss: 0.06881631910800934\n",
            "Epoch 85059: train loss: 0.06881409883499146\n",
            "Epoch 85060: train loss: 0.06881189346313477\n",
            "Epoch 85061: train loss: 0.06880967319011688\n",
            "Epoch 85062: train loss: 0.06880746781826019\n",
            "Epoch 85063: train loss: 0.0688052549958229\n",
            "Epoch 85064: train loss: 0.06880303472280502\n",
            "Epoch 85065: train loss: 0.06880081444978714\n",
            "Epoch 85066: train loss: 0.06879859417676926\n",
            "Epoch 85067: train loss: 0.06879638135433197\n",
            "Epoch 85068: train loss: 0.06879416853189468\n",
            "Epoch 85069: train loss: 0.0687919557094574\n",
            "Epoch 85070: train loss: 0.06878974288702011\n",
            "Epoch 85071: train loss: 0.06878753751516342\n",
            "Epoch 85072: train loss: 0.06878531724214554\n",
            "Epoch 85073: train loss: 0.06878310441970825\n",
            "Epoch 85074: train loss: 0.06878090649843216\n",
            "Epoch 85075: train loss: 0.06877870112657547\n",
            "Epoch 85076: train loss: 0.06877648830413818\n",
            "Epoch 85077: train loss: 0.0687742754817009\n",
            "Epoch 85078: train loss: 0.06877206265926361\n",
            "Epoch 85079: train loss: 0.06876986473798752\n",
            "Epoch 85080: train loss: 0.06876764446496964\n",
            "Epoch 85081: train loss: 0.06876543909311295\n",
            "Epoch 85082: train loss: 0.06876321882009506\n",
            "Epoch 85083: train loss: 0.06876102089881897\n",
            "Epoch 85084: train loss: 0.06875881552696228\n",
            "Epoch 85085: train loss: 0.06875661015510559\n",
            "Epoch 85086: train loss: 0.0687543973326683\n",
            "Epoch 85087: train loss: 0.06875219196081161\n",
            "Epoch 85088: train loss: 0.06874998658895493\n",
            "Epoch 85089: train loss: 0.06874778866767883\n",
            "Epoch 85090: train loss: 0.06874557584524155\n",
            "Epoch 85091: train loss: 0.06874337047338486\n",
            "Epoch 85092: train loss: 0.06874117255210876\n",
            "Epoch 85093: train loss: 0.06873895227909088\n",
            "Epoch 85094: train loss: 0.06873675435781479\n",
            "Epoch 85095: train loss: 0.0687345564365387\n",
            "Epoch 85096: train loss: 0.06873234361410141\n",
            "Epoch 85097: train loss: 0.06873015314340591\n",
            "Epoch 85098: train loss: 0.06872794032096863\n",
            "Epoch 85099: train loss: 0.06872573494911194\n",
            "Epoch 85100: train loss: 0.06872352957725525\n",
            "Epoch 85101: train loss: 0.06872133165597916\n",
            "Epoch 85102: train loss: 0.06871912628412247\n",
            "Epoch 85103: train loss: 0.06871693581342697\n",
            "Epoch 85104: train loss: 0.06871472299098969\n",
            "Epoch 85105: train loss: 0.0687125101685524\n",
            "Epoch 85106: train loss: 0.06871030479669571\n",
            "Epoch 85107: train loss: 0.06870809942483902\n",
            "Epoch 85108: train loss: 0.06870590150356293\n",
            "Epoch 85109: train loss: 0.06870369613170624\n",
            "Epoch 85110: train loss: 0.06870149075984955\n",
            "Epoch 85111: train loss: 0.06869929283857346\n",
            "Epoch 85112: train loss: 0.06869709491729736\n",
            "Epoch 85113: train loss: 0.06869488209486008\n",
            "Epoch 85114: train loss: 0.06869268417358398\n",
            "Epoch 85115: train loss: 0.0686904788017273\n",
            "Epoch 85116: train loss: 0.0686882734298706\n",
            "Epoch 85117: train loss: 0.06868607550859451\n",
            "Epoch 85118: train loss: 0.06868387013673782\n",
            "Epoch 85119: train loss: 0.06868166476488113\n",
            "Epoch 85120: train loss: 0.06867946684360504\n",
            "Epoch 85121: train loss: 0.06867726147174835\n",
            "Epoch 85122: train loss: 0.06867506355047226\n",
            "Epoch 85123: train loss: 0.06867285817861557\n",
            "Epoch 85124: train loss: 0.06867065280675888\n",
            "Epoch 85125: train loss: 0.06866846233606339\n",
            "Epoch 85126: train loss: 0.0686662569642067\n",
            "Epoch 85127: train loss: 0.0686640590429306\n",
            "Epoch 85128: train loss: 0.06866185367107391\n",
            "Epoch 85129: train loss: 0.06865966320037842\n",
            "Epoch 85130: train loss: 0.06865745037794113\n",
            "Epoch 85131: train loss: 0.06865526735782623\n",
            "Epoch 85132: train loss: 0.06865306198596954\n",
            "Epoch 85133: train loss: 0.06865085661411285\n",
            "Epoch 85134: train loss: 0.06864865869283676\n",
            "Epoch 85135: train loss: 0.06864646822214127\n",
            "Epoch 85136: train loss: 0.06864425539970398\n",
            "Epoch 85137: train loss: 0.06864205747842789\n",
            "Epoch 85138: train loss: 0.06863986700773239\n",
            "Epoch 85139: train loss: 0.0686376690864563\n",
            "Epoch 85140: train loss: 0.06863545626401901\n",
            "Epoch 85141: train loss: 0.06863326579332352\n",
            "Epoch 85142: train loss: 0.06863106042146683\n",
            "Epoch 85143: train loss: 0.06862886995077133\n",
            "Epoch 85144: train loss: 0.06862666457891464\n",
            "Epoch 85145: train loss: 0.06862445920705795\n",
            "Epoch 85146: train loss: 0.06862227618694305\n",
            "Epoch 85147: train loss: 0.06862007081508636\n",
            "Epoch 85148: train loss: 0.06861786544322968\n",
            "Epoch 85149: train loss: 0.06861568242311478\n",
            "Epoch 85150: train loss: 0.06861348450183868\n",
            "Epoch 85151: train loss: 0.06861128658056259\n",
            "Epoch 85152: train loss: 0.0686090886592865\n",
            "Epoch 85153: train loss: 0.0686068907380104\n",
            "Epoch 85154: train loss: 0.06860469281673431\n",
            "Epoch 85155: train loss: 0.06860250234603882\n",
            "Epoch 85156: train loss: 0.06860030442476273\n",
            "Epoch 85157: train loss: 0.06859811395406723\n",
            "Epoch 85158: train loss: 0.06859590858221054\n",
            "Epoch 85159: train loss: 0.06859371066093445\n",
            "Epoch 85160: train loss: 0.06859152019023895\n",
            "Epoch 85161: train loss: 0.06858932971954346\n",
            "Epoch 85162: train loss: 0.06858712434768677\n",
            "Epoch 85163: train loss: 0.06858493387699127\n",
            "Epoch 85164: train loss: 0.06858273595571518\n",
            "Epoch 85165: train loss: 0.06858054548501968\n",
            "Epoch 85166: train loss: 0.06857834756374359\n",
            "Epoch 85167: train loss: 0.0685761570930481\n",
            "Epoch 85168: train loss: 0.06857394427061081\n",
            "Epoch 85169: train loss: 0.06857176870107651\n",
            "Epoch 85170: train loss: 0.06856957077980042\n",
            "Epoch 85171: train loss: 0.06856736540794373\n",
            "Epoch 85172: train loss: 0.06856517493724823\n",
            "Epoch 85173: train loss: 0.06856298446655273\n",
            "Epoch 85174: train loss: 0.06856078654527664\n",
            "Epoch 85175: train loss: 0.06855858117341995\n",
            "Epoch 85176: train loss: 0.06855639070272446\n",
            "Epoch 85177: train loss: 0.06855420023202896\n",
            "Epoch 85178: train loss: 0.06855201721191406\n",
            "Epoch 85179: train loss: 0.06854981184005737\n",
            "Epoch 85180: train loss: 0.06854761391878128\n",
            "Epoch 85181: train loss: 0.06854542344808578\n",
            "Epoch 85182: train loss: 0.06854323297739029\n",
            "Epoch 85183: train loss: 0.0685410276055336\n",
            "Epoch 85184: train loss: 0.0685388371348381\n",
            "Epoch 85185: train loss: 0.06853664666414261\n",
            "Epoch 85186: train loss: 0.06853446364402771\n",
            "Epoch 85187: train loss: 0.06853226572275162\n",
            "Epoch 85188: train loss: 0.06853006780147552\n",
            "Epoch 85189: train loss: 0.06852787733078003\n",
            "Epoch 85190: train loss: 0.06852567940950394\n",
            "Epoch 85191: train loss: 0.06852350383996964\n",
            "Epoch 85192: train loss: 0.06852130591869354\n",
            "Epoch 85193: train loss: 0.06851910054683685\n",
            "Epoch 85194: train loss: 0.06851692497730255\n",
            "Epoch 85195: train loss: 0.06851471215486526\n",
            "Epoch 85196: train loss: 0.06851253658533096\n",
            "Epoch 85197: train loss: 0.06851033866405487\n",
            "Epoch 85198: train loss: 0.06850814819335938\n",
            "Epoch 85199: train loss: 0.06850596517324448\n",
            "Epoch 85200: train loss: 0.06850376725196838\n",
            "Epoch 85201: train loss: 0.06850158423185349\n",
            "Epoch 85202: train loss: 0.0684993788599968\n",
            "Epoch 85203: train loss: 0.0684971958398819\n",
            "Epoch 85204: train loss: 0.0684949979186058\n",
            "Epoch 85205: train loss: 0.0684928148984909\n",
            "Epoch 85206: train loss: 0.06849061697721481\n",
            "Epoch 85207: train loss: 0.06848841905593872\n",
            "Epoch 85208: train loss: 0.06848622858524323\n",
            "Epoch 85209: train loss: 0.06848403066396713\n",
            "Epoch 85210: train loss: 0.06848185509443283\n",
            "Epoch 85211: train loss: 0.06847965717315674\n",
            "Epoch 85212: train loss: 0.06847746670246124\n",
            "Epoch 85213: train loss: 0.06847527623176575\n",
            "Epoch 85214: train loss: 0.06847309321165085\n",
            "Epoch 85215: train loss: 0.06847088783979416\n",
            "Epoch 85216: train loss: 0.06846871227025986\n",
            "Epoch 85217: train loss: 0.06846652179956436\n",
            "Epoch 85218: train loss: 0.06846432387828827\n",
            "Epoch 85219: train loss: 0.06846214085817337\n",
            "Epoch 85220: train loss: 0.06845995038747787\n",
            "Epoch 85221: train loss: 0.06845775246620178\n",
            "Epoch 85222: train loss: 0.06845557689666748\n",
            "Epoch 85223: train loss: 0.06845337897539139\n",
            "Epoch 85224: train loss: 0.06845119595527649\n",
            "Epoch 85225: train loss: 0.0684489980340004\n",
            "Epoch 85226: train loss: 0.0684468150138855\n",
            "Epoch 85227: train loss: 0.0684446170926094\n",
            "Epoch 85228: train loss: 0.0684424340724945\n",
            "Epoch 85229: train loss: 0.06844023615121841\n",
            "Epoch 85230: train loss: 0.06843805313110352\n",
            "Epoch 85231: train loss: 0.06843586266040802\n",
            "Epoch 85232: train loss: 0.06843365728855133\n",
            "Epoch 85233: train loss: 0.06843148916959763\n",
            "Epoch 85234: train loss: 0.06842929124832153\n",
            "Epoch 85235: train loss: 0.06842710822820663\n",
            "Epoch 85236: train loss: 0.06842491030693054\n",
            "Epoch 85237: train loss: 0.06842272728681564\n",
            "Epoch 85238: train loss: 0.06842053681612015\n",
            "Epoch 85239: train loss: 0.06841833889484406\n",
            "Epoch 85240: train loss: 0.06841615587472916\n",
            "Epoch 85241: train loss: 0.06841398030519485\n",
            "Epoch 85242: train loss: 0.06841178983449936\n",
            "Epoch 85243: train loss: 0.06840959936380386\n",
            "Epoch 85244: train loss: 0.06840740889310837\n",
            "Epoch 85245: train loss: 0.06840521842241287\n",
            "Epoch 85246: train loss: 0.06840303540229797\n",
            "Epoch 85247: train loss: 0.06840083748102188\n",
            "Epoch 85248: train loss: 0.06839866191148758\n",
            "Epoch 85249: train loss: 0.06839647144079208\n",
            "Epoch 85250: train loss: 0.06839428097009659\n",
            "Epoch 85251: train loss: 0.0683920830488205\n",
            "Epoch 85252: train loss: 0.0683899000287056\n",
            "Epoch 85253: train loss: 0.0683877095580101\n",
            "Epoch 85254: train loss: 0.0683855339884758\n",
            "Epoch 85255: train loss: 0.06838333606719971\n",
            "Epoch 85256: train loss: 0.06838115304708481\n",
            "Epoch 85257: train loss: 0.06837896257638931\n",
            "Epoch 85258: train loss: 0.06837678700685501\n",
            "Epoch 85259: train loss: 0.06837459653615952\n",
            "Epoch 85260: train loss: 0.06837241351604462\n",
            "Epoch 85261: train loss: 0.06837023049592972\n",
            "Epoch 85262: train loss: 0.06836805492639542\n",
            "Epoch 85263: train loss: 0.06836586445569992\n",
            "Epoch 85264: train loss: 0.06836368143558502\n",
            "Epoch 85265: train loss: 0.06836150586605072\n",
            "Epoch 85266: train loss: 0.06835932284593582\n",
            "Epoch 85267: train loss: 0.06835713982582092\n",
            "Epoch 85268: train loss: 0.06835496425628662\n",
            "Epoch 85269: train loss: 0.06835277378559113\n",
            "Epoch 85270: train loss: 0.06835059076547623\n",
            "Epoch 85271: train loss: 0.06834842264652252\n",
            "Epoch 85272: train loss: 0.06834623217582703\n",
            "Epoch 85273: train loss: 0.06834405660629272\n",
            "Epoch 85274: train loss: 0.06834186613559723\n",
            "Epoch 85275: train loss: 0.06833968311548233\n",
            "Epoch 85276: train loss: 0.06833750009536743\n",
            "Epoch 85277: train loss: 0.06833532452583313\n",
            "Epoch 85278: train loss: 0.06833312660455704\n",
            "Epoch 85279: train loss: 0.06833096593618393\n",
            "Epoch 85280: train loss: 0.06832877546548843\n",
            "Epoch 85281: train loss: 0.06832659244537354\n",
            "Epoch 85282: train loss: 0.06832441687583923\n",
            "Epoch 85283: train loss: 0.06832223385572433\n",
            "Epoch 85284: train loss: 0.06832005828619003\n",
            "Epoch 85285: train loss: 0.06831786781549454\n",
            "Epoch 85286: train loss: 0.06831568479537964\n",
            "Epoch 85287: train loss: 0.06831350922584534\n",
            "Epoch 85288: train loss: 0.06831132620573044\n",
            "Epoch 85289: train loss: 0.06830915063619614\n",
            "Epoch 85290: train loss: 0.06830696761608124\n",
            "Epoch 85291: train loss: 0.06830479204654694\n",
            "Epoch 85292: train loss: 0.06830260157585144\n",
            "Epoch 85293: train loss: 0.06830043345689774\n",
            "Epoch 85294: train loss: 0.06829824298620224\n",
            "Epoch 85295: train loss: 0.06829606741666794\n",
            "Epoch 85296: train loss: 0.06829388439655304\n",
            "Epoch 85297: train loss: 0.06829170137643814\n",
            "Epoch 85298: train loss: 0.06828951835632324\n",
            "Epoch 85299: train loss: 0.06828735023736954\n",
            "Epoch 85300: train loss: 0.06828516721725464\n",
            "Epoch 85301: train loss: 0.06828298419713974\n",
            "Epoch 85302: train loss: 0.06828080862760544\n",
            "Epoch 85303: train loss: 0.06827861815690994\n",
            "Epoch 85304: train loss: 0.06827644258737564\n",
            "Epoch 85305: train loss: 0.06827425211668015\n",
            "Epoch 85306: train loss: 0.06827207654714584\n",
            "Epoch 85307: train loss: 0.06826991587877274\n",
            "Epoch 85308: train loss: 0.06826771795749664\n",
            "Epoch 85309: train loss: 0.06826554238796234\n",
            "Epoch 85310: train loss: 0.06826336681842804\n",
            "Epoch 85311: train loss: 0.06826118379831314\n",
            "Epoch 85312: train loss: 0.06825900822877884\n",
            "Epoch 85313: train loss: 0.06825682520866394\n",
            "Epoch 85314: train loss: 0.06825465708971024\n",
            "Epoch 85315: train loss: 0.06825246661901474\n",
            "Epoch 85316: train loss: 0.06825029104948044\n",
            "Epoch 85317: train loss: 0.06824812293052673\n",
            "Epoch 85318: train loss: 0.06824593245983124\n",
            "Epoch 85319: train loss: 0.06824375689029694\n",
            "Epoch 85320: train loss: 0.06824158132076263\n",
            "Epoch 85321: train loss: 0.06823939830064774\n",
            "Epoch 85322: train loss: 0.06823722273111343\n",
            "Epoch 85323: train loss: 0.06823503971099854\n",
            "Epoch 85324: train loss: 0.06823287159204483\n",
            "Epoch 85325: train loss: 0.06823068112134933\n",
            "Epoch 85326: train loss: 0.06822852045297623\n",
            "Epoch 85327: train loss: 0.06822633743286133\n",
            "Epoch 85328: train loss: 0.06822415441274643\n",
            "Epoch 85329: train loss: 0.06822199374437332\n",
            "Epoch 85330: train loss: 0.06821978837251663\n",
            "Epoch 85331: train loss: 0.06821762770414352\n",
            "Epoch 85332: train loss: 0.06821545213460922\n",
            "Epoch 85333: train loss: 0.06821326911449432\n",
            "Epoch 85334: train loss: 0.06821109354496002\n",
            "Epoch 85335: train loss: 0.06820891797542572\n",
            "Epoch 85336: train loss: 0.06820674985647202\n",
            "Epoch 85337: train loss: 0.06820456683635712\n",
            "Epoch 85338: train loss: 0.06820239871740341\n",
            "Epoch 85339: train loss: 0.06820022314786911\n",
            "Epoch 85340: train loss: 0.06819803267717361\n",
            "Epoch 85341: train loss: 0.06819585710763931\n",
            "Epoch 85342: train loss: 0.06819368153810501\n",
            "Epoch 85343: train loss: 0.0681915134191513\n",
            "Epoch 85344: train loss: 0.06818933039903641\n",
            "Epoch 85345: train loss: 0.06818713992834091\n",
            "Epoch 85346: train loss: 0.06818497180938721\n",
            "Epoch 85347: train loss: 0.06818278878927231\n",
            "Epoch 85348: train loss: 0.0681806281208992\n",
            "Epoch 85349: train loss: 0.0681784525513649\n",
            "Epoch 85350: train loss: 0.0681762620806694\n",
            "Epoch 85351: train loss: 0.0681740865111351\n",
            "Epoch 85352: train loss: 0.0681719183921814\n",
            "Epoch 85353: train loss: 0.0681697428226471\n",
            "Epoch 85354: train loss: 0.06816756725311279\n",
            "Epoch 85355: train loss: 0.06816539913415909\n",
            "Epoch 85356: train loss: 0.06816320866346359\n",
            "Epoch 85357: train loss: 0.06816104799509048\n",
            "Epoch 85358: train loss: 0.06815885752439499\n",
            "Epoch 85359: train loss: 0.06815668195486069\n",
            "Epoch 85360: train loss: 0.06815452128648758\n",
            "Epoch 85361: train loss: 0.06815233826637268\n",
            "Epoch 85362: train loss: 0.06815015524625778\n",
            "Epoch 85363: train loss: 0.06814798712730408\n",
            "Epoch 85364: train loss: 0.06814581900835037\n",
            "Epoch 85365: train loss: 0.06814364343881607\n",
            "Epoch 85366: train loss: 0.06814146786928177\n",
            "Epoch 85367: train loss: 0.06813928484916687\n",
            "Epoch 85368: train loss: 0.06813712418079376\n",
            "Epoch 85369: train loss: 0.06813494861125946\n",
            "Epoch 85370: train loss: 0.06813277304172516\n",
            "Epoch 85371: train loss: 0.06813060492277145\n",
            "Epoch 85372: train loss: 0.06812842190265656\n",
            "Epoch 85373: train loss: 0.06812626123428345\n",
            "Epoch 85374: train loss: 0.06812409311532974\n",
            "Epoch 85375: train loss: 0.06812191009521484\n",
            "Epoch 85376: train loss: 0.06811973452568054\n",
            "Epoch 85377: train loss: 0.06811757385730743\n",
            "Epoch 85378: train loss: 0.06811541318893433\n",
            "Epoch 85379: train loss: 0.06811323016881943\n",
            "Epoch 85380: train loss: 0.06811106204986572\n",
            "Epoch 85381: train loss: 0.06810890138149261\n",
            "Epoch 85382: train loss: 0.06810672581195831\n",
            "Epoch 85383: train loss: 0.06810455024242401\n",
            "Epoch 85384: train loss: 0.0681023821234703\n",
            "Epoch 85385: train loss: 0.0681002140045166\n",
            "Epoch 85386: train loss: 0.0680980384349823\n",
            "Epoch 85387: train loss: 0.06809587776660919\n",
            "Epoch 85388: train loss: 0.06809370964765549\n",
            "Epoch 85389: train loss: 0.06809154897928238\n",
            "Epoch 85390: train loss: 0.06808938831090927\n",
            "Epoch 85391: train loss: 0.06808722019195557\n",
            "Epoch 85392: train loss: 0.06808505952358246\n",
            "Epoch 85393: train loss: 0.06808289140462875\n",
            "Epoch 85394: train loss: 0.06808070838451385\n",
            "Epoch 85395: train loss: 0.06807854771614075\n",
            "Epoch 85396: train loss: 0.06807639449834824\n",
            "Epoch 85397: train loss: 0.06807421892881393\n",
            "Epoch 85398: train loss: 0.06807205080986023\n",
            "Epoch 85399: train loss: 0.06806989759206772\n",
            "Epoch 85400: train loss: 0.06806772947311401\n",
            "Epoch 85401: train loss: 0.06806556135416031\n",
            "Epoch 85402: train loss: 0.0680633932352066\n",
            "Epoch 85403: train loss: 0.0680612325668335\n",
            "Epoch 85404: train loss: 0.06805906444787979\n",
            "Epoch 85405: train loss: 0.06805689632892609\n",
            "Epoch 85406: train loss: 0.06805472820997238\n",
            "Epoch 85407: train loss: 0.06805257499217987\n",
            "Epoch 85408: train loss: 0.06805041432380676\n",
            "Epoch 85409: train loss: 0.06804826110601425\n",
            "Epoch 85410: train loss: 0.06804607808589935\n",
            "Epoch 85411: train loss: 0.06804391741752625\n",
            "Epoch 85412: train loss: 0.06804175674915314\n",
            "Epoch 85413: train loss: 0.06803958117961884\n",
            "Epoch 85414: train loss: 0.06803742051124573\n",
            "Epoch 85415: train loss: 0.06803526729345322\n",
            "Epoch 85416: train loss: 0.06803309917449951\n",
            "Epoch 85417: train loss: 0.0680309385061264\n",
            "Epoch 85418: train loss: 0.0680287703871727\n",
            "Epoch 85419: train loss: 0.06802660971879959\n",
            "Epoch 85420: train loss: 0.06802444905042648\n",
            "Epoch 85421: train loss: 0.06802228838205338\n",
            "Epoch 85422: train loss: 0.06802012771368027\n",
            "Epoch 85423: train loss: 0.06801796704530716\n",
            "Epoch 85424: train loss: 0.06801580637693405\n",
            "Epoch 85425: train loss: 0.06801363825798035\n",
            "Epoch 85426: train loss: 0.06801147013902664\n",
            "Epoch 85427: train loss: 0.06800931692123413\n",
            "Epoch 85428: train loss: 0.06800714135169983\n",
            "Epoch 85429: train loss: 0.06800497323274612\n",
            "Epoch 85430: train loss: 0.06800282001495361\n",
            "Epoch 85431: train loss: 0.0680006593465805\n",
            "Epoch 85432: train loss: 0.0679984912276268\n",
            "Epoch 85433: train loss: 0.06799633055925369\n",
            "Epoch 85434: train loss: 0.06799417734146118\n",
            "Epoch 85435: train loss: 0.06799200922250748\n",
            "Epoch 85436: train loss: 0.06798984110355377\n",
            "Epoch 85437: train loss: 0.06798768788576126\n",
            "Epoch 85438: train loss: 0.06798552721738815\n",
            "Epoch 85439: train loss: 0.06798335909843445\n",
            "Epoch 85440: train loss: 0.06798120588064194\n",
            "Epoch 85441: train loss: 0.06797903776168823\n",
            "Epoch 85442: train loss: 0.06797687709331512\n",
            "Epoch 85443: train loss: 0.06797473132610321\n",
            "Epoch 85444: train loss: 0.0679725706577301\n",
            "Epoch 85445: train loss: 0.0679704025387764\n",
            "Epoch 85446: train loss: 0.06796824932098389\n",
            "Epoch 85447: train loss: 0.06796609610319138\n",
            "Epoch 85448: train loss: 0.06796392798423767\n",
            "Epoch 85449: train loss: 0.06796177476644516\n",
            "Epoch 85450: train loss: 0.06795961409807205\n",
            "Epoch 85451: train loss: 0.06795746833086014\n",
            "Epoch 85452: train loss: 0.06795531511306763\n",
            "Epoch 85453: train loss: 0.06795315444469452\n",
            "Epoch 85454: train loss: 0.06795100122690201\n",
            "Epoch 85455: train loss: 0.0679488480091095\n",
            "Epoch 85456: train loss: 0.06794669479131699\n",
            "Epoch 85457: train loss: 0.06794453412294388\n",
            "Epoch 85458: train loss: 0.06794238090515137\n",
            "Epoch 85459: train loss: 0.06794021278619766\n",
            "Epoch 85460: train loss: 0.06793805956840515\n",
            "Epoch 85461: train loss: 0.06793591380119324\n",
            "Epoch 85462: train loss: 0.06793376058340073\n",
            "Epoch 85463: train loss: 0.06793159246444702\n",
            "Epoch 85464: train loss: 0.06792944669723511\n",
            "Epoch 85465: train loss: 0.0679272785782814\n",
            "Epoch 85466: train loss: 0.06792513281106949\n",
            "Epoch 85467: train loss: 0.06792298704385757\n",
            "Epoch 85468: train loss: 0.06792081892490387\n",
            "Epoch 85469: train loss: 0.06791866570711136\n",
            "Epoch 85470: train loss: 0.06791651248931885\n",
            "Epoch 85471: train loss: 0.06791435927152634\n",
            "Epoch 85472: train loss: 0.06791219860315323\n",
            "Epoch 85473: train loss: 0.06791005283594131\n",
            "Epoch 85474: train loss: 0.0679078921675682\n",
            "Epoch 85475: train loss: 0.06790574640035629\n",
            "Epoch 85476: train loss: 0.06790357828140259\n",
            "Epoch 85477: train loss: 0.06790144741535187\n",
            "Epoch 85478: train loss: 0.06789927929639816\n",
            "Epoch 85479: train loss: 0.06789711862802505\n",
            "Epoch 85480: train loss: 0.06789497286081314\n",
            "Epoch 85481: train loss: 0.06789282709360123\n",
            "Epoch 85482: train loss: 0.06789067387580872\n",
            "Epoch 85483: train loss: 0.0678885206580162\n",
            "Epoch 85484: train loss: 0.0678863674402237\n",
            "Epoch 85485: train loss: 0.06788421422243118\n",
            "Epoch 85486: train loss: 0.06788206100463867\n",
            "Epoch 85487: train loss: 0.06787990778684616\n",
            "Epoch 85488: train loss: 0.06787775456905365\n",
            "Epoch 85489: train loss: 0.06787560880184174\n",
            "Epoch 85490: train loss: 0.06787346303462982\n",
            "Epoch 85491: train loss: 0.06787129491567612\n",
            "Epoch 85492: train loss: 0.0678691416978836\n",
            "Epoch 85493: train loss: 0.06786700338125229\n",
            "Epoch 85494: train loss: 0.06786485761404037\n",
            "Epoch 85495: train loss: 0.06786270439624786\n",
            "Epoch 85496: train loss: 0.06786054372787476\n",
            "Epoch 85497: train loss: 0.06785840541124344\n",
            "Epoch 85498: train loss: 0.06785624474287033\n",
            "Epoch 85499: train loss: 0.06785409897565842\n",
            "Epoch 85500: train loss: 0.0678519457578659\n",
            "Epoch 85501: train loss: 0.06784979999065399\n",
            "Epoch 85502: train loss: 0.06784764677286148\n",
            "Epoch 85503: train loss: 0.06784549355506897\n",
            "Epoch 85504: train loss: 0.06784334033727646\n",
            "Epoch 85505: train loss: 0.06784117966890335\n",
            "Epoch 85506: train loss: 0.06783904135227203\n",
            "Epoch 85507: train loss: 0.06783689558506012\n",
            "Epoch 85508: train loss: 0.0678347498178482\n",
            "Epoch 85509: train loss: 0.06783260405063629\n",
            "Epoch 85510: train loss: 0.06783045083284378\n",
            "Epoch 85511: train loss: 0.06782829761505127\n",
            "Epoch 85512: train loss: 0.06782616674900055\n",
            "Epoch 85513: train loss: 0.06782400608062744\n",
            "Epoch 85514: train loss: 0.06782186776399612\n",
            "Epoch 85515: train loss: 0.06781971454620361\n",
            "Epoch 85516: train loss: 0.0678175687789917\n",
            "Epoch 85517: train loss: 0.06781542301177979\n",
            "Epoch 85518: train loss: 0.06781327724456787\n",
            "Epoch 85519: train loss: 0.06781113147735596\n",
            "Epoch 85520: train loss: 0.06780898571014404\n",
            "Epoch 85521: train loss: 0.06780683249235153\n",
            "Epoch 85522: train loss: 0.06780468672513962\n",
            "Epoch 85523: train loss: 0.0678025409579277\n",
            "Epoch 85524: train loss: 0.06780039519071579\n",
            "Epoch 85525: train loss: 0.06779824942350388\n",
            "Epoch 85526: train loss: 0.06779611110687256\n",
            "Epoch 85527: train loss: 0.06779394298791885\n",
            "Epoch 85528: train loss: 0.06779180467128754\n",
            "Epoch 85529: train loss: 0.06778966635465622\n",
            "Epoch 85530: train loss: 0.06778751313686371\n",
            "Epoch 85531: train loss: 0.0677853599190712\n",
            "Epoch 85532: train loss: 0.06778322160243988\n",
            "Epoch 85533: train loss: 0.06778106838464737\n",
            "Epoch 85534: train loss: 0.06777893006801605\n",
            "Epoch 85535: train loss: 0.06777677685022354\n",
            "Epoch 85536: train loss: 0.06777463853359222\n",
            "Epoch 85537: train loss: 0.06777250021696091\n",
            "Epoch 85538: train loss: 0.0677703469991684\n",
            "Epoch 85539: train loss: 0.06776819378137589\n",
            "Epoch 85540: train loss: 0.06776604801416397\n",
            "Epoch 85541: train loss: 0.06776390969753265\n",
            "Epoch 85542: train loss: 0.06776176393032074\n",
            "Epoch 85543: train loss: 0.06775961816310883\n",
            "Epoch 85544: train loss: 0.06775747239589691\n",
            "Epoch 85545: train loss: 0.0677553340792656\n",
            "Epoch 85546: train loss: 0.06775317341089249\n",
            "Epoch 85547: train loss: 0.06775102764368057\n",
            "Epoch 85548: train loss: 0.06774888932704926\n",
            "Epoch 85549: train loss: 0.06774673610925674\n",
            "Epoch 85550: train loss: 0.06774459779262543\n",
            "Epoch 85551: train loss: 0.06774245947599411\n",
            "Epoch 85552: train loss: 0.0677403062582016\n",
            "Epoch 85553: train loss: 0.06773816794157028\n",
            "Epoch 85554: train loss: 0.06773602962493896\n",
            "Epoch 85555: train loss: 0.06773387640714645\n",
            "Epoch 85556: train loss: 0.06773173809051514\n",
            "Epoch 85557: train loss: 0.06772959232330322\n",
            "Epoch 85558: train loss: 0.06772744655609131\n",
            "Epoch 85559: train loss: 0.0677253007888794\n",
            "Epoch 85560: train loss: 0.06772315502166748\n",
            "Epoch 85561: train loss: 0.06772101670503616\n",
            "Epoch 85562: train loss: 0.06771887093782425\n",
            "Epoch 85563: train loss: 0.06771672517061234\n",
            "Epoch 85564: train loss: 0.06771458685398102\n",
            "Epoch 85565: train loss: 0.06771243363618851\n",
            "Epoch 85566: train loss: 0.06771029531955719\n",
            "Epoch 85567: train loss: 0.06770814955234528\n",
            "Epoch 85568: train loss: 0.06770599633455276\n",
            "Epoch 85569: train loss: 0.06770387291908264\n",
            "Epoch 85570: train loss: 0.06770172715187073\n",
            "Epoch 85571: train loss: 0.06769958138465881\n",
            "Epoch 85572: train loss: 0.0676974430680275\n",
            "Epoch 85573: train loss: 0.06769528985023499\n",
            "Epoch 85574: train loss: 0.06769315153360367\n",
            "Epoch 85575: train loss: 0.06769100576639175\n",
            "Epoch 85576: train loss: 0.06768886744976044\n",
            "Epoch 85577: train loss: 0.06768672913312912\n",
            "Epoch 85578: train loss: 0.0676845833659172\n",
            "Epoch 85579: train loss: 0.06768243759870529\n",
            "Epoch 85580: train loss: 0.06768029183149338\n",
            "Epoch 85581: train loss: 0.06767815351486206\n",
            "Epoch 85582: train loss: 0.06767600774765015\n",
            "Epoch 85583: train loss: 0.06767386198043823\n",
            "Epoch 85584: train loss: 0.06767173111438751\n",
            "Epoch 85585: train loss: 0.0676695853471756\n",
            "Epoch 85586: train loss: 0.06766743957996368\n",
            "Epoch 85587: train loss: 0.06766529381275177\n",
            "Epoch 85588: train loss: 0.06766316294670105\n",
            "Epoch 85589: train loss: 0.06766100972890854\n",
            "Epoch 85590: train loss: 0.06765887141227722\n",
            "Epoch 85591: train loss: 0.06765672564506531\n",
            "Epoch 85592: train loss: 0.06765458732843399\n",
            "Epoch 85593: train loss: 0.06765244901180267\n",
            "Epoch 85594: train loss: 0.06765030324459076\n",
            "Epoch 85595: train loss: 0.06764817237854004\n",
            "Epoch 85596: train loss: 0.06764602661132812\n",
            "Epoch 85597: train loss: 0.06764388829469681\n",
            "Epoch 85598: train loss: 0.06764174997806549\n",
            "Epoch 85599: train loss: 0.06763959676027298\n",
            "Epoch 85600: train loss: 0.06763745099306107\n",
            "Epoch 85601: train loss: 0.06763531267642975\n",
            "Epoch 85602: train loss: 0.06763318181037903\n",
            "Epoch 85603: train loss: 0.06763104349374771\n",
            "Epoch 85604: train loss: 0.0676288902759552\n",
            "Epoch 85605: train loss: 0.06762675195932388\n",
            "Epoch 85606: train loss: 0.06762460619211197\n",
            "Epoch 85607: train loss: 0.06762246787548065\n",
            "Epoch 85608: train loss: 0.06762033700942993\n",
            "Epoch 85609: train loss: 0.06761820614337921\n",
            "Epoch 85610: train loss: 0.0676160678267479\n",
            "Epoch 85611: train loss: 0.06761392951011658\n",
            "Epoch 85612: train loss: 0.06761178374290466\n",
            "Epoch 85613: train loss: 0.06760965287685394\n",
            "Epoch 85614: train loss: 0.06760751456022263\n",
            "Epoch 85615: train loss: 0.06760537624359131\n",
            "Epoch 85616: train loss: 0.0676032304763794\n",
            "Epoch 85617: train loss: 0.06760109215974808\n",
            "Epoch 85618: train loss: 0.06759895384311676\n",
            "Epoch 85619: train loss: 0.06759682297706604\n",
            "Epoch 85620: train loss: 0.06759469211101532\n",
            "Epoch 85621: train loss: 0.067592553794384\n",
            "Epoch 85622: train loss: 0.06759042292833328\n",
            "Epoch 85623: train loss: 0.06758827716112137\n",
            "Epoch 85624: train loss: 0.06758614629507065\n",
            "Epoch 85625: train loss: 0.06758400052785873\n",
            "Epoch 85626: train loss: 0.06758186966180801\n",
            "Epoch 85627: train loss: 0.0675797313451767\n",
            "Epoch 85628: train loss: 0.06757760047912598\n",
            "Epoch 85629: train loss: 0.06757546216249466\n",
            "Epoch 85630: train loss: 0.06757333874702454\n",
            "Epoch 85631: train loss: 0.06757120788097382\n",
            "Epoch 85632: train loss: 0.0675690770149231\n",
            "Epoch 85633: train loss: 0.06756694614887238\n",
            "Epoch 85634: train loss: 0.06756482273340225\n",
            "Epoch 85635: train loss: 0.06756268441677094\n",
            "Epoch 85636: train loss: 0.06756055355072021\n",
            "Epoch 85637: train loss: 0.0675584226846695\n",
            "Epoch 85638: train loss: 0.06755629926919937\n",
            "Epoch 85639: train loss: 0.06755416095256805\n",
            "Epoch 85640: train loss: 0.06755203008651733\n",
            "Epoch 85641: train loss: 0.06754989922046661\n",
            "Epoch 85642: train loss: 0.0675477683544159\n",
            "Epoch 85643: train loss: 0.06754563748836517\n",
            "Epoch 85644: train loss: 0.06754350662231445\n",
            "Epoch 85645: train loss: 0.06754137575626373\n",
            "Epoch 85646: train loss: 0.06753925234079361\n",
            "Epoch 85647: train loss: 0.06753711402416229\n",
            "Epoch 85648: train loss: 0.06753498315811157\n",
            "Epoch 85649: train loss: 0.06753285974264145\n",
            "Epoch 85650: train loss: 0.06753072887659073\n",
            "Epoch 85651: train loss: 0.06752859801054001\n",
            "Epoch 85652: train loss: 0.06752645969390869\n",
            "Epoch 85653: train loss: 0.06752434372901917\n",
            "Epoch 85654: train loss: 0.06752220541238785\n",
            "Epoch 85655: train loss: 0.06752008199691772\n",
            "Epoch 85656: train loss: 0.067517951130867\n",
            "Epoch 85657: train loss: 0.06751582026481628\n",
            "Epoch 85658: train loss: 0.06751368939876556\n",
            "Epoch 85659: train loss: 0.06751156598329544\n",
            "Epoch 85660: train loss: 0.06750944256782532\n",
            "Epoch 85661: train loss: 0.0675073117017746\n",
            "Epoch 85662: train loss: 0.06750517338514328\n",
            "Epoch 85663: train loss: 0.06750304996967316\n",
            "Epoch 85664: train loss: 0.06750092655420303\n",
            "Epoch 85665: train loss: 0.06749879568815231\n",
            "Epoch 85666: train loss: 0.067496657371521\n",
            "Epoch 85667: train loss: 0.06749453395605087\n",
            "Epoch 85668: train loss: 0.06749240309000015\n",
            "Epoch 85669: train loss: 0.06749029457569122\n",
            "Epoch 85670: train loss: 0.0674881562590599\n",
            "Epoch 85671: train loss: 0.06748603284358978\n",
            "Epoch 85672: train loss: 0.06748389452695847\n",
            "Epoch 85673: train loss: 0.06748176366090775\n",
            "Epoch 85674: train loss: 0.06747964769601822\n",
            "Epoch 85675: train loss: 0.0674775093793869\n",
            "Epoch 85676: train loss: 0.06747538596391678\n",
            "Epoch 85677: train loss: 0.06747326999902725\n",
            "Epoch 85678: train loss: 0.06747113913297653\n",
            "Epoch 85679: train loss: 0.06746900826692581\n",
            "Epoch 85680: train loss: 0.06746688485145569\n",
            "Epoch 85681: train loss: 0.06746476143598557\n",
            "Epoch 85682: train loss: 0.06746263056993484\n",
            "Epoch 85683: train loss: 0.06746049970388412\n",
            "Epoch 85684: train loss: 0.06745835393667221\n",
            "Epoch 85685: train loss: 0.06745625287294388\n",
            "Epoch 85686: train loss: 0.06745411455631256\n",
            "Epoch 85687: train loss: 0.06745198369026184\n",
            "Epoch 85688: train loss: 0.06744986772537231\n",
            "Epoch 85689: train loss: 0.06744774430990219\n",
            "Epoch 85690: train loss: 0.06744560599327087\n",
            "Epoch 85691: train loss: 0.06744347512722015\n",
            "Epoch 85692: train loss: 0.06744136661291122\n",
            "Epoch 85693: train loss: 0.06743922829627991\n",
            "Epoch 85694: train loss: 0.06743711233139038\n",
            "Epoch 85695: train loss: 0.06743498146533966\n",
            "Epoch 85696: train loss: 0.06743285059928894\n",
            "Epoch 85697: train loss: 0.06743072718381882\n",
            "Epoch 85698: train loss: 0.06742861866950989\n",
            "Epoch 85699: train loss: 0.06742648035287857\n",
            "Epoch 85700: train loss: 0.06742436438798904\n",
            "Epoch 85701: train loss: 0.06742224097251892\n",
            "Epoch 85702: train loss: 0.0674201250076294\n",
            "Epoch 85703: train loss: 0.06741799414157867\n",
            "Epoch 85704: train loss: 0.06741587072610855\n",
            "Epoch 85705: train loss: 0.06741375476121902\n",
            "Epoch 85706: train loss: 0.0674116313457489\n",
            "Epoch 85707: train loss: 0.06740951538085938\n",
            "Epoch 85708: train loss: 0.06740738451480865\n",
            "Epoch 85709: train loss: 0.06740526109933853\n",
            "Epoch 85710: train loss: 0.067403145134449\n",
            "Epoch 85711: train loss: 0.06740103662014008\n",
            "Epoch 85712: train loss: 0.06739891320466995\n",
            "Epoch 85713: train loss: 0.06739677488803864\n",
            "Epoch 85714: train loss: 0.0673946738243103\n",
            "Epoch 85715: train loss: 0.06739255040884018\n",
            "Epoch 85716: train loss: 0.06739041209220886\n",
            "Epoch 85717: train loss: 0.06738831102848053\n",
            "Epoch 85718: train loss: 0.0673861876130104\n",
            "Epoch 85719: train loss: 0.06738406419754028\n",
            "Epoch 85720: train loss: 0.06738194823265076\n",
            "Epoch 85721: train loss: 0.06737983226776123\n",
            "Epoch 85722: train loss: 0.06737770885229111\n",
            "Epoch 85723: train loss: 0.06737559288740158\n",
            "Epoch 85724: train loss: 0.06737346947193146\n",
            "Epoch 85725: train loss: 0.06737134605646133\n",
            "Epoch 85726: train loss: 0.0673692375421524\n",
            "Epoch 85727: train loss: 0.06736711412668228\n",
            "Epoch 85728: train loss: 0.06736499071121216\n",
            "Epoch 85729: train loss: 0.06736288219690323\n",
            "Epoch 85730: train loss: 0.0673607587814331\n",
            "Epoch 85731: train loss: 0.06735864281654358\n",
            "Epoch 85732: train loss: 0.06735651940107346\n",
            "Epoch 85733: train loss: 0.06735440343618393\n",
            "Epoch 85734: train loss: 0.0673522800207138\n",
            "Epoch 85735: train loss: 0.06735015660524368\n",
            "Epoch 85736: train loss: 0.06734804064035416\n",
            "Epoch 85737: train loss: 0.06734593212604523\n",
            "Epoch 85738: train loss: 0.0673438087105751\n",
            "Epoch 85739: train loss: 0.06734169274568558\n",
            "Epoch 85740: train loss: 0.06733956933021545\n",
            "Epoch 85741: train loss: 0.06733746081590652\n",
            "Epoch 85742: train loss: 0.0673353299498558\n",
            "Epoch 85743: train loss: 0.06733321398496628\n",
            "Epoch 85744: train loss: 0.06733111292123795\n",
            "Epoch 85745: train loss: 0.06732898205518723\n",
            "Epoch 85746: train loss: 0.0673268660902977\n",
            "Epoch 85747: train loss: 0.06732475012540817\n",
            "Epoch 85748: train loss: 0.06732263416051865\n",
            "Epoch 85749: train loss: 0.06732051074504852\n",
            "Epoch 85750: train loss: 0.067318394780159\n",
            "Epoch 85751: train loss: 0.06731627136468887\n",
            "Epoch 85752: train loss: 0.06731416285037994\n",
            "Epoch 85753: train loss: 0.06731204688549042\n",
            "Epoch 85754: train loss: 0.06730993092060089\n",
            "Epoch 85755: train loss: 0.06730781495571136\n",
            "Epoch 85756: train loss: 0.06730569154024124\n",
            "Epoch 85757: train loss: 0.06730357557535172\n",
            "Epoch 85758: train loss: 0.06730145215988159\n",
            "Epoch 85759: train loss: 0.06729934364557266\n",
            "Epoch 85760: train loss: 0.06729722023010254\n",
            "Epoch 85761: train loss: 0.06729511171579361\n",
            "Epoch 85762: train loss: 0.06729298830032349\n",
            "Epoch 85763: train loss: 0.06729087978601456\n",
            "Epoch 85764: train loss: 0.06728876382112503\n",
            "Epoch 85765: train loss: 0.06728662550449371\n",
            "Epoch 85766: train loss: 0.06728451699018478\n",
            "Epoch 85767: train loss: 0.06728240847587585\n",
            "Epoch 85768: train loss: 0.06728029251098633\n",
            "Epoch 85769: train loss: 0.0672781690955162\n",
            "Epoch 85770: train loss: 0.06727606058120728\n",
            "Epoch 85771: train loss: 0.06727393716573715\n",
            "Epoch 85772: train loss: 0.06727182865142822\n",
            "Epoch 85773: train loss: 0.0672697052359581\n",
            "Epoch 85774: train loss: 0.06726760417222977\n",
            "Epoch 85775: train loss: 0.06726548820734024\n",
            "Epoch 85776: train loss: 0.06726334989070892\n",
            "Epoch 85777: train loss: 0.0672612413764\n",
            "Epoch 85778: train loss: 0.06725913286209106\n",
            "Epoch 85779: train loss: 0.06725701689720154\n",
            "Epoch 85780: train loss: 0.06725490093231201\n",
            "Epoch 85781: train loss: 0.06725278496742249\n",
            "Epoch 85782: train loss: 0.06725066900253296\n",
            "Epoch 85783: train loss: 0.06724854558706284\n",
            "Epoch 85784: train loss: 0.0672464370727539\n",
            "Epoch 85785: train loss: 0.06724432855844498\n",
            "Epoch 85786: train loss: 0.06724221259355545\n",
            "Epoch 85787: train loss: 0.06724009662866592\n",
            "Epoch 85788: train loss: 0.0672379732131958\n",
            "Epoch 85789: train loss: 0.06723586469888687\n",
            "Epoch 85790: train loss: 0.06723374873399734\n",
            "Epoch 85791: train loss: 0.06723164021968842\n",
            "Epoch 85792: train loss: 0.06722951680421829\n",
            "Epoch 85793: train loss: 0.06722740083932877\n",
            "Epoch 85794: train loss: 0.06722529232501984\n",
            "Epoch 85795: train loss: 0.06722318381071091\n",
            "Epoch 85796: train loss: 0.06722106039524078\n",
            "Epoch 85797: train loss: 0.06721894443035126\n",
            "Epoch 85798: train loss: 0.06721683591604233\n",
            "Epoch 85799: train loss: 0.0672147199511528\n",
            "Epoch 85800: train loss: 0.06721261143684387\n",
            "Epoch 85801: train loss: 0.06721049547195435\n",
            "Epoch 85802: train loss: 0.06720837205648422\n",
            "Epoch 85803: train loss: 0.06720627099275589\n",
            "Epoch 85804: train loss: 0.06720414757728577\n",
            "Epoch 85805: train loss: 0.06720203906297684\n",
            "Epoch 85806: train loss: 0.06719992309808731\n",
            "Epoch 85807: train loss: 0.06719781458377838\n",
            "Epoch 85808: train loss: 0.06719569861888885\n",
            "Epoch 85809: train loss: 0.06719358265399933\n",
            "Epoch 85810: train loss: 0.067191481590271\n",
            "Epoch 85811: train loss: 0.06718935817480087\n",
            "Epoch 85812: train loss: 0.06718724966049194\n",
            "Epoch 85813: train loss: 0.06718513369560242\n",
            "Epoch 85814: train loss: 0.06718302518129349\n",
            "Epoch 85815: train loss: 0.06718090921640396\n",
            "Epoch 85816: train loss: 0.06717879325151443\n",
            "Epoch 85817: train loss: 0.0671766921877861\n",
            "Epoch 85818: train loss: 0.06717456877231598\n",
            "Epoch 85819: train loss: 0.06717245280742645\n",
            "Epoch 85820: train loss: 0.06717035174369812\n",
            "Epoch 85821: train loss: 0.0671682357788086\n",
            "Epoch 85822: train loss: 0.06716612726449966\n",
            "Epoch 85823: train loss: 0.06716402620077133\n",
            "Epoch 85824: train loss: 0.06716190278530121\n",
            "Epoch 85825: train loss: 0.06715980917215347\n",
            "Epoch 85826: train loss: 0.06715770065784454\n",
            "Epoch 85827: train loss: 0.06715559214353561\n",
            "Epoch 85828: train loss: 0.06715347617864609\n",
            "Epoch 85829: train loss: 0.06715137511491776\n",
            "Epoch 85830: train loss: 0.06714925169944763\n",
            "Epoch 85831: train loss: 0.0671471506357193\n",
            "Epoch 85832: train loss: 0.06714504957199097\n",
            "Epoch 85833: train loss: 0.06714293360710144\n",
            "Epoch 85834: train loss: 0.06714082509279251\n",
            "Epoch 85835: train loss: 0.06713871657848358\n",
            "Epoch 85836: train loss: 0.06713661551475525\n",
            "Epoch 85837: train loss: 0.06713451445102692\n",
            "Epoch 85838: train loss: 0.06713240593671799\n",
            "Epoch 85839: train loss: 0.06713029742240906\n",
            "Epoch 85840: train loss: 0.06712819635868073\n",
            "Epoch 85841: train loss: 0.0671260803937912\n",
            "Epoch 85842: train loss: 0.06712397933006287\n",
            "Epoch 85843: train loss: 0.06712187826633453\n",
            "Epoch 85844: train loss: 0.0671197697520256\n",
            "Epoch 85845: train loss: 0.06711766123771667\n",
            "Epoch 85846: train loss: 0.06711556762456894\n",
            "Epoch 85847: train loss: 0.06711345165967941\n",
            "Epoch 85848: train loss: 0.06711135059595108\n",
            "Epoch 85849: train loss: 0.06710925698280334\n",
            "Epoch 85850: train loss: 0.06710714846849442\n",
            "Epoch 85851: train loss: 0.06710504740476608\n",
            "Epoch 85852: train loss: 0.06710294634103775\n",
            "Epoch 85853: train loss: 0.06710084527730942\n",
            "Epoch 85854: train loss: 0.06709874421358109\n",
            "Epoch 85855: train loss: 0.06709664314985275\n",
            "Epoch 85856: train loss: 0.06709453463554382\n",
            "Epoch 85857: train loss: 0.06709244102239609\n",
            "Epoch 85858: train loss: 0.06709034740924835\n",
            "Epoch 85859: train loss: 0.06708823889493942\n",
            "Epoch 85860: train loss: 0.06708613783121109\n",
            "Epoch 85861: train loss: 0.06708403676748276\n",
            "Epoch 85862: train loss: 0.06708193570375443\n",
            "Epoch 85863: train loss: 0.06707984209060669\n",
            "Epoch 85864: train loss: 0.06707774102687836\n",
            "Epoch 85865: train loss: 0.06707563251256943\n",
            "Epoch 85866: train loss: 0.0670735314488411\n",
            "Epoch 85867: train loss: 0.06707143038511276\n",
            "Epoch 85868: train loss: 0.06706933677196503\n",
            "Epoch 85869: train loss: 0.0670672357082367\n",
            "Epoch 85870: train loss: 0.06706514209508896\n",
            "Epoch 85871: train loss: 0.06706304103136063\n",
            "Epoch 85872: train loss: 0.06706094741821289\n",
            "Epoch 85873: train loss: 0.06705883890390396\n",
            "Epoch 85874: train loss: 0.06705674529075623\n",
            "Epoch 85875: train loss: 0.0670546442270279\n",
            "Epoch 85876: train loss: 0.06705255061388016\n",
            "Epoch 85877: train loss: 0.06705044209957123\n",
            "Epoch 85878: train loss: 0.0670483410358429\n",
            "Epoch 85879: train loss: 0.06704624742269516\n",
            "Epoch 85880: train loss: 0.06704413890838623\n",
            "Epoch 85881: train loss: 0.06704205274581909\n",
            "Epoch 85882: train loss: 0.06703995168209076\n",
            "Epoch 85883: train loss: 0.06703785061836243\n",
            "Epoch 85884: train loss: 0.06703575700521469\n",
            "Epoch 85885: train loss: 0.06703364849090576\n",
            "Epoch 85886: train loss: 0.06703155487775803\n",
            "Epoch 85887: train loss: 0.0670294463634491\n",
            "Epoch 85888: train loss: 0.06702736020088196\n",
            "Epoch 85889: train loss: 0.06702525913715363\n",
            "Epoch 85890: train loss: 0.06702315807342529\n",
            "Epoch 85891: train loss: 0.06702106446027756\n",
            "Epoch 85892: train loss: 0.06701896339654922\n",
            "Epoch 85893: train loss: 0.06701686978340149\n",
            "Epoch 85894: train loss: 0.06701476871967316\n",
            "Epoch 85895: train loss: 0.06701266765594482\n",
            "Epoch 85896: train loss: 0.06701056659221649\n",
            "Epoch 85897: train loss: 0.06700846552848816\n",
            "Epoch 85898: train loss: 0.06700637191534042\n",
            "Epoch 85899: train loss: 0.06700427830219269\n",
            "Epoch 85900: train loss: 0.06700217723846436\n",
            "Epoch 85901: train loss: 0.06700008362531662\n",
            "Epoch 85902: train loss: 0.06699799001216888\n",
            "Epoch 85903: train loss: 0.06699588894844055\n",
            "Epoch 85904: train loss: 0.06699379533529282\n",
            "Epoch 85905: train loss: 0.06699168682098389\n",
            "Epoch 85906: train loss: 0.06698960065841675\n",
            "Epoch 85907: train loss: 0.06698749959468842\n",
            "Epoch 85908: train loss: 0.06698540598154068\n",
            "Epoch 85909: train loss: 0.06698331981897354\n",
            "Epoch 85910: train loss: 0.0669812262058258\n",
            "Epoch 85911: train loss: 0.06697911024093628\n",
            "Epoch 85912: train loss: 0.06697700917720795\n",
            "Epoch 85913: train loss: 0.06697491556406021\n",
            "Epoch 85914: train loss: 0.06697282940149307\n",
            "Epoch 85915: train loss: 0.06697072088718414\n",
            "Epoch 85916: train loss: 0.066968634724617\n",
            "Epoch 85917: train loss: 0.06696653366088867\n",
            "Epoch 85918: train loss: 0.06696443259716034\n",
            "Epoch 85919: train loss: 0.0669623389840126\n",
            "Epoch 85920: train loss: 0.06696024537086487\n",
            "Epoch 85921: train loss: 0.06695814430713654\n",
            "Epoch 85922: train loss: 0.0669560506939888\n",
            "Epoch 85923: train loss: 0.06695395708084106\n",
            "Epoch 85924: train loss: 0.06695186346769333\n",
            "Epoch 85925: train loss: 0.066949762403965\n",
            "Epoch 85926: train loss: 0.06694766879081726\n",
            "Epoch 85927: train loss: 0.06694557517766953\n",
            "Epoch 85928: train loss: 0.06694348156452179\n",
            "Epoch 85929: train loss: 0.06694138050079346\n",
            "Epoch 85930: train loss: 0.06693928688764572\n",
            "Epoch 85931: train loss: 0.06693719327449799\n",
            "Epoch 85932: train loss: 0.06693509966135025\n",
            "Epoch 85933: train loss: 0.06693299114704132\n",
            "Epoch 85934: train loss: 0.06693090498447418\n",
            "Epoch 85935: train loss: 0.06692881882190704\n",
            "Epoch 85936: train loss: 0.06692671030759811\n",
            "Epoch 85937: train loss: 0.06692462414503098\n",
            "Epoch 85938: train loss: 0.06692252308130264\n",
            "Epoch 85939: train loss: 0.06692042946815491\n",
            "Epoch 85940: train loss: 0.06691833585500717\n",
            "Epoch 85941: train loss: 0.06691624224185944\n",
            "Epoch 85942: train loss: 0.0669141411781311\n",
            "Epoch 85943: train loss: 0.06691205501556396\n",
            "Epoch 85944: train loss: 0.06690996140241623\n",
            "Epoch 85945: train loss: 0.0669078677892685\n",
            "Epoch 85946: train loss: 0.06690576672554016\n",
            "Epoch 85947: train loss: 0.06690367311239243\n",
            "Epoch 85948: train loss: 0.06690157949924469\n",
            "Epoch 85949: train loss: 0.06689949333667755\n",
            "Epoch 85950: train loss: 0.06689740717411041\n",
            "Epoch 85951: train loss: 0.06689530611038208\n",
            "Epoch 85952: train loss: 0.06689321249723434\n",
            "Epoch 85953: train loss: 0.06689111143350601\n",
            "Epoch 85954: train loss: 0.06688901782035828\n",
            "Epoch 85955: train loss: 0.06688691675662994\n",
            "Epoch 85956: train loss: 0.0668848305940628\n",
            "Epoch 85957: train loss: 0.06688272953033447\n",
            "Epoch 85958: train loss: 0.06688064336776733\n",
            "Epoch 85959: train loss: 0.066878542304039\n",
            "Epoch 85960: train loss: 0.06687646359205246\n",
            "Epoch 85961: train loss: 0.06687435507774353\n",
            "Epoch 85962: train loss: 0.06687226891517639\n",
            "Epoch 85963: train loss: 0.06687017530202866\n",
            "Epoch 85964: train loss: 0.06686808913946152\n",
            "Epoch 85965: train loss: 0.06686598062515259\n",
            "Epoch 85966: train loss: 0.06686389446258545\n",
            "Epoch 85967: train loss: 0.06686181575059891\n",
            "Epoch 85968: train loss: 0.06685972213745117\n",
            "Epoch 85969: train loss: 0.06685762107372284\n",
            "Epoch 85970: train loss: 0.0668555349111557\n",
            "Epoch 85971: train loss: 0.06685344129800797\n",
            "Epoch 85972: train loss: 0.06685135513544083\n",
            "Epoch 85973: train loss: 0.06684926152229309\n",
            "Epoch 85974: train loss: 0.06684716790914536\n",
            "Epoch 85975: train loss: 0.06684508174657822\n",
            "Epoch 85976: train loss: 0.06684299558401108\n",
            "Epoch 85977: train loss: 0.06684090197086334\n",
            "Epoch 85978: train loss: 0.0668388083577156\n",
            "Epoch 85979: train loss: 0.06683671474456787\n",
            "Epoch 85980: train loss: 0.06683462858200073\n",
            "Epoch 85981: train loss: 0.0668325424194336\n",
            "Epoch 85982: train loss: 0.06683044135570526\n",
            "Epoch 85983: train loss: 0.06682836264371872\n",
            "Epoch 85984: train loss: 0.06682627648115158\n",
            "Epoch 85985: train loss: 0.06682417541742325\n",
            "Epoch 85986: train loss: 0.0668220967054367\n",
            "Epoch 85987: train loss: 0.06682000309228897\n",
            "Epoch 85988: train loss: 0.06681790202856064\n",
            "Epoch 85989: train loss: 0.0668158233165741\n",
            "Epoch 85990: train loss: 0.06681373715400696\n",
            "Epoch 85991: train loss: 0.06681164354085922\n",
            "Epoch 85992: train loss: 0.06680955737829208\n",
            "Epoch 85993: train loss: 0.06680747121572495\n",
            "Epoch 85994: train loss: 0.0668053925037384\n",
            "Epoch 85995: train loss: 0.06680329144001007\n",
            "Epoch 85996: train loss: 0.06680119782686234\n",
            "Epoch 85997: train loss: 0.0667991191148758\n",
            "Epoch 85998: train loss: 0.06679702550172806\n",
            "Epoch 85999: train loss: 0.06679493933916092\n",
            "Epoch 86000: train loss: 0.06679285317659378\n",
            "Epoch 86001: train loss: 0.06679076701402664\n",
            "Epoch 86002: train loss: 0.0667886734008789\n",
            "Epoch 86003: train loss: 0.06678658723831177\n",
            "Epoch 86004: train loss: 0.06678450107574463\n",
            "Epoch 86005: train loss: 0.06678242236375809\n",
            "Epoch 86006: train loss: 0.06678033620119095\n",
            "Epoch 86007: train loss: 0.06677825003862381\n",
            "Epoch 86008: train loss: 0.06677616387605667\n",
            "Epoch 86009: train loss: 0.06677408516407013\n",
            "Epoch 86010: train loss: 0.06677199900150299\n",
            "Epoch 86011: train loss: 0.06676990538835526\n",
            "Epoch 86012: train loss: 0.06676781922578812\n",
            "Epoch 86013: train loss: 0.06676574796438217\n",
            "Epoch 86014: train loss: 0.06676365435123444\n",
            "Epoch 86015: train loss: 0.0667615756392479\n",
            "Epoch 86016: train loss: 0.06675948947668076\n",
            "Epoch 86017: train loss: 0.06675741076469421\n",
            "Epoch 86018: train loss: 0.06675533205270767\n",
            "Epoch 86019: train loss: 0.06675326079130173\n",
            "Epoch 86020: train loss: 0.06675118207931519\n",
            "Epoch 86021: train loss: 0.06674908846616745\n",
            "Epoch 86022: train loss: 0.06674700975418091\n",
            "Epoch 86023: train loss: 0.06674493104219437\n",
            "Epoch 86024: train loss: 0.06674285233020782\n",
            "Epoch 86025: train loss: 0.06674078106880188\n",
            "Epoch 86026: train loss: 0.06673868745565414\n",
            "Epoch 86027: train loss: 0.0667366161942482\n",
            "Epoch 86028: train loss: 0.06673453748226166\n",
            "Epoch 86029: train loss: 0.06673245131969452\n",
            "Epoch 86030: train loss: 0.06673038005828857\n",
            "Epoch 86031: train loss: 0.06672829389572144\n",
            "Epoch 86032: train loss: 0.06672622263431549\n",
            "Epoch 86033: train loss: 0.06672414392232895\n",
            "Epoch 86034: train loss: 0.06672205775976181\n",
            "Epoch 86035: train loss: 0.06671997904777527\n",
            "Epoch 86036: train loss: 0.06671790033578873\n",
            "Epoch 86037: train loss: 0.06671582162380219\n",
            "Epoch 86038: train loss: 0.06671375036239624\n",
            "Epoch 86039: train loss: 0.0667116791009903\n",
            "Epoch 86040: train loss: 0.06670958548784256\n",
            "Epoch 86041: train loss: 0.06670750677585602\n",
            "Epoch 86042: train loss: 0.06670542806386948\n",
            "Epoch 86043: train loss: 0.06670335680246353\n",
            "Epoch 86044: train loss: 0.06670127809047699\n",
            "Epoch 86045: train loss: 0.06669919937849045\n",
            "Epoch 86046: train loss: 0.0666971281170845\n",
            "Epoch 86047: train loss: 0.06669504195451736\n",
            "Epoch 86048: train loss: 0.06669297069311142\n",
            "Epoch 86049: train loss: 0.06669089198112488\n",
            "Epoch 86050: train loss: 0.06668881326913834\n",
            "Epoch 86051: train loss: 0.06668674945831299\n",
            "Epoch 86052: train loss: 0.06668467074632645\n",
            "Epoch 86053: train loss: 0.0666825994849205\n",
            "Epoch 86054: train loss: 0.06668051332235336\n",
            "Epoch 86055: train loss: 0.06667844206094742\n",
            "Epoch 86056: train loss: 0.06667636334896088\n",
            "Epoch 86057: train loss: 0.06667428463697433\n",
            "Epoch 86058: train loss: 0.06667221337556839\n",
            "Epoch 86059: train loss: 0.06667014211416245\n",
            "Epoch 86060: train loss: 0.0666680708527565\n",
            "Epoch 86061: train loss: 0.06666599959135056\n",
            "Epoch 86062: train loss: 0.06666392832994461\n",
            "Epoch 86063: train loss: 0.06666184961795807\n",
            "Epoch 86064: train loss: 0.06665977090597153\n",
            "Epoch 86065: train loss: 0.06665769964456558\n",
            "Epoch 86066: train loss: 0.06665562093257904\n",
            "Epoch 86067: train loss: 0.06665355712175369\n",
            "Epoch 86068: train loss: 0.06665147840976715\n",
            "Epoch 86069: train loss: 0.06664939969778061\n",
            "Epoch 86070: train loss: 0.06664732843637466\n",
            "Epoch 86071: train loss: 0.06664525717496872\n",
            "Epoch 86072: train loss: 0.06664318591356277\n",
            "Epoch 86073: train loss: 0.06664111465215683\n",
            "Epoch 86074: train loss: 0.06663904339075089\n",
            "Epoch 86075: train loss: 0.06663697212934494\n",
            "Epoch 86076: train loss: 0.0666348934173584\n",
            "Epoch 86077: train loss: 0.06663282215595245\n",
            "Epoch 86078: train loss: 0.06663074344396591\n",
            "Epoch 86079: train loss: 0.06662867218255997\n",
            "Epoch 86080: train loss: 0.06662660837173462\n",
            "Epoch 86081: train loss: 0.06662452965974808\n",
            "Epoch 86082: train loss: 0.06662245839834213\n",
            "Epoch 86083: train loss: 0.06662038713693619\n",
            "Epoch 86084: train loss: 0.06661830842494965\n",
            "Epoch 86085: train loss: 0.0666162446141243\n",
            "Epoch 86086: train loss: 0.06661418080329895\n",
            "Epoch 86087: train loss: 0.06661209464073181\n",
            "Epoch 86088: train loss: 0.06661002337932587\n",
            "Epoch 86089: train loss: 0.06660795956850052\n",
            "Epoch 86090: train loss: 0.06660588085651398\n",
            "Epoch 86091: train loss: 0.06660380959510803\n",
            "Epoch 86092: train loss: 0.06660173833370209\n",
            "Epoch 86093: train loss: 0.06659966707229614\n",
            "Epoch 86094: train loss: 0.0665975958108902\n",
            "Epoch 86095: train loss: 0.06659553945064545\n",
            "Epoch 86096: train loss: 0.0665934607386589\n",
            "Epoch 86097: train loss: 0.06659139692783356\n",
            "Epoch 86098: train loss: 0.06658932566642761\n",
            "Epoch 86099: train loss: 0.06658725440502167\n",
            "Epoch 86100: train loss: 0.06658518314361572\n",
            "Epoch 86101: train loss: 0.06658311933279037\n",
            "Epoch 86102: train loss: 0.06658104807138443\n",
            "Epoch 86103: train loss: 0.06657897680997849\n",
            "Epoch 86104: train loss: 0.06657692044973373\n",
            "Epoch 86105: train loss: 0.06657484173774719\n",
            "Epoch 86106: train loss: 0.06657277792692184\n",
            "Epoch 86107: train loss: 0.0665707066655159\n",
            "Epoch 86108: train loss: 0.06656863540410995\n",
            "Epoch 86109: train loss: 0.0665665715932846\n",
            "Epoch 86110: train loss: 0.06656449288129807\n",
            "Epoch 86111: train loss: 0.06656242161989212\n",
            "Epoch 86112: train loss: 0.06656035780906677\n",
            "Epoch 86113: train loss: 0.06655830144882202\n",
            "Epoch 86114: train loss: 0.06655623018741608\n",
            "Epoch 86115: train loss: 0.06655415892601013\n",
            "Epoch 86116: train loss: 0.06655208021402359\n",
            "Epoch 86117: train loss: 0.06655002385377884\n",
            "Epoch 86118: train loss: 0.06654796749353409\n",
            "Epoch 86119: train loss: 0.06654588878154755\n",
            "Epoch 86120: train loss: 0.0665438175201416\n",
            "Epoch 86121: train loss: 0.06654174625873566\n",
            "Epoch 86122: train loss: 0.06653967499732971\n",
            "Epoch 86123: train loss: 0.06653761863708496\n",
            "Epoch 86124: train loss: 0.06653555482625961\n",
            "Epoch 86125: train loss: 0.06653347611427307\n",
            "Epoch 86126: train loss: 0.06653141230344772\n",
            "Epoch 86127: train loss: 0.06652934104204178\n",
            "Epoch 86128: train loss: 0.06652728468179703\n",
            "Epoch 86129: train loss: 0.06652520596981049\n",
            "Epoch 86130: train loss: 0.06652313470840454\n",
            "Epoch 86131: train loss: 0.06652108579874039\n",
            "Epoch 86132: train loss: 0.06651901453733444\n",
            "Epoch 86133: train loss: 0.0665169432759285\n",
            "Epoch 86134: train loss: 0.06651487946510315\n",
            "Epoch 86135: train loss: 0.0665128156542778\n",
            "Epoch 86136: train loss: 0.06651075184345245\n",
            "Epoch 86137: train loss: 0.06650867313146591\n",
            "Epoch 86138: train loss: 0.06650661677122116\n",
            "Epoch 86139: train loss: 0.06650454550981522\n",
            "Epoch 86140: train loss: 0.06650248169898987\n",
            "Epoch 86141: train loss: 0.06650041043758392\n",
            "Epoch 86142: train loss: 0.06649836152791977\n",
            "Epoch 86143: train loss: 0.06649628281593323\n",
            "Epoch 86144: train loss: 0.06649421900510788\n",
            "Epoch 86145: train loss: 0.06649216264486313\n",
            "Epoch 86146: train loss: 0.06649008393287659\n",
            "Epoch 86147: train loss: 0.06648802757263184\n",
            "Epoch 86148: train loss: 0.06648596376180649\n",
            "Epoch 86149: train loss: 0.06648388504981995\n",
            "Epoch 86150: train loss: 0.06648183614015579\n",
            "Epoch 86151: train loss: 0.06647976487874985\n",
            "Epoch 86152: train loss: 0.0664777010679245\n",
            "Epoch 86153: train loss: 0.06647563725709915\n",
            "Epoch 86154: train loss: 0.0664735659956932\n",
            "Epoch 86155: train loss: 0.06647150963544846\n",
            "Epoch 86156: train loss: 0.06646943837404251\n",
            "Epoch 86157: train loss: 0.06646736711263657\n",
            "Epoch 86158: train loss: 0.06646531075239182\n",
            "Epoch 86159: train loss: 0.06646324694156647\n",
            "Epoch 86160: train loss: 0.06646119058132172\n",
            "Epoch 86161: train loss: 0.06645911931991577\n",
            "Epoch 86162: train loss: 0.06645704805850983\n",
            "Epoch 86163: train loss: 0.06645498424768448\n",
            "Epoch 86164: train loss: 0.06645292043685913\n",
            "Epoch 86165: train loss: 0.06645085662603378\n",
            "Epoch 86166: train loss: 0.06644879281520844\n",
            "Epoch 86167: train loss: 0.06644674390554428\n",
            "Epoch 86168: train loss: 0.06644467264413834\n",
            "Epoch 86169: train loss: 0.06644260138273239\n",
            "Epoch 86170: train loss: 0.06644054502248764\n",
            "Epoch 86171: train loss: 0.0664384737610817\n",
            "Epoch 86172: train loss: 0.06643640249967575\n",
            "Epoch 86173: train loss: 0.0664343535900116\n",
            "Epoch 86174: train loss: 0.06643228977918625\n",
            "Epoch 86175: train loss: 0.0664302185177803\n",
            "Epoch 86176: train loss: 0.06642815470695496\n",
            "Epoch 86177: train loss: 0.06642609089612961\n",
            "Epoch 86178: train loss: 0.06642402708530426\n",
            "Epoch 86179: train loss: 0.06642197072505951\n",
            "Epoch 86180: train loss: 0.06641989946365356\n",
            "Epoch 86181: train loss: 0.06641783565282822\n",
            "Epoch 86182: train loss: 0.06641577184200287\n",
            "Epoch 86183: train loss: 0.06641372293233871\n",
            "Epoch 86184: train loss: 0.06641165912151337\n",
            "Epoch 86185: train loss: 0.06640958786010742\n",
            "Epoch 86186: train loss: 0.06640753149986267\n",
            "Epoch 86187: train loss: 0.06640547513961792\n",
            "Epoch 86188: train loss: 0.06640341877937317\n",
            "Epoch 86189: train loss: 0.06640136241912842\n",
            "Epoch 86190: train loss: 0.06639929115772247\n",
            "Epoch 86191: train loss: 0.06639722734689713\n",
            "Epoch 86192: train loss: 0.06639516353607178\n",
            "Epoch 86193: train loss: 0.06639309972524643\n",
            "Epoch 86194: train loss: 0.06639102846384048\n",
            "Epoch 86195: train loss: 0.06638896465301514\n",
            "Epoch 86196: train loss: 0.06638690084218979\n",
            "Epoch 86197: train loss: 0.06638484448194504\n",
            "Epoch 86198: train loss: 0.06638277322053909\n",
            "Epoch 86199: train loss: 0.06638071686029434\n",
            "Epoch 86200: train loss: 0.066378653049469\n",
            "Epoch 86201: train loss: 0.06637660413980484\n",
            "Epoch 86202: train loss: 0.0663745328783989\n",
            "Epoch 86203: train loss: 0.06637247651815414\n",
            "Epoch 86204: train loss: 0.0663704201579094\n",
            "Epoch 86205: train loss: 0.06636835634708405\n",
            "Epoch 86206: train loss: 0.0663662999868393\n",
            "Epoch 86207: train loss: 0.06636423617601395\n",
            "Epoch 86208: train loss: 0.0663621798157692\n",
            "Epoch 86209: train loss: 0.06636012345552444\n",
            "Epoch 86210: train loss: 0.0663580596446991\n",
            "Epoch 86211: train loss: 0.06635600328445435\n",
            "Epoch 86212: train loss: 0.066353939473629\n",
            "Epoch 86213: train loss: 0.06635189056396484\n",
            "Epoch 86214: train loss: 0.06634983420372009\n",
            "Epoch 86215: train loss: 0.06634778529405594\n",
            "Epoch 86216: train loss: 0.06634572148323059\n",
            "Epoch 86217: train loss: 0.06634366512298584\n",
            "Epoch 86218: train loss: 0.06634160131216049\n",
            "Epoch 86219: train loss: 0.06633955240249634\n",
            "Epoch 86220: train loss: 0.06633749604225159\n",
            "Epoch 86221: train loss: 0.06633543968200684\n",
            "Epoch 86222: train loss: 0.06633338332176208\n",
            "Epoch 86223: train loss: 0.06633132696151733\n",
            "Epoch 86224: train loss: 0.06632927805185318\n",
            "Epoch 86225: train loss: 0.06632722169160843\n",
            "Epoch 86226: train loss: 0.06632516533136368\n",
            "Epoch 86227: train loss: 0.06632310152053833\n",
            "Epoch 86228: train loss: 0.06632106006145477\n",
            "Epoch 86229: train loss: 0.06631900370121002\n",
            "Epoch 86230: train loss: 0.06631696224212646\n",
            "Epoch 86231: train loss: 0.06631489843130112\n",
            "Epoch 86232: train loss: 0.06631284207105637\n",
            "Epoch 86233: train loss: 0.06631079316139221\n",
            "Epoch 86234: train loss: 0.06630874425172806\n",
            "Epoch 86235: train loss: 0.0663066953420639\n",
            "Epoch 86236: train loss: 0.06630463153123856\n",
            "Epoch 86237: train loss: 0.0663025751709938\n",
            "Epoch 86238: train loss: 0.06630052626132965\n",
            "Epoch 86239: train loss: 0.0662984699010849\n",
            "Epoch 86240: train loss: 0.06629641354084015\n",
            "Epoch 86241: train loss: 0.06629437208175659\n",
            "Epoch 86242: train loss: 0.06629232317209244\n",
            "Epoch 86243: train loss: 0.06629027426242828\n",
            "Epoch 86244: train loss: 0.06628821045160294\n",
            "Epoch 86245: train loss: 0.06628616154193878\n",
            "Epoch 86246: train loss: 0.06628410518169403\n",
            "Epoch 86247: train loss: 0.06628205627202988\n",
            "Epoch 86248: train loss: 0.06628000736236572\n",
            "Epoch 86249: train loss: 0.06627795100212097\n",
            "Epoch 86250: train loss: 0.06627590954303741\n",
            "Epoch 86251: train loss: 0.06627385318279266\n",
            "Epoch 86252: train loss: 0.06627179682254791\n",
            "Epoch 86253: train loss: 0.06626974791288376\n",
            "Epoch 86254: train loss: 0.06626769155263901\n",
            "Epoch 86255: train loss: 0.06626563519239426\n",
            "Epoch 86256: train loss: 0.0662636011838913\n",
            "Epoch 86257: train loss: 0.06626155227422714\n",
            "Epoch 86258: train loss: 0.0662594884634018\n",
            "Epoch 86259: train loss: 0.06625743955373764\n",
            "Epoch 86260: train loss: 0.06625538319349289\n",
            "Epoch 86261: train loss: 0.06625332683324814\n",
            "Epoch 86262: train loss: 0.06625128537416458\n",
            "Epoch 86263: train loss: 0.06624923646450043\n",
            "Epoch 86264: train loss: 0.06624718010425568\n",
            "Epoch 86265: train loss: 0.06624513864517212\n",
            "Epoch 86266: train loss: 0.06624307483434677\n",
            "Epoch 86267: train loss: 0.06624104082584381\n",
            "Epoch 86268: train loss: 0.06623897701501846\n",
            "Epoch 86269: train loss: 0.0662369355559349\n",
            "Epoch 86270: train loss: 0.06623488664627075\n",
            "Epoch 86271: train loss: 0.0662328451871872\n",
            "Epoch 86272: train loss: 0.06623078882694244\n",
            "Epoch 86273: train loss: 0.06622873246669769\n",
            "Epoch 86274: train loss: 0.06622668355703354\n",
            "Epoch 86275: train loss: 0.06622463464736938\n",
            "Epoch 86276: train loss: 0.06622258573770523\n",
            "Epoch 86277: train loss: 0.06622053682804108\n",
            "Epoch 86278: train loss: 0.06621848791837692\n",
            "Epoch 86279: train loss: 0.06621643900871277\n",
            "Epoch 86280: train loss: 0.06621439009904861\n",
            "Epoch 86281: train loss: 0.06621234863996506\n",
            "Epoch 86282: train loss: 0.0662102922797203\n",
            "Epoch 86283: train loss: 0.06620824337005615\n",
            "Epoch 86284: train loss: 0.066206194460392\n",
            "Epoch 86285: train loss: 0.06620413810014725\n",
            "Epoch 86286: train loss: 0.06620209664106369\n",
            "Epoch 86287: train loss: 0.06620004773139954\n",
            "Epoch 86288: train loss: 0.06619799882173538\n",
            "Epoch 86289: train loss: 0.06619593501091003\n",
            "Epoch 86290: train loss: 0.06619390100240707\n",
            "Epoch 86291: train loss: 0.06619185954332352\n",
            "Epoch 86292: train loss: 0.06618980318307877\n",
            "Epoch 86293: train loss: 0.06618775427341461\n",
            "Epoch 86294: train loss: 0.06618571281433105\n",
            "Epoch 86295: train loss: 0.0661836639046669\n",
            "Epoch 86296: train loss: 0.06618162244558334\n",
            "Epoch 86297: train loss: 0.06617957353591919\n",
            "Epoch 86298: train loss: 0.06617752462625504\n",
            "Epoch 86299: train loss: 0.06617546826601028\n",
            "Epoch 86300: train loss: 0.06617341935634613\n",
            "Epoch 86301: train loss: 0.06617137044668198\n",
            "Epoch 86302: train loss: 0.06616933643817902\n",
            "Epoch 86303: train loss: 0.06616728007793427\n",
            "Epoch 86304: train loss: 0.0661652460694313\n",
            "Epoch 86305: train loss: 0.06616318970918655\n",
            "Epoch 86306: train loss: 0.066161148250103\n",
            "Epoch 86307: train loss: 0.06615910679101944\n",
            "Epoch 86308: train loss: 0.06615706533193588\n",
            "Epoch 86309: train loss: 0.06615500152111053\n",
            "Epoch 86310: train loss: 0.06615296006202698\n",
            "Epoch 86311: train loss: 0.06615091115236282\n",
            "Epoch 86312: train loss: 0.06614886969327927\n",
            "Epoch 86313: train loss: 0.06614681333303452\n",
            "Epoch 86314: train loss: 0.06614477932453156\n",
            "Epoch 86315: train loss: 0.0661427304148674\n",
            "Epoch 86316: train loss: 0.06614068150520325\n",
            "Epoch 86317: train loss: 0.06613863259553909\n",
            "Epoch 86318: train loss: 0.06613659113645554\n",
            "Epoch 86319: train loss: 0.06613454967737198\n",
            "Epoch 86320: train loss: 0.06613249331712723\n",
            "Epoch 86321: train loss: 0.06613045930862427\n",
            "Epoch 86322: train loss: 0.06612841039896011\n",
            "Epoch 86323: train loss: 0.06612636893987656\n",
            "Epoch 86324: train loss: 0.066124327480793\n",
            "Epoch 86325: train loss: 0.06612227112054825\n",
            "Epoch 86326: train loss: 0.06612025201320648\n",
            "Epoch 86327: train loss: 0.06611820310354233\n",
            "Epoch 86328: train loss: 0.06611614674329758\n",
            "Epoch 86329: train loss: 0.06611411273479462\n",
            "Epoch 86330: train loss: 0.06611206382513046\n",
            "Epoch 86331: train loss: 0.0661100298166275\n",
            "Epoch 86332: train loss: 0.06610798835754395\n",
            "Epoch 86333: train loss: 0.06610593944787979\n",
            "Epoch 86334: train loss: 0.06610389798879623\n",
            "Epoch 86335: train loss: 0.06610184907913208\n",
            "Epoch 86336: train loss: 0.06609980762004852\n",
            "Epoch 86337: train loss: 0.06609776616096497\n",
            "Epoch 86338: train loss: 0.066095732152462\n",
            "Epoch 86339: train loss: 0.06609367579221725\n",
            "Epoch 86340: train loss: 0.06609164923429489\n",
            "Epoch 86341: train loss: 0.06608960032463074\n",
            "Epoch 86342: train loss: 0.06608756631612778\n",
            "Epoch 86343: train loss: 0.06608553230762482\n",
            "Epoch 86344: train loss: 0.06608348339796066\n",
            "Epoch 86345: train loss: 0.0660814419388771\n",
            "Epoch 86346: train loss: 0.06607940047979355\n",
            "Epoch 86347: train loss: 0.06607735902070999\n",
            "Epoch 86348: train loss: 0.06607532501220703\n",
            "Epoch 86349: train loss: 0.06607329845428467\n",
            "Epoch 86350: train loss: 0.06607123464345932\n",
            "Epoch 86351: train loss: 0.06606920808553696\n",
            "Epoch 86352: train loss: 0.066067174077034\n",
            "Epoch 86353: train loss: 0.06606512516736984\n",
            "Epoch 86354: train loss: 0.06606309115886688\n",
            "Epoch 86355: train loss: 0.06606104969978333\n",
            "Epoch 86356: train loss: 0.06605901569128036\n",
            "Epoch 86357: train loss: 0.06605697423219681\n",
            "Epoch 86358: train loss: 0.06605494022369385\n",
            "Epoch 86359: train loss: 0.0660528913140297\n",
            "Epoch 86360: train loss: 0.06605084985494614\n",
            "Epoch 86361: train loss: 0.06604882329702377\n",
            "Epoch 86362: train loss: 0.06604678183794022\n",
            "Epoch 86363: train loss: 0.06604474037885666\n",
            "Epoch 86364: train loss: 0.0660426989197731\n",
            "Epoch 86365: train loss: 0.06604066491127014\n",
            "Epoch 86366: train loss: 0.06603863090276718\n",
            "Epoch 86367: train loss: 0.06603658199310303\n",
            "Epoch 86368: train loss: 0.06603454798460007\n",
            "Epoch 86369: train loss: 0.0660325214266777\n",
            "Epoch 86370: train loss: 0.06603047251701355\n",
            "Epoch 86371: train loss: 0.06602843850851059\n",
            "Epoch 86372: train loss: 0.06602641195058823\n",
            "Epoch 86373: train loss: 0.06602437049150467\n",
            "Epoch 86374: train loss: 0.06602232903242111\n",
            "Epoch 86375: train loss: 0.06602028757333755\n",
            "Epoch 86376: train loss: 0.0660182535648346\n",
            "Epoch 86377: train loss: 0.06601621210575104\n",
            "Epoch 86378: train loss: 0.06601417064666748\n",
            "Epoch 86379: train loss: 0.06601213663816452\n",
            "Epoch 86380: train loss: 0.06601010262966156\n",
            "Epoch 86381: train loss: 0.0660080686211586\n",
            "Epoch 86382: train loss: 0.06600602716207504\n",
            "Epoch 86383: train loss: 0.06600398570299149\n",
            "Epoch 86384: train loss: 0.06600195914506912\n",
            "Epoch 86385: train loss: 0.06599991768598557\n",
            "Epoch 86386: train loss: 0.0659978836774826\n",
            "Epoch 86387: train loss: 0.06599584221839905\n",
            "Epoch 86388: train loss: 0.06599380075931549\n",
            "Epoch 86389: train loss: 0.06599178165197372\n",
            "Epoch 86390: train loss: 0.06598974019289017\n",
            "Epoch 86391: train loss: 0.06598769873380661\n",
            "Epoch 86392: train loss: 0.06598565727472305\n",
            "Epoch 86393: train loss: 0.06598363816738129\n",
            "Epoch 86394: train loss: 0.06598158925771713\n",
            "Epoch 86395: train loss: 0.06597955524921417\n",
            "Epoch 86396: train loss: 0.06597752124071121\n",
            "Epoch 86397: train loss: 0.06597547978162766\n",
            "Epoch 86398: train loss: 0.0659734457731247\n",
            "Epoch 86399: train loss: 0.06597141176462173\n",
            "Epoch 86400: train loss: 0.06596937775611877\n",
            "Epoch 86401: train loss: 0.06596733629703522\n",
            "Epoch 86402: train loss: 0.06596530228853226\n",
            "Epoch 86403: train loss: 0.0659632757306099\n",
            "Epoch 86404: train loss: 0.06596124172210693\n",
            "Epoch 86405: train loss: 0.06595921516418457\n",
            "Epoch 86406: train loss: 0.06595718115568161\n",
            "Epoch 86407: train loss: 0.06595515459775925\n",
            "Epoch 86408: train loss: 0.06595312803983688\n",
            "Epoch 86409: train loss: 0.06595107913017273\n",
            "Epoch 86410: train loss: 0.06594906747341156\n",
            "Epoch 86411: train loss: 0.065947026014328\n",
            "Epoch 86412: train loss: 0.06594500690698624\n",
            "Epoch 86413: train loss: 0.06594296544790268\n",
            "Epoch 86414: train loss: 0.06594095379114151\n",
            "Epoch 86415: train loss: 0.06593891233205795\n",
            "Epoch 86416: train loss: 0.06593688577413559\n",
            "Epoch 86417: train loss: 0.06593485921621323\n",
            "Epoch 86418: train loss: 0.06593282520771027\n",
            "Epoch 86419: train loss: 0.0659308061003685\n",
            "Epoch 86420: train loss: 0.06592877209186554\n",
            "Epoch 86421: train loss: 0.06592675298452377\n",
            "Epoch 86422: train loss: 0.06592471152544022\n",
            "Epoch 86423: train loss: 0.06592269986867905\n",
            "Epoch 86424: train loss: 0.06592066586017609\n",
            "Epoch 86425: train loss: 0.06591864675283432\n",
            "Epoch 86426: train loss: 0.06591660529375076\n",
            "Epoch 86427: train loss: 0.0659145787358284\n",
            "Epoch 86428: train loss: 0.06591253727674484\n",
            "Epoch 86429: train loss: 0.06591052561998367\n",
            "Epoch 86430: train loss: 0.0659085065126419\n",
            "Epoch 86431: train loss: 0.06590646505355835\n",
            "Epoch 86432: train loss: 0.06590444594621658\n",
            "Epoch 86433: train loss: 0.06590241193771362\n",
            "Epoch 86434: train loss: 0.06590038537979126\n",
            "Epoch 86435: train loss: 0.0658983662724495\n",
            "Epoch 86436: train loss: 0.06589633226394653\n",
            "Epoch 86437: train loss: 0.06589430570602417\n",
            "Epoch 86438: train loss: 0.0658922791481018\n",
            "Epoch 86439: train loss: 0.06589025259017944\n",
            "Epoch 86440: train loss: 0.06588821858167648\n",
            "Epoch 86441: train loss: 0.06588620692491531\n",
            "Epoch 86442: train loss: 0.06588418781757355\n",
            "Epoch 86443: train loss: 0.06588215380907059\n",
            "Epoch 86444: train loss: 0.06588012725114822\n",
            "Epoch 86445: train loss: 0.06587810814380646\n",
            "Epoch 86446: train loss: 0.0658760741353035\n",
            "Epoch 86447: train loss: 0.06587405502796173\n",
            "Epoch 86448: train loss: 0.06587203592061996\n",
            "Epoch 86449: train loss: 0.0658700093626976\n",
            "Epoch 86450: train loss: 0.06586799025535583\n",
            "Epoch 86451: train loss: 0.06586595624685287\n",
            "Epoch 86452: train loss: 0.06586393713951111\n",
            "Epoch 86453: train loss: 0.06586191058158875\n",
            "Epoch 86454: train loss: 0.06585989147424698\n",
            "Epoch 86455: train loss: 0.06585786491632462\n",
            "Epoch 86456: train loss: 0.06585584580898285\n",
            "Epoch 86457: train loss: 0.06585382670164108\n",
            "Epoch 86458: train loss: 0.06585180759429932\n",
            "Epoch 86459: train loss: 0.06584976613521576\n",
            "Epoch 86460: train loss: 0.06584775447845459\n",
            "Epoch 86461: train loss: 0.06584573537111282\n",
            "Epoch 86462: train loss: 0.06584370136260986\n",
            "Epoch 86463: train loss: 0.0658416897058487\n",
            "Epoch 86464: train loss: 0.06583966314792633\n",
            "Epoch 86465: train loss: 0.06583764404058456\n",
            "Epoch 86466: train loss: 0.0658356100320816\n",
            "Epoch 86467: train loss: 0.06583359092473984\n",
            "Epoch 86468: train loss: 0.06583157926797867\n",
            "Epoch 86469: train loss: 0.0658295527100563\n",
            "Epoch 86470: train loss: 0.06582753360271454\n",
            "Epoch 86471: train loss: 0.06582549959421158\n",
            "Epoch 86472: train loss: 0.06582348048686981\n",
            "Epoch 86473: train loss: 0.06582145392894745\n",
            "Epoch 86474: train loss: 0.06581944227218628\n",
            "Epoch 86475: train loss: 0.06581740826368332\n",
            "Epoch 86476: train loss: 0.06581539660692215\n",
            "Epoch 86477: train loss: 0.06581337004899979\n",
            "Epoch 86478: train loss: 0.06581135094165802\n",
            "Epoch 86479: train loss: 0.06580932438373566\n",
            "Epoch 86480: train loss: 0.06580730527639389\n",
            "Epoch 86481: train loss: 0.06580529361963272\n",
            "Epoch 86482: train loss: 0.06580326706171036\n",
            "Epoch 86483: train loss: 0.065801240503788\n",
            "Epoch 86484: train loss: 0.06579922884702682\n",
            "Epoch 86485: train loss: 0.06579720973968506\n",
            "Epoch 86486: train loss: 0.0657951831817627\n",
            "Epoch 86487: train loss: 0.06579315662384033\n",
            "Epoch 86488: train loss: 0.06579113751649857\n",
            "Epoch 86489: train loss: 0.0657891035079956\n",
            "Epoch 86490: train loss: 0.06578709930181503\n",
            "Epoch 86491: train loss: 0.06578506529331207\n",
            "Epoch 86492: train loss: 0.0657830536365509\n",
            "Epoch 86493: train loss: 0.06578102707862854\n",
            "Epoch 86494: train loss: 0.06577900797128677\n",
            "Epoch 86495: train loss: 0.06577697396278381\n",
            "Epoch 86496: train loss: 0.06577495485544205\n",
            "Epoch 86497: train loss: 0.06577294319868088\n",
            "Epoch 86498: train loss: 0.06577093154191971\n",
            "Epoch 86499: train loss: 0.06576890498399734\n",
            "Epoch 86500: train loss: 0.06576687842607498\n",
            "Epoch 86501: train loss: 0.06576486676931381\n",
            "Epoch 86502: train loss: 0.06576284766197205\n",
            "Epoch 86503: train loss: 0.06576082110404968\n",
            "Epoch 86504: train loss: 0.06575879454612732\n",
            "Epoch 86505: train loss: 0.06575678288936615\n",
            "Epoch 86506: train loss: 0.06575476378202438\n",
            "Epoch 86507: train loss: 0.06575275212526321\n",
            "Epoch 86508: train loss: 0.06575071811676025\n",
            "Epoch 86509: train loss: 0.06574869155883789\n",
            "Epoch 86510: train loss: 0.06574667990207672\n",
            "Epoch 86511: train loss: 0.06574466079473495\n",
            "Epoch 86512: train loss: 0.06574264168739319\n",
            "Epoch 86513: train loss: 0.06574062258005142\n",
            "Epoch 86514: train loss: 0.06573859602212906\n",
            "Epoch 86515: train loss: 0.06573658436536789\n",
            "Epoch 86516: train loss: 0.06573456525802612\n",
            "Epoch 86517: train loss: 0.06573253870010376\n",
            "Epoch 86518: train loss: 0.06573053449392319\n",
            "Epoch 86519: train loss: 0.06572851538658142\n",
            "Epoch 86520: train loss: 0.06572649627923965\n",
            "Epoch 86521: train loss: 0.06572446972131729\n",
            "Epoch 86522: train loss: 0.06572245061397552\n",
            "Epoch 86523: train loss: 0.06572042405605316\n",
            "Epoch 86524: train loss: 0.06571841984987259\n",
            "Epoch 86525: train loss: 0.06571639329195023\n",
            "Epoch 86526: train loss: 0.06571438908576965\n",
            "Epoch 86527: train loss: 0.0657123550772667\n",
            "Epoch 86528: train loss: 0.06571034342050552\n",
            "Epoch 86529: train loss: 0.06570833176374435\n",
            "Epoch 86530: train loss: 0.06570631265640259\n",
            "Epoch 86531: train loss: 0.06570430099964142\n",
            "Epoch 86532: train loss: 0.06570228189229965\n",
            "Epoch 86533: train loss: 0.06570025533437729\n",
            "Epoch 86534: train loss: 0.06569824367761612\n",
            "Epoch 86535: train loss: 0.06569622457027435\n",
            "Epoch 86536: train loss: 0.06569421291351318\n",
            "Epoch 86537: train loss: 0.06569219380617142\n",
            "Epoch 86538: train loss: 0.06569017469882965\n",
            "Epoch 86539: train loss: 0.06568816304206848\n",
            "Epoch 86540: train loss: 0.06568614393472672\n",
            "Epoch 86541: train loss: 0.06568412482738495\n",
            "Epoch 86542: train loss: 0.06568209081888199\n",
            "Epoch 86543: train loss: 0.06568009406328201\n",
            "Epoch 86544: train loss: 0.06567806750535965\n",
            "Epoch 86545: train loss: 0.06567604839801788\n",
            "Epoch 86546: train loss: 0.06567403674125671\n",
            "Epoch 86547: train loss: 0.06567202508449554\n",
            "Epoch 86548: train loss: 0.06567000597715378\n",
            "Epoch 86549: train loss: 0.06566798686981201\n",
            "Epoch 86550: train loss: 0.06566596031188965\n",
            "Epoch 86551: train loss: 0.06566394865512848\n",
            "Epoch 86552: train loss: 0.06566193699836731\n",
            "Epoch 86553: train loss: 0.06565992534160614\n",
            "Epoch 86554: train loss: 0.06565790623426437\n",
            "Epoch 86555: train loss: 0.06565588712692261\n",
            "Epoch 86556: train loss: 0.06565388292074203\n",
            "Epoch 86557: train loss: 0.06565185636281967\n",
            "Epoch 86558: train loss: 0.0656498372554779\n",
            "Epoch 86559: train loss: 0.06564781814813614\n",
            "Epoch 86560: train loss: 0.06564580649137497\n",
            "Epoch 86561: train loss: 0.0656437873840332\n",
            "Epoch 86562: train loss: 0.06564177572727203\n",
            "Epoch 86563: train loss: 0.06563976407051086\n",
            "Epoch 86564: train loss: 0.0656377524137497\n",
            "Epoch 86565: train loss: 0.06563574075698853\n",
            "Epoch 86566: train loss: 0.06563372164964676\n",
            "Epoch 86567: train loss: 0.0656316950917244\n",
            "Epoch 86568: train loss: 0.06562969088554382\n",
            "Epoch 86569: train loss: 0.06562767177820206\n",
            "Epoch 86570: train loss: 0.06562565267086029\n",
            "Epoch 86571: train loss: 0.06562364101409912\n",
            "Epoch 86572: train loss: 0.06562162190675735\n",
            "Epoch 86573: train loss: 0.06561961770057678\n",
            "Epoch 86574: train loss: 0.06561759859323502\n",
            "Epoch 86575: train loss: 0.06561558693647385\n",
            "Epoch 86576: train loss: 0.06561357527971268\n",
            "Epoch 86577: train loss: 0.06561156362295151\n",
            "Epoch 86578: train loss: 0.06560954451560974\n",
            "Epoch 86579: train loss: 0.06560752540826797\n",
            "Epoch 86580: train loss: 0.06560550630092621\n",
            "Epoch 86581: train loss: 0.06560348719358444\n",
            "Epoch 86582: train loss: 0.06560148298740387\n",
            "Epoch 86583: train loss: 0.0655994787812233\n",
            "Epoch 86584: train loss: 0.06559745222330093\n",
            "Epoch 86585: train loss: 0.06559543311595917\n",
            "Epoch 86586: train loss: 0.065593421459198\n",
            "Epoch 86587: train loss: 0.06559141725301743\n",
            "Epoch 86588: train loss: 0.06558939814567566\n",
            "Epoch 86589: train loss: 0.06558737903833389\n",
            "Epoch 86590: train loss: 0.06558537483215332\n",
            "Epoch 86591: train loss: 0.06558335572481155\n",
            "Epoch 86592: train loss: 0.06558135151863098\n",
            "Epoch 86593: train loss: 0.06557933241128922\n",
            "Epoch 86594: train loss: 0.06557732075452805\n",
            "Epoch 86595: train loss: 0.06557530909776688\n",
            "Epoch 86596: train loss: 0.0655732974410057\n",
            "Epoch 86597: train loss: 0.06557129323482513\n",
            "Epoch 86598: train loss: 0.06556928157806396\n",
            "Epoch 86599: train loss: 0.0655672699213028\n",
            "Epoch 86600: train loss: 0.06556525081396103\n",
            "Epoch 86601: train loss: 0.06556323915719986\n",
            "Epoch 86602: train loss: 0.06556122750043869\n",
            "Epoch 86603: train loss: 0.06555922329425812\n",
            "Epoch 86604: train loss: 0.06555720418691635\n",
            "Epoch 86605: train loss: 0.06555521488189697\n",
            "Epoch 86606: train loss: 0.0655531957745552\n",
            "Epoch 86607: train loss: 0.06555118411779404\n",
            "Epoch 86608: train loss: 0.06554917991161346\n",
            "Epoch 86609: train loss: 0.06554719060659409\n",
            "Epoch 86610: train loss: 0.06554517894983292\n",
            "Epoch 86611: train loss: 0.06554316729307175\n",
            "Epoch 86612: train loss: 0.06554115563631058\n",
            "Epoch 86613: train loss: 0.0655391588807106\n",
            "Epoch 86614: train loss: 0.06553715467453003\n",
            "Epoch 86615: train loss: 0.06553516536951065\n",
            "Epoch 86616: train loss: 0.06553314626216888\n",
            "Epoch 86617: train loss: 0.06553113460540771\n",
            "Epoch 86618: train loss: 0.06552913784980774\n",
            "Epoch 86619: train loss: 0.06552714109420776\n",
            "Epoch 86620: train loss: 0.065525121986866\n",
            "Epoch 86621: train loss: 0.06552311778068542\n",
            "Epoch 86622: train loss: 0.06552111357450485\n",
            "Epoch 86623: train loss: 0.06551912426948547\n",
            "Epoch 86624: train loss: 0.06551710516214371\n",
            "Epoch 86625: train loss: 0.06551510840654373\n",
            "Epoch 86626: train loss: 0.06551310420036316\n",
            "Epoch 86627: train loss: 0.06551109999418259\n",
            "Epoch 86628: train loss: 0.06550910323858261\n",
            "Epoch 86629: train loss: 0.06550709158182144\n",
            "Epoch 86630: train loss: 0.06550507992506027\n",
            "Epoch 86631: train loss: 0.0655030757188797\n",
            "Epoch 86632: train loss: 0.06550109386444092\n",
            "Epoch 86633: train loss: 0.06549907475709915\n",
            "Epoch 86634: train loss: 0.06549707055091858\n",
            "Epoch 86635: train loss: 0.065495066344738\n",
            "Epoch 86636: train loss: 0.06549306958913803\n",
            "Epoch 86637: train loss: 0.06549106538295746\n",
            "Epoch 86638: train loss: 0.06548906117677689\n",
            "Epoch 86639: train loss: 0.06548706442117691\n",
            "Epoch 86640: train loss: 0.06548506766557693\n",
            "Epoch 86641: train loss: 0.06548305600881577\n",
            "Epoch 86642: train loss: 0.06548105925321579\n",
            "Epoch 86643: train loss: 0.06547906994819641\n",
            "Epoch 86644: train loss: 0.06547705829143524\n",
            "Epoch 86645: train loss: 0.06547505408525467\n",
            "Epoch 86646: train loss: 0.0654730573296547\n",
            "Epoch 86647: train loss: 0.06547106057405472\n",
            "Epoch 86648: train loss: 0.06546906381845474\n",
            "Epoch 86649: train loss: 0.06546706706285477\n",
            "Epoch 86650: train loss: 0.0654650554060936\n",
            "Epoch 86651: train loss: 0.06546307355165482\n",
            "Epoch 86652: train loss: 0.06546106934547424\n",
            "Epoch 86653: train loss: 0.06545906513929367\n",
            "Epoch 86654: train loss: 0.0654570683836937\n",
            "Epoch 86655: train loss: 0.06545507907867432\n",
            "Epoch 86656: train loss: 0.06545306742191315\n",
            "Epoch 86657: train loss: 0.06545107066631317\n",
            "Epoch 86658: train loss: 0.06544908136129379\n",
            "Epoch 86659: train loss: 0.06544706970453262\n",
            "Epoch 86660: train loss: 0.06544507294893265\n",
            "Epoch 86661: train loss: 0.06544307619333267\n",
            "Epoch 86662: train loss: 0.0654410868883133\n",
            "Epoch 86663: train loss: 0.06543908268213272\n",
            "Epoch 86664: train loss: 0.06543708592653275\n",
            "Epoch 86665: train loss: 0.06543508917093277\n",
            "Epoch 86666: train loss: 0.0654330849647522\n",
            "Epoch 86667: train loss: 0.06543109565973282\n",
            "Epoch 86668: train loss: 0.06542909145355225\n",
            "Epoch 86669: train loss: 0.06542709469795227\n",
            "Epoch 86670: train loss: 0.0654250979423523\n",
            "Epoch 86671: train loss: 0.06542308628559113\n",
            "Epoch 86672: train loss: 0.06542110443115234\n",
            "Epoch 86673: train loss: 0.06541910022497177\n",
            "Epoch 86674: train loss: 0.06541711091995239\n",
            "Epoch 86675: train loss: 0.06541510671377182\n",
            "Epoch 86676: train loss: 0.06541311740875244\n",
            "Epoch 86677: train loss: 0.06541112065315247\n",
            "Epoch 86678: train loss: 0.06540912389755249\n",
            "Epoch 86679: train loss: 0.06540711969137192\n",
            "Epoch 86680: train loss: 0.06540512293577194\n",
            "Epoch 86681: train loss: 0.06540312618017197\n",
            "Epoch 86682: train loss: 0.0654011219739914\n",
            "Epoch 86683: train loss: 0.06539914011955261\n",
            "Epoch 86684: train loss: 0.06539714336395264\n",
            "Epoch 86685: train loss: 0.06539514660835266\n",
            "Epoch 86686: train loss: 0.06539314985275269\n",
            "Epoch 86687: train loss: 0.06539115309715271\n",
            "Epoch 86688: train loss: 0.06538914889097214\n",
            "Epoch 86689: train loss: 0.06538714468479156\n",
            "Epoch 86690: train loss: 0.06538514792919159\n",
            "Epoch 86691: train loss: 0.06538315862417221\n",
            "Epoch 86692: train loss: 0.06538116931915283\n",
            "Epoch 86693: train loss: 0.06537916511297226\n",
            "Epoch 86694: train loss: 0.06537716835737228\n",
            "Epoch 86695: train loss: 0.0653751790523529\n",
            "Epoch 86696: train loss: 0.06537317484617233\n",
            "Epoch 86697: train loss: 0.06537118554115295\n",
            "Epoch 86698: train loss: 0.06536918133497238\n",
            "Epoch 86699: train loss: 0.0653671845793724\n",
            "Epoch 86700: train loss: 0.06536519527435303\n",
            "Epoch 86701: train loss: 0.06536319851875305\n",
            "Epoch 86702: train loss: 0.06536120176315308\n",
            "Epoch 86703: train loss: 0.0653592124581337\n",
            "Epoch 86704: train loss: 0.06535721570253372\n",
            "Epoch 86705: train loss: 0.06535521894693375\n",
            "Epoch 86706: train loss: 0.06535322964191437\n",
            "Epoch 86707: train loss: 0.0653512254357338\n",
            "Epoch 86708: train loss: 0.06534923613071442\n",
            "Epoch 86709: train loss: 0.06534723192453384\n",
            "Epoch 86710: train loss: 0.06534525007009506\n",
            "Epoch 86711: train loss: 0.06534325331449509\n",
            "Epoch 86712: train loss: 0.06534126400947571\n",
            "Epoch 86713: train loss: 0.06533926725387573\n",
            "Epoch 86714: train loss: 0.06533727049827576\n",
            "Epoch 86715: train loss: 0.06533526629209518\n",
            "Epoch 86716: train loss: 0.0653332769870758\n",
            "Epoch 86717: train loss: 0.06533128023147583\n",
            "Epoch 86718: train loss: 0.06532928347587585\n",
            "Epoch 86719: train loss: 0.06532728672027588\n",
            "Epoch 86720: train loss: 0.0653252974152565\n",
            "Epoch 86721: train loss: 0.06532331556081772\n",
            "Epoch 86722: train loss: 0.06532131135463715\n",
            "Epoch 86723: train loss: 0.06531931459903717\n",
            "Epoch 86724: train loss: 0.06531732529401779\n",
            "Epoch 86725: train loss: 0.06531531363725662\n",
            "Epoch 86726: train loss: 0.06531333178281784\n",
            "Epoch 86727: train loss: 0.06531134247779846\n",
            "Epoch 86728: train loss: 0.06530933827161789\n",
            "Epoch 86729: train loss: 0.06530734896659851\n",
            "Epoch 86730: train loss: 0.06530535221099854\n",
            "Epoch 86731: train loss: 0.06530336290597916\n",
            "Epoch 86732: train loss: 0.06530137360095978\n",
            "Epoch 86733: train loss: 0.0652993768453598\n",
            "Epoch 86734: train loss: 0.06529738008975983\n",
            "Epoch 86735: train loss: 0.06529539078474045\n",
            "Epoch 86736: train loss: 0.06529340147972107\n",
            "Epoch 86737: train loss: 0.0652914047241211\n",
            "Epoch 86738: train loss: 0.06528940796852112\n",
            "Epoch 86739: train loss: 0.06528741121292114\n",
            "Epoch 86740: train loss: 0.06528542190790176\n",
            "Epoch 86741: train loss: 0.06528342515230179\n",
            "Epoch 86742: train loss: 0.06528143584728241\n",
            "Epoch 86743: train loss: 0.06527945399284363\n",
            "Epoch 86744: train loss: 0.06527744978666306\n",
            "Epoch 86745: train loss: 0.06527545303106308\n",
            "Epoch 86746: train loss: 0.0652734637260437\n",
            "Epoch 86747: train loss: 0.06527148187160492\n",
            "Epoch 86748: train loss: 0.06526948511600494\n",
            "Epoch 86749: train loss: 0.06526748836040497\n",
            "Epoch 86750: train loss: 0.06526550650596619\n",
            "Epoch 86751: train loss: 0.06526350975036621\n",
            "Epoch 86752: train loss: 0.06526150554418564\n",
            "Epoch 86753: train loss: 0.06525952368974686\n",
            "Epoch 86754: train loss: 0.06525753438472748\n",
            "Epoch 86755: train loss: 0.0652555450797081\n",
            "Epoch 86756: train loss: 0.06525354087352753\n",
            "Epoch 86757: train loss: 0.06525155156850815\n",
            "Epoch 86758: train loss: 0.06524956226348877\n",
            "Epoch 86759: train loss: 0.06524757295846939\n",
            "Epoch 86760: train loss: 0.06524559110403061\n",
            "Epoch 86761: train loss: 0.06524360179901123\n",
            "Epoch 86762: train loss: 0.06524161249399185\n",
            "Epoch 86763: train loss: 0.06523962318897247\n",
            "Epoch 86764: train loss: 0.0652376338839531\n",
            "Epoch 86765: train loss: 0.06523564457893372\n",
            "Epoch 86766: train loss: 0.06523367017507553\n",
            "Epoch 86767: train loss: 0.06523167341947556\n",
            "Epoch 86768: train loss: 0.06522968411445618\n",
            "Epoch 86769: train loss: 0.0652276873588562\n",
            "Epoch 86770: train loss: 0.06522570550441742\n",
            "Epoch 86771: train loss: 0.06522372364997864\n",
            "Epoch 86772: train loss: 0.06522171944379807\n",
            "Epoch 86773: train loss: 0.06521974503993988\n",
            "Epoch 86774: train loss: 0.0652177557349205\n",
            "Epoch 86775: train loss: 0.06521576642990112\n",
            "Epoch 86776: train loss: 0.06521377712488174\n",
            "Epoch 86777: train loss: 0.06521178781986237\n",
            "Epoch 86778: train loss: 0.06520981341600418\n",
            "Epoch 86779: train loss: 0.0652078166604042\n",
            "Epoch 86780: train loss: 0.06520581245422363\n",
            "Epoch 86781: train loss: 0.06520383805036545\n",
            "Epoch 86782: train loss: 0.06520184874534607\n",
            "Epoch 86783: train loss: 0.06519985944032669\n",
            "Epoch 86784: train loss: 0.06519787758588791\n",
            "Epoch 86785: train loss: 0.06519588828086853\n",
            "Epoch 86786: train loss: 0.06519389897584915\n",
            "Epoch 86787: train loss: 0.06519190967082977\n",
            "Epoch 86788: train loss: 0.06518992781639099\n",
            "Epoch 86789: train loss: 0.06518793851137161\n",
            "Epoch 86790: train loss: 0.06518594920635223\n",
            "Epoch 86791: train loss: 0.06518396735191345\n",
            "Epoch 86792: train loss: 0.06518197804689407\n",
            "Epoch 86793: train loss: 0.0651799887418747\n",
            "Epoch 86794: train loss: 0.06517799943685532\n",
            "Epoch 86795: train loss: 0.06517601758241653\n",
            "Epoch 86796: train loss: 0.06517404317855835\n",
            "Epoch 86797: train loss: 0.06517204642295837\n",
            "Epoch 86798: train loss: 0.065170057117939\n",
            "Epoch 86799: train loss: 0.06516807526350021\n",
            "Epoch 86800: train loss: 0.06516608595848083\n",
            "Epoch 86801: train loss: 0.06516408920288086\n",
            "Epoch 86802: train loss: 0.06516211479902267\n",
            "Epoch 86803: train loss: 0.06516014039516449\n",
            "Epoch 86804: train loss: 0.06515815854072571\n",
            "Epoch 86805: train loss: 0.06515617668628693\n",
            "Epoch 86806: train loss: 0.06515419483184814\n",
            "Epoch 86807: train loss: 0.06515220552682877\n",
            "Epoch 86808: train loss: 0.06515022367238998\n",
            "Epoch 86809: train loss: 0.0651482343673706\n",
            "Epoch 86810: train loss: 0.06514625996351242\n",
            "Epoch 86811: train loss: 0.06514427810907364\n",
            "Epoch 86812: train loss: 0.06514229625463486\n",
            "Epoch 86813: train loss: 0.06514031440019608\n",
            "Epoch 86814: train loss: 0.0651383250951767\n",
            "Epoch 86815: train loss: 0.06513635069131851\n",
            "Epoch 86816: train loss: 0.06513436883687973\n",
            "Epoch 86817: train loss: 0.06513239443302155\n",
            "Epoch 86818: train loss: 0.06513041257858276\n",
            "Epoch 86819: train loss: 0.06512843817472458\n",
            "Epoch 86820: train loss: 0.0651264488697052\n",
            "Epoch 86821: train loss: 0.06512445956468582\n",
            "Epoch 86822: train loss: 0.06512248516082764\n",
            "Epoch 86823: train loss: 0.06512050330638885\n",
            "Epoch 86824: train loss: 0.06511852145195007\n",
            "Epoch 86825: train loss: 0.0651165321469307\n",
            "Epoch 86826: train loss: 0.06511455774307251\n",
            "Epoch 86827: train loss: 0.06511257588863373\n",
            "Epoch 86828: train loss: 0.06511059403419495\n",
            "Epoch 86829: train loss: 0.06510862708091736\n",
            "Epoch 86830: train loss: 0.06510662287473679\n",
            "Epoch 86831: train loss: 0.0651046559214592\n",
            "Epoch 86832: train loss: 0.06510267406702042\n",
            "Epoch 86833: train loss: 0.06510069966316223\n",
            "Epoch 86834: train loss: 0.06509871780872345\n",
            "Epoch 86835: train loss: 0.06509673595428467\n",
            "Epoch 86836: train loss: 0.06509474664926529\n",
            "Epoch 86837: train loss: 0.0650927871465683\n",
            "Epoch 86838: train loss: 0.06509078294038773\n",
            "Epoch 86839: train loss: 0.06508881598711014\n",
            "Epoch 86840: train loss: 0.06508683413267136\n",
            "Epoch 86841: train loss: 0.06508485227823257\n",
            "Epoch 86842: train loss: 0.06508287787437439\n",
            "Epoch 86843: train loss: 0.0650809034705162\n",
            "Epoch 86844: train loss: 0.06507892161607742\n",
            "Epoch 86845: train loss: 0.06507693976163864\n",
            "Epoch 86846: train loss: 0.06507495790719986\n",
            "Epoch 86847: train loss: 0.06507299095392227\n",
            "Epoch 86848: train loss: 0.06507100909948349\n",
            "Epoch 86849: train loss: 0.06506902724504471\n",
            "Epoch 86850: train loss: 0.06506705284118652\n",
            "Epoch 86851: train loss: 0.06506507843732834\n",
            "Epoch 86852: train loss: 0.06506308913230896\n",
            "Epoch 86853: train loss: 0.06506112962961197\n",
            "Epoch 86854: train loss: 0.06505914777517319\n",
            "Epoch 86855: train loss: 0.065057173371315\n",
            "Epoch 86856: train loss: 0.06505518406629562\n",
            "Epoch 86857: train loss: 0.06505320966243744\n",
            "Epoch 86858: train loss: 0.06505122780799866\n",
            "Epoch 86859: train loss: 0.06504926830530167\n",
            "Epoch 86860: train loss: 0.06504729390144348\n",
            "Epoch 86861: train loss: 0.0650453194975853\n",
            "Epoch 86862: train loss: 0.06504334509372711\n",
            "Epoch 86863: train loss: 0.06504136323928833\n",
            "Epoch 86864: train loss: 0.06503938883543015\n",
            "Epoch 86865: train loss: 0.06503741443157196\n",
            "Epoch 86866: train loss: 0.06503544002771378\n",
            "Epoch 86867: train loss: 0.06503347307443619\n",
            "Epoch 86868: train loss: 0.0650314912199974\n",
            "Epoch 86869: train loss: 0.06502950936555862\n",
            "Epoch 86870: train loss: 0.06502754241228104\n",
            "Epoch 86871: train loss: 0.06502556055784225\n",
            "Epoch 86872: train loss: 0.06502358615398407\n",
            "Epoch 86873: train loss: 0.06502161920070648\n",
            "Epoch 86874: train loss: 0.0650196447968483\n",
            "Epoch 86875: train loss: 0.06501766294240952\n",
            "Epoch 86876: train loss: 0.06501568853855133\n",
            "Epoch 86877: train loss: 0.06501371413469315\n",
            "Epoch 86878: train loss: 0.06501173973083496\n",
            "Epoch 86879: train loss: 0.06500975787639618\n",
            "Epoch 86880: train loss: 0.06500780582427979\n",
            "Epoch 86881: train loss: 0.0650058165192604\n",
            "Epoch 86882: train loss: 0.06500384211540222\n",
            "Epoch 86883: train loss: 0.06500186771154404\n",
            "Epoch 86884: train loss: 0.06499990075826645\n",
            "Epoch 86885: train loss: 0.06499793380498886\n",
            "Epoch 86886: train loss: 0.06499595940113068\n",
            "Epoch 86887: train loss: 0.0649939775466919\n",
            "Epoch 86888: train loss: 0.06499200314283371\n",
            "Epoch 86889: train loss: 0.06499002873897552\n",
            "Epoch 86890: train loss: 0.06498804688453674\n",
            "Epoch 86891: train loss: 0.06498607993125916\n",
            "Epoch 86892: train loss: 0.06498412042856216\n",
            "Epoch 86893: train loss: 0.06498213857412338\n",
            "Epoch 86894: train loss: 0.0649801641702652\n",
            "Epoch 86895: train loss: 0.06497818976640701\n",
            "Epoch 86896: train loss: 0.06497621536254883\n",
            "Epoch 86897: train loss: 0.06497425585985184\n",
            "Epoch 86898: train loss: 0.06497228145599365\n",
            "Epoch 86899: train loss: 0.06497029215097427\n",
            "Epoch 86900: train loss: 0.06496833264827728\n",
            "Epoch 86901: train loss: 0.0649663507938385\n",
            "Epoch 86902: train loss: 0.06496438384056091\n",
            "Epoch 86903: train loss: 0.06496240198612213\n",
            "Epoch 86904: train loss: 0.06496044248342514\n",
            "Epoch 86905: train loss: 0.06495845317840576\n",
            "Epoch 86906: train loss: 0.06495648622512817\n",
            "Epoch 86907: train loss: 0.06495451927185059\n",
            "Epoch 86908: train loss: 0.064952552318573\n",
            "Epoch 86909: train loss: 0.06495057046413422\n",
            "Epoch 86910: train loss: 0.06494860351085663\n",
            "Epoch 86911: train loss: 0.06494662165641785\n",
            "Epoch 86912: train loss: 0.06494464725255966\n",
            "Epoch 86913: train loss: 0.06494268029928207\n",
            "Epoch 86914: train loss: 0.06494071334600449\n",
            "Epoch 86915: train loss: 0.0649387389421463\n",
            "Epoch 86916: train loss: 0.06493675708770752\n",
            "Epoch 86917: train loss: 0.06493479758501053\n",
            "Epoch 86918: train loss: 0.06493281573057175\n",
            "Epoch 86919: train loss: 0.06493085622787476\n",
            "Epoch 86920: train loss: 0.06492888927459717\n",
            "Epoch 86921: train loss: 0.06492692232131958\n",
            "Epoch 86922: train loss: 0.0649249479174614\n",
            "Epoch 86923: train loss: 0.06492297351360321\n",
            "Epoch 86924: train loss: 0.06492099910974503\n",
            "Epoch 86925: train loss: 0.06491903215646744\n",
            "Epoch 86926: train loss: 0.06491706520318985\n",
            "Epoch 86927: train loss: 0.06491509824991226\n",
            "Epoch 86928: train loss: 0.06491313874721527\n",
            "Epoch 86929: train loss: 0.06491116434335709\n",
            "Epoch 86930: train loss: 0.0649091973900795\n",
            "Epoch 86931: train loss: 0.06490722298622131\n",
            "Epoch 86932: train loss: 0.06490526348352432\n",
            "Epoch 86933: train loss: 0.06490328907966614\n",
            "Epoch 86934: train loss: 0.06490132212638855\n",
            "Epoch 86935: train loss: 0.06489935517311096\n",
            "Epoch 86936: train loss: 0.06489738821983337\n",
            "Epoch 86937: train loss: 0.06489542871713638\n",
            "Epoch 86938: train loss: 0.0648934468626976\n",
            "Epoch 86939: train loss: 0.06489147245883942\n",
            "Epoch 86940: train loss: 0.06488951295614243\n",
            "Epoch 86941: train loss: 0.06488754600286484\n",
            "Epoch 86942: train loss: 0.06488557904958725\n",
            "Epoch 86943: train loss: 0.06488360464572906\n",
            "Epoch 86944: train loss: 0.06488163769245148\n",
            "Epoch 86945: train loss: 0.06487968564033508\n",
            "Epoch 86946: train loss: 0.0648777037858963\n",
            "Epoch 86947: train loss: 0.06487572938203812\n",
            "Epoch 86948: train loss: 0.06487376987934113\n",
            "Epoch 86949: train loss: 0.06487180292606354\n",
            "Epoch 86950: train loss: 0.06486984342336655\n",
            "Epoch 86951: train loss: 0.06486786156892776\n",
            "Epoch 86952: train loss: 0.06486590206623077\n",
            "Epoch 86953: train loss: 0.06486393511295319\n",
            "Epoch 86954: train loss: 0.064861960709095\n",
            "Epoch 86955: train loss: 0.06486000120639801\n",
            "Epoch 86956: train loss: 0.06485802680253983\n",
            "Epoch 86957: train loss: 0.06485605984926224\n",
            "Epoch 86958: train loss: 0.06485409289598465\n",
            "Epoch 86959: train loss: 0.06485212594270706\n",
            "Epoch 86960: train loss: 0.06485015898942947\n",
            "Epoch 86961: train loss: 0.06484819948673248\n",
            "Epoch 86962: train loss: 0.0648462250828743\n",
            "Epoch 86963: train loss: 0.06484425812959671\n",
            "Epoch 86964: train loss: 0.06484229117631912\n",
            "Epoch 86965: train loss: 0.06484033912420273\n",
            "Epoch 86966: train loss: 0.06483836472034454\n",
            "Epoch 86967: train loss: 0.06483638286590576\n",
            "Epoch 86968: train loss: 0.06483442336320877\n",
            "Epoch 86969: train loss: 0.06483246386051178\n",
            "Epoch 86970: train loss: 0.06483049690723419\n",
            "Epoch 86971: train loss: 0.0648285299539566\n",
            "Epoch 86972: train loss: 0.06482657045125961\n",
            "Epoch 86973: train loss: 0.06482459604740143\n",
            "Epoch 86974: train loss: 0.06482263654470444\n",
            "Epoch 86975: train loss: 0.06482066214084625\n",
            "Epoch 86976: train loss: 0.06481871008872986\n",
            "Epoch 86977: train loss: 0.06481675058603287\n",
            "Epoch 86978: train loss: 0.06481477618217468\n",
            "Epoch 86979: train loss: 0.06481282413005829\n",
            "Epoch 86980: train loss: 0.0648108497262001\n",
            "Epoch 86981: train loss: 0.06480888277292252\n",
            "Epoch 86982: train loss: 0.06480692327022552\n",
            "Epoch 86983: train loss: 0.06480497121810913\n",
            "Epoch 86984: train loss: 0.06480300426483154\n",
            "Epoch 86985: train loss: 0.06480105221271515\n",
            "Epoch 86986: train loss: 0.06479908525943756\n",
            "Epoch 86987: train loss: 0.06479712575674057\n",
            "Epoch 86988: train loss: 0.06479515880346298\n",
            "Epoch 86989: train loss: 0.0647931918501854\n",
            "Epoch 86990: train loss: 0.064791239798069\n",
            "Epoch 86991: train loss: 0.06478928029537201\n",
            "Epoch 86992: train loss: 0.06478730589151382\n",
            "Epoch 86993: train loss: 0.06478535383939743\n",
            "Epoch 86994: train loss: 0.06478338688611984\n",
            "Epoch 86995: train loss: 0.06478144228458405\n",
            "Epoch 86996: train loss: 0.06477947533130646\n",
            "Epoch 86997: train loss: 0.06477750837802887\n",
            "Epoch 86998: train loss: 0.06477554142475128\n",
            "Epoch 86999: train loss: 0.0647735744714737\n",
            "Epoch 87000: train loss: 0.0647716224193573\n",
            "Epoch 87001: train loss: 0.06476966291666031\n",
            "Epoch 87002: train loss: 0.06476770341396332\n",
            "Epoch 87003: train loss: 0.06476574391126633\n",
            "Epoch 87004: train loss: 0.06476379185914993\n",
            "Epoch 87005: train loss: 0.06476183235645294\n",
            "Epoch 87006: train loss: 0.06475987285375595\n",
            "Epoch 87007: train loss: 0.06475792080163956\n",
            "Epoch 87008: train loss: 0.06475595384836197\n",
            "Epoch 87009: train loss: 0.06475400179624557\n",
            "Epoch 87010: train loss: 0.06475204229354858\n",
            "Epoch 87011: train loss: 0.06475008279085159\n",
            "Epoch 87012: train loss: 0.0647481307387352\n",
            "Epoch 87013: train loss: 0.0647461786866188\n",
            "Epoch 87014: train loss: 0.06474421918392181\n",
            "Epoch 87015: train loss: 0.06474226713180542\n",
            "Epoch 87016: train loss: 0.06474031507968903\n",
            "Epoch 87017: train loss: 0.06473835557699203\n",
            "Epoch 87018: train loss: 0.06473640352487564\n",
            "Epoch 87019: train loss: 0.06473443657159805\n",
            "Epoch 87020: train loss: 0.06473249942064285\n",
            "Epoch 87021: train loss: 0.06473053991794586\n",
            "Epoch 87022: train loss: 0.06472858786582947\n",
            "Epoch 87023: train loss: 0.06472662836313248\n",
            "Epoch 87024: train loss: 0.06472468376159668\n",
            "Epoch 87025: train loss: 0.06472272425889969\n",
            "Epoch 87026: train loss: 0.0647207722067833\n",
            "Epoch 87027: train loss: 0.0647188276052475\n",
            "Epoch 87028: train loss: 0.06471685320138931\n",
            "Epoch 87029: train loss: 0.06471490859985352\n",
            "Epoch 87030: train loss: 0.06471296399831772\n",
            "Epoch 87031: train loss: 0.06471101194620132\n",
            "Epoch 87032: train loss: 0.06470904499292374\n",
            "Epoch 87033: train loss: 0.06470709294080734\n",
            "Epoch 87034: train loss: 0.06470514088869095\n",
            "Epoch 87035: train loss: 0.06470318883657455\n",
            "Epoch 87036: train loss: 0.06470124423503876\n",
            "Epoch 87037: train loss: 0.06469929218292236\n",
            "Epoch 87038: train loss: 0.06469731777906418\n",
            "Epoch 87039: train loss: 0.06469536572694778\n",
            "Epoch 87040: train loss: 0.06469342112541199\n",
            "Epoch 87041: train loss: 0.06469147652387619\n",
            "Epoch 87042: train loss: 0.0646895244717598\n",
            "Epoch 87043: train loss: 0.0646875649690628\n",
            "Epoch 87044: train loss: 0.06468561291694641\n",
            "Epoch 87045: train loss: 0.06468365341424942\n",
            "Epoch 87046: train loss: 0.06468170136213303\n",
            "Epoch 87047: train loss: 0.06467975676059723\n",
            "Epoch 87048: train loss: 0.06467780470848083\n",
            "Epoch 87049: train loss: 0.06467584520578384\n",
            "Epoch 87050: train loss: 0.06467389315366745\n",
            "Epoch 87051: train loss: 0.06467194110155106\n",
            "Epoch 87052: train loss: 0.06466999650001526\n",
            "Epoch 87053: train loss: 0.06466804444789886\n",
            "Epoch 87054: train loss: 0.06466609239578247\n",
            "Epoch 87055: train loss: 0.06466414034366608\n",
            "Epoch 87056: train loss: 0.06466218084096909\n",
            "Epoch 87057: train loss: 0.06466023623943329\n",
            "Epoch 87058: train loss: 0.0646582841873169\n",
            "Epoch 87059: train loss: 0.0646563395857811\n",
            "Epoch 87060: train loss: 0.0646543800830841\n",
            "Epoch 87061: train loss: 0.06465242803096771\n",
            "Epoch 87062: train loss: 0.06465048342943192\n",
            "Epoch 87063: train loss: 0.06464853137731552\n",
            "Epoch 87064: train loss: 0.06464658677577972\n",
            "Epoch 87065: train loss: 0.06464463472366333\n",
            "Epoch 87066: train loss: 0.06464267522096634\n",
            "Epoch 87067: train loss: 0.06464073061943054\n",
            "Epoch 87068: train loss: 0.06463877111673355\n",
            "Epoch 87069: train loss: 0.06463682651519775\n",
            "Epoch 87070: train loss: 0.06463488191366196\n",
            "Epoch 87071: train loss: 0.06463292986154556\n",
            "Epoch 87072: train loss: 0.06463097780942917\n",
            "Epoch 87073: train loss: 0.06462904065847397\n",
            "Epoch 87074: train loss: 0.06462708115577698\n",
            "Epoch 87075: train loss: 0.06462513655424118\n",
            "Epoch 87076: train loss: 0.06462317705154419\n",
            "Epoch 87077: train loss: 0.06462123245000839\n",
            "Epoch 87078: train loss: 0.064619280397892\n",
            "Epoch 87079: train loss: 0.0646173283457756\n",
            "Epoch 87080: train loss: 0.06461537629365921\n",
            "Epoch 87081: train loss: 0.06461342424154282\n",
            "Epoch 87082: train loss: 0.06461147218942642\n",
            "Epoch 87083: train loss: 0.06460952758789062\n",
            "Epoch 87084: train loss: 0.06460758298635483\n",
            "Epoch 87085: train loss: 0.06460563093423843\n",
            "Epoch 87086: train loss: 0.06460368633270264\n",
            "Epoch 87087: train loss: 0.06460172683000565\n",
            "Epoch 87088: train loss: 0.06459978222846985\n",
            "Epoch 87089: train loss: 0.06459783017635345\n",
            "Epoch 87090: train loss: 0.06459587812423706\n",
            "Epoch 87091: train loss: 0.06459392607212067\n",
            "Epoch 87092: train loss: 0.06459198147058487\n",
            "Epoch 87093: train loss: 0.06459003686904907\n",
            "Epoch 87094: train loss: 0.06458809226751328\n",
            "Epoch 87095: train loss: 0.06458614021539688\n",
            "Epoch 87096: train loss: 0.06458418071269989\n",
            "Epoch 87097: train loss: 0.06458223611116409\n",
            "Epoch 87098: train loss: 0.0645802915096283\n",
            "Epoch 87099: train loss: 0.0645783469080925\n",
            "Epoch 87100: train loss: 0.06457638740539551\n",
            "Epoch 87101: train loss: 0.06457444280385971\n",
            "Epoch 87102: train loss: 0.06457249820232391\n",
            "Epoch 87103: train loss: 0.06457053869962692\n",
            "Epoch 87104: train loss: 0.06456860154867172\n",
            "Epoch 87105: train loss: 0.06456664949655533\n",
            "Epoch 87106: train loss: 0.06456469744443893\n",
            "Epoch 87107: train loss: 0.06456276029348373\n",
            "Epoch 87108: train loss: 0.06456080079078674\n",
            "Epoch 87109: train loss: 0.06455886363983154\n",
            "Epoch 87110: train loss: 0.06455691158771515\n",
            "Epoch 87111: train loss: 0.06455496698617935\n",
            "Epoch 87112: train loss: 0.06455301493406296\n",
            "Epoch 87113: train loss: 0.06455107778310776\n",
            "Epoch 87114: train loss: 0.06454912573099136\n",
            "Epoch 87115: train loss: 0.06454718112945557\n",
            "Epoch 87116: train loss: 0.06454523652791977\n",
            "Epoch 87117: train loss: 0.06454329192638397\n",
            "Epoch 87118: train loss: 0.06454134732484818\n",
            "Epoch 87119: train loss: 0.06453940272331238\n",
            "Epoch 87120: train loss: 0.06453746557235718\n",
            "Epoch 87121: train loss: 0.06453549861907959\n",
            "Epoch 87122: train loss: 0.06453357636928558\n",
            "Epoch 87123: train loss: 0.06453161686658859\n",
            "Epoch 87124: train loss: 0.06452967971563339\n",
            "Epoch 87125: train loss: 0.064527727663517\n",
            "Epoch 87126: train loss: 0.0645257756114006\n",
            "Epoch 87127: train loss: 0.0645238384604454\n",
            "Epoch 87128: train loss: 0.06452188640832901\n",
            "Epoch 87129: train loss: 0.06451994180679321\n",
            "Epoch 87130: train loss: 0.06451801210641861\n",
            "Epoch 87131: train loss: 0.06451606005430222\n",
            "Epoch 87132: train loss: 0.06451412290334702\n",
            "Epoch 87133: train loss: 0.06451217830181122\n",
            "Epoch 87134: train loss: 0.06451023370027542\n",
            "Epoch 87135: train loss: 0.06450830399990082\n",
            "Epoch 87136: train loss: 0.06450634449720383\n",
            "Epoch 87137: train loss: 0.06450442224740982\n",
            "Epoch 87138: train loss: 0.06450247019529343\n",
            "Epoch 87139: train loss: 0.06450053304433823\n",
            "Epoch 87140: train loss: 0.06449858844280243\n",
            "Epoch 87141: train loss: 0.06449665129184723\n",
            "Epoch 87142: train loss: 0.06449470669031143\n",
            "Epoch 87143: train loss: 0.06449275463819504\n",
            "Epoch 87144: train loss: 0.06449082493782043\n",
            "Epoch 87145: train loss: 0.06448888033628464\n",
            "Epoch 87146: train loss: 0.06448693573474884\n",
            "Epoch 87147: train loss: 0.06448500603437424\n",
            "Epoch 87148: train loss: 0.06448305398225784\n",
            "Epoch 87149: train loss: 0.06448110938072205\n",
            "Epoch 87150: train loss: 0.06447917968034744\n",
            "Epoch 87151: train loss: 0.06447723507881165\n",
            "Epoch 87152: train loss: 0.06447529047727585\n",
            "Epoch 87153: train loss: 0.06447335332632065\n",
            "Epoch 87154: train loss: 0.06447140872478485\n",
            "Epoch 87155: train loss: 0.06446946412324905\n",
            "Epoch 87156: train loss: 0.06446752697229385\n",
            "Epoch 87157: train loss: 0.06446558982133865\n",
            "Epoch 87158: train loss: 0.06446363776922226\n",
            "Epoch 87159: train loss: 0.06446170061826706\n",
            "Epoch 87160: train loss: 0.06445976346731186\n",
            "Epoch 87161: train loss: 0.06445782631635666\n",
            "Epoch 87162: train loss: 0.06445588916540146\n",
            "Epoch 87163: train loss: 0.06445394456386566\n",
            "Epoch 87164: train loss: 0.06445199251174927\n",
            "Epoch 87165: train loss: 0.06445006281137466\n",
            "Epoch 87166: train loss: 0.06444811820983887\n",
            "Epoch 87167: train loss: 0.06444618105888367\n",
            "Epoch 87168: train loss: 0.06444424390792847\n",
            "Epoch 87169: train loss: 0.06444229930639267\n",
            "Epoch 87170: train loss: 0.06444036960601807\n",
            "Epoch 87171: train loss: 0.06443842500448227\n",
            "Epoch 87172: train loss: 0.06443648040294647\n",
            "Epoch 87173: train loss: 0.06443453580141068\n",
            "Epoch 87174: train loss: 0.06443259865045547\n",
            "Epoch 87175: train loss: 0.06443066895008087\n",
            "Epoch 87176: train loss: 0.06442872434854507\n",
            "Epoch 87177: train loss: 0.06442678719758987\n",
            "Epoch 87178: train loss: 0.06442484259605408\n",
            "Epoch 87179: train loss: 0.06442290544509888\n",
            "Epoch 87180: train loss: 0.06442096829414368\n",
            "Epoch 87181: train loss: 0.06441903114318848\n",
            "Epoch 87182: train loss: 0.06441708654165268\n",
            "Epoch 87183: train loss: 0.06441514194011688\n",
            "Epoch 87184: train loss: 0.06441321223974228\n",
            "Epoch 87185: train loss: 0.06441127508878708\n",
            "Epoch 87186: train loss: 0.06440933793783188\n",
            "Epoch 87187: train loss: 0.06440738588571548\n",
            "Epoch 87188: train loss: 0.06440545618534088\n",
            "Epoch 87189: train loss: 0.06440351903438568\n",
            "Epoch 87190: train loss: 0.06440158188343048\n",
            "Epoch 87191: train loss: 0.06439963728189468\n",
            "Epoch 87192: train loss: 0.06439770013093948\n",
            "Epoch 87193: train loss: 0.06439574807882309\n",
            "Epoch 87194: train loss: 0.06439381837844849\n",
            "Epoch 87195: train loss: 0.06439188122749329\n",
            "Epoch 87196: train loss: 0.06438995152711868\n",
            "Epoch 87197: train loss: 0.06438801437616348\n",
            "Epoch 87198: train loss: 0.06438606977462769\n",
            "Epoch 87199: train loss: 0.06438413262367249\n",
            "Epoch 87200: train loss: 0.06438219547271729\n",
            "Epoch 87201: train loss: 0.06438025832176208\n",
            "Epoch 87202: train loss: 0.06437831372022629\n",
            "Epoch 87203: train loss: 0.06437637656927109\n",
            "Epoch 87204: train loss: 0.06437443941831589\n",
            "Epoch 87205: train loss: 0.06437250226736069\n",
            "Epoch 87206: train loss: 0.06437056511640549\n",
            "Epoch 87207: train loss: 0.06436862051486969\n",
            "Epoch 87208: train loss: 0.06436669081449509\n",
            "Epoch 87209: train loss: 0.06436475366353989\n",
            "Epoch 87210: train loss: 0.06436281651258469\n",
            "Epoch 87211: train loss: 0.06436088681221008\n",
            "Epoch 87212: train loss: 0.06435894221067429\n",
            "Epoch 87213: train loss: 0.06435700505971909\n",
            "Epoch 87214: train loss: 0.06435507535934448\n",
            "Epoch 87215: train loss: 0.06435313075780869\n",
            "Epoch 87216: train loss: 0.06435120105743408\n",
            "Epoch 87217: train loss: 0.06434926390647888\n",
            "Epoch 87218: train loss: 0.06434733420610428\n",
            "Epoch 87219: train loss: 0.06434539705514908\n",
            "Epoch 87220: train loss: 0.06434346735477448\n",
            "Epoch 87221: train loss: 0.06434153020381927\n",
            "Epoch 87222: train loss: 0.06433960795402527\n",
            "Epoch 87223: train loss: 0.06433767825365067\n",
            "Epoch 87224: train loss: 0.06433573365211487\n",
            "Epoch 87225: train loss: 0.06433379650115967\n",
            "Epoch 87226: train loss: 0.06433187425136566\n",
            "Epoch 87227: train loss: 0.06432994455099106\n",
            "Epoch 87228: train loss: 0.06432801485061646\n",
            "Epoch 87229: train loss: 0.06432608515024185\n",
            "Epoch 87230: train loss: 0.06432414799928665\n",
            "Epoch 87231: train loss: 0.06432221084833145\n",
            "Epoch 87232: train loss: 0.06432028859853745\n",
            "Epoch 87233: train loss: 0.06431835144758224\n",
            "Epoch 87234: train loss: 0.06431641429662704\n",
            "Epoch 87235: train loss: 0.06431448459625244\n",
            "Epoch 87236: train loss: 0.06431254744529724\n",
            "Epoch 87237: train loss: 0.06431061774492264\n",
            "Epoch 87238: train loss: 0.06430868804454803\n",
            "Epoch 87239: train loss: 0.06430675089359283\n",
            "Epoch 87240: train loss: 0.06430482864379883\n",
            "Epoch 87241: train loss: 0.06430289894342422\n",
            "Epoch 87242: train loss: 0.06430096179246902\n",
            "Epoch 87243: train loss: 0.06429903209209442\n",
            "Epoch 87244: train loss: 0.06429710239171982\n",
            "Epoch 87245: train loss: 0.06429517269134521\n",
            "Epoch 87246: train loss: 0.06429323554039001\n",
            "Epoch 87247: train loss: 0.06429129838943481\n",
            "Epoch 87248: train loss: 0.06428937613964081\n",
            "Epoch 87249: train loss: 0.0642874464392662\n",
            "Epoch 87250: train loss: 0.064285509288311\n",
            "Epoch 87251: train loss: 0.0642835795879364\n",
            "Epoch 87252: train loss: 0.0642816573381424\n",
            "Epoch 87253: train loss: 0.06427973508834839\n",
            "Epoch 87254: train loss: 0.06427779048681259\n",
            "Epoch 87255: train loss: 0.06427586078643799\n",
            "Epoch 87256: train loss: 0.06427392363548279\n",
            "Epoch 87257: train loss: 0.06427200138568878\n",
            "Epoch 87258: train loss: 0.06427007168531418\n",
            "Epoch 87259: train loss: 0.06426814198493958\n",
            "Epoch 87260: train loss: 0.06426620483398438\n",
            "Epoch 87261: train loss: 0.06426428258419037\n",
            "Epoch 87262: train loss: 0.06426236033439636\n",
            "Epoch 87263: train loss: 0.06426041573286057\n",
            "Epoch 87264: train loss: 0.06425849348306656\n",
            "Epoch 87265: train loss: 0.06425654888153076\n",
            "Epoch 87266: train loss: 0.06425462663173676\n",
            "Epoch 87267: train loss: 0.06425270438194275\n",
            "Epoch 87268: train loss: 0.06425077468156815\n",
            "Epoch 87269: train loss: 0.06424884498119354\n",
            "Epoch 87270: train loss: 0.06424691528081894\n",
            "Epoch 87271: train loss: 0.06424498558044434\n",
            "Epoch 87272: train loss: 0.06424305588006973\n",
            "Epoch 87273: train loss: 0.06424113363027573\n",
            "Epoch 87274: train loss: 0.06423919647932053\n",
            "Epoch 87275: train loss: 0.06423727422952652\n",
            "Epoch 87276: train loss: 0.06423534452915192\n",
            "Epoch 87277: train loss: 0.06423342227935791\n",
            "Epoch 87278: train loss: 0.0642314925789833\n",
            "Epoch 87279: train loss: 0.0642295628786087\n",
            "Epoch 87280: train loss: 0.0642276331782341\n",
            "Epoch 87281: train loss: 0.0642257034778595\n",
            "Epoch 87282: train loss: 0.06422378122806549\n",
            "Epoch 87283: train loss: 0.06422185897827148\n",
            "Epoch 87284: train loss: 0.06421992182731628\n",
            "Epoch 87285: train loss: 0.06421799212694168\n",
            "Epoch 87286: train loss: 0.06421606987714767\n",
            "Epoch 87287: train loss: 0.06421413272619247\n",
            "Epoch 87288: train loss: 0.06421221047639847\n",
            "Epoch 87289: train loss: 0.06421028822660446\n",
            "Epoch 87290: train loss: 0.06420835852622986\n",
            "Epoch 87291: train loss: 0.06420643627643585\n",
            "Epoch 87292: train loss: 0.06420449912548065\n",
            "Epoch 87293: train loss: 0.06420258432626724\n",
            "Epoch 87294: train loss: 0.06420065462589264\n",
            "Epoch 87295: train loss: 0.06419871747493744\n",
            "Epoch 87296: train loss: 0.06419679522514343\n",
            "Epoch 87297: train loss: 0.06419488042593002\n",
            "Epoch 87298: train loss: 0.06419293582439423\n",
            "Epoch 87299: train loss: 0.06419101357460022\n",
            "Epoch 87300: train loss: 0.06418908387422562\n",
            "Epoch 87301: train loss: 0.06418716907501221\n",
            "Epoch 87302: train loss: 0.0641852393746376\n",
            "Epoch 87303: train loss: 0.0641833022236824\n",
            "Epoch 87304: train loss: 0.064181387424469\n",
            "Epoch 87305: train loss: 0.06417945772409439\n",
            "Epoch 87306: train loss: 0.06417752802371979\n",
            "Epoch 87307: train loss: 0.06417562067508698\n",
            "Epoch 87308: train loss: 0.06417368352413177\n",
            "Epoch 87309: train loss: 0.06417175382375717\n",
            "Epoch 87310: train loss: 0.06416983902454376\n",
            "Epoch 87311: train loss: 0.06416791677474976\n",
            "Epoch 87312: train loss: 0.06416599452495575\n",
            "Epoch 87313: train loss: 0.06416404992341995\n",
            "Epoch 87314: train loss: 0.06416213512420654\n",
            "Epoch 87315: train loss: 0.06416021287441254\n",
            "Epoch 87316: train loss: 0.06415829062461853\n",
            "Epoch 87317: train loss: 0.06415636837482452\n",
            "Epoch 87318: train loss: 0.06415443867444992\n",
            "Epoch 87319: train loss: 0.06415252387523651\n",
            "Epoch 87320: train loss: 0.0641506090760231\n",
            "Epoch 87321: train loss: 0.0641486793756485\n",
            "Epoch 87322: train loss: 0.0641467496752739\n",
            "Epoch 87323: train loss: 0.06414482742547989\n",
            "Epoch 87324: train loss: 0.06414290517568588\n",
            "Epoch 87325: train loss: 0.06414099037647247\n",
            "Epoch 87326: train loss: 0.06413906812667847\n",
            "Epoch 87327: train loss: 0.06413713842630386\n",
            "Epoch 87328: train loss: 0.06413522362709045\n",
            "Epoch 87329: train loss: 0.06413330137729645\n",
            "Epoch 87330: train loss: 0.06413137912750244\n",
            "Epoch 87331: train loss: 0.06412945687770844\n",
            "Epoch 87332: train loss: 0.06412752717733383\n",
            "Epoch 87333: train loss: 0.06412560492753983\n",
            "Epoch 87334: train loss: 0.06412368267774582\n",
            "Epoch 87335: train loss: 0.06412176042795181\n",
            "Epoch 87336: train loss: 0.0641198456287384\n",
            "Epoch 87337: train loss: 0.0641179084777832\n",
            "Epoch 87338: train loss: 0.06411600112915039\n",
            "Epoch 87339: train loss: 0.06411407142877579\n",
            "Epoch 87340: train loss: 0.06411214917898178\n",
            "Epoch 87341: train loss: 0.06411022692918777\n",
            "Epoch 87342: train loss: 0.06410831212997437\n",
            "Epoch 87343: train loss: 0.06410638242959976\n",
            "Epoch 87344: train loss: 0.06410446017980576\n",
            "Epoch 87345: train loss: 0.06410253047943115\n",
            "Epoch 87346: train loss: 0.06410062313079834\n",
            "Epoch 87347: train loss: 0.06409870088100433\n",
            "Epoch 87348: train loss: 0.06409678608179092\n",
            "Epoch 87349: train loss: 0.06409485638141632\n",
            "Epoch 87350: train loss: 0.06409294158220291\n",
            "Epoch 87351: train loss: 0.06409100443124771\n",
            "Epoch 87352: train loss: 0.0640890970826149\n",
            "Epoch 87353: train loss: 0.06408717483282089\n",
            "Epoch 87354: train loss: 0.06408524513244629\n",
            "Epoch 87355: train loss: 0.06408333033323288\n",
            "Epoch 87356: train loss: 0.06408140808343887\n",
            "Epoch 87357: train loss: 0.06407948583364487\n",
            "Epoch 87358: train loss: 0.06407757103443146\n",
            "Epoch 87359: train loss: 0.06407565623521805\n",
            "Epoch 87360: train loss: 0.06407372653484344\n",
            "Epoch 87361: train loss: 0.06407181173563004\n",
            "Epoch 87362: train loss: 0.06406988203525543\n",
            "Epoch 87363: train loss: 0.06406795978546143\n",
            "Epoch 87364: train loss: 0.06406604498624802\n",
            "Epoch 87365: train loss: 0.06406412273645401\n",
            "Epoch 87366: train loss: 0.0640622079372406\n",
            "Epoch 87367: train loss: 0.0640602856874466\n",
            "Epoch 87368: train loss: 0.06405837088823318\n",
            "Epoch 87369: train loss: 0.06405644118785858\n",
            "Epoch 87370: train loss: 0.06405452638864517\n",
            "Epoch 87371: train loss: 0.06405261158943176\n",
            "Epoch 87372: train loss: 0.06405068933963776\n",
            "Epoch 87373: train loss: 0.06404876708984375\n",
            "Epoch 87374: train loss: 0.06404684484004974\n",
            "Epoch 87375: train loss: 0.06404493004083633\n",
            "Epoch 87376: train loss: 0.06404300779104233\n",
            "Epoch 87377: train loss: 0.06404109299182892\n",
            "Epoch 87378: train loss: 0.06403917819261551\n",
            "Epoch 87379: train loss: 0.0640372633934021\n",
            "Epoch 87380: train loss: 0.0640353411436081\n",
            "Epoch 87381: train loss: 0.06403341889381409\n",
            "Epoch 87382: train loss: 0.06403149664402008\n",
            "Epoch 87383: train loss: 0.06402958184480667\n",
            "Epoch 87384: train loss: 0.06402766704559326\n",
            "Epoch 87385: train loss: 0.06402575224637985\n",
            "Epoch 87386: train loss: 0.06402382999658585\n",
            "Epoch 87387: train loss: 0.06402191519737244\n",
            "Epoch 87388: train loss: 0.06402000039815903\n",
            "Epoch 87389: train loss: 0.06401808559894562\n",
            "Epoch 87390: train loss: 0.06401617079973221\n",
            "Epoch 87391: train loss: 0.0640142410993576\n",
            "Epoch 87392: train loss: 0.0640123263001442\n",
            "Epoch 87393: train loss: 0.06401041895151138\n",
            "Epoch 87394: train loss: 0.06400851160287857\n",
            "Epoch 87395: train loss: 0.06400658935308456\n",
            "Epoch 87396: train loss: 0.06400466710329056\n",
            "Epoch 87397: train loss: 0.06400275975465775\n",
            "Epoch 87398: train loss: 0.06400083750486374\n",
            "Epoch 87399: train loss: 0.06399891525506973\n",
            "Epoch 87400: train loss: 0.06399700045585632\n",
            "Epoch 87401: train loss: 0.06399507820606232\n",
            "Epoch 87402: train loss: 0.0639931708574295\n",
            "Epoch 87403: train loss: 0.0639912560582161\n",
            "Epoch 87404: train loss: 0.06398934870958328\n",
            "Epoch 87405: train loss: 0.06398743391036987\n",
            "Epoch 87406: train loss: 0.06398551166057587\n",
            "Epoch 87407: train loss: 0.06398360431194305\n",
            "Epoch 87408: train loss: 0.06398168206214905\n",
            "Epoch 87409: train loss: 0.06397977471351624\n",
            "Epoch 87410: train loss: 0.06397785991430283\n",
            "Epoch 87411: train loss: 0.06397594511508942\n",
            "Epoch 87412: train loss: 0.06397403031587601\n",
            "Epoch 87413: train loss: 0.0639721229672432\n",
            "Epoch 87414: train loss: 0.06397020816802979\n",
            "Epoch 87415: train loss: 0.06396830081939697\n",
            "Epoch 87416: train loss: 0.06396639347076416\n",
            "Epoch 87417: train loss: 0.06396448612213135\n",
            "Epoch 87418: train loss: 0.06396257132291794\n",
            "Epoch 87419: train loss: 0.06396066397428513\n",
            "Epoch 87420: train loss: 0.06395875662565231\n",
            "Epoch 87421: train loss: 0.0639568418264389\n",
            "Epoch 87422: train loss: 0.06395493447780609\n",
            "Epoch 87423: train loss: 0.06395302712917328\n",
            "Epoch 87424: train loss: 0.06395111978054047\n",
            "Epoch 87425: train loss: 0.06394921988248825\n",
            "Epoch 87426: train loss: 0.06394730508327484\n",
            "Epoch 87427: train loss: 0.06394540518522263\n",
            "Epoch 87428: train loss: 0.06394349783658981\n",
            "Epoch 87429: train loss: 0.063941590487957\n",
            "Epoch 87430: train loss: 0.06393968313932419\n",
            "Epoch 87431: train loss: 0.06393777579069138\n",
            "Epoch 87432: train loss: 0.06393587589263916\n",
            "Epoch 87433: train loss: 0.06393396854400635\n",
            "Epoch 87434: train loss: 0.06393205374479294\n",
            "Epoch 87435: train loss: 0.06393014639616013\n",
            "Epoch 87436: train loss: 0.06392824649810791\n",
            "Epoch 87437: train loss: 0.0639263391494751\n",
            "Epoch 87438: train loss: 0.06392443180084229\n",
            "Epoch 87439: train loss: 0.06392252445220947\n",
            "Epoch 87440: train loss: 0.06392061710357666\n",
            "Epoch 87441: train loss: 0.06391870975494385\n",
            "Epoch 87442: train loss: 0.06391680985689163\n",
            "Epoch 87443: train loss: 0.06391490250825882\n",
            "Epoch 87444: train loss: 0.0639130026102066\n",
            "Epoch 87445: train loss: 0.06391109526157379\n",
            "Epoch 87446: train loss: 0.06390919536352158\n",
            "Epoch 87447: train loss: 0.06390728801488876\n",
            "Epoch 87448: train loss: 0.06390538811683655\n",
            "Epoch 87449: train loss: 0.06390346586704254\n",
            "Epoch 87450: train loss: 0.06390156596899033\n",
            "Epoch 87451: train loss: 0.06389965116977692\n",
            "Epoch 87452: train loss: 0.0638977661728859\n",
            "Epoch 87453: train loss: 0.06389585137367249\n",
            "Epoch 87454: train loss: 0.06389394402503967\n",
            "Epoch 87455: train loss: 0.06389204412698746\n",
            "Epoch 87456: train loss: 0.06389014422893524\n",
            "Epoch 87457: train loss: 0.06388824433088303\n",
            "Epoch 87458: train loss: 0.06388632953166962\n",
            "Epoch 87459: train loss: 0.0638844221830368\n",
            "Epoch 87460: train loss: 0.06388252228498459\n",
            "Epoch 87461: train loss: 0.06388062238693237\n",
            "Epoch 87462: train loss: 0.06387872248888016\n",
            "Epoch 87463: train loss: 0.06387681514024734\n",
            "Epoch 87464: train loss: 0.06387491524219513\n",
            "Epoch 87465: train loss: 0.06387301534414291\n",
            "Epoch 87466: train loss: 0.0638711079955101\n",
            "Epoch 87467: train loss: 0.06386920064687729\n",
            "Epoch 87468: train loss: 0.06386729329824448\n",
            "Epoch 87469: train loss: 0.06386539340019226\n",
            "Epoch 87470: train loss: 0.06386349350214005\n",
            "Epoch 87471: train loss: 0.06386158615350723\n",
            "Epoch 87472: train loss: 0.06385968625545502\n",
            "Epoch 87473: train loss: 0.0638577789068222\n",
            "Epoch 87474: train loss: 0.06385587155818939\n",
            "Epoch 87475: train loss: 0.06385397166013718\n",
            "Epoch 87476: train loss: 0.06385208666324615\n",
            "Epoch 87477: train loss: 0.06385017931461334\n",
            "Epoch 87478: train loss: 0.06384827941656113\n",
            "Epoch 87479: train loss: 0.06384636461734772\n",
            "Epoch 87480: train loss: 0.0638444796204567\n",
            "Epoch 87481: train loss: 0.06384255737066269\n",
            "Epoch 87482: train loss: 0.06384066492319107\n",
            "Epoch 87483: train loss: 0.06383875757455826\n",
            "Epoch 87484: train loss: 0.06383685767650604\n",
            "Epoch 87485: train loss: 0.06383496522903442\n",
            "Epoch 87486: train loss: 0.06383305788040161\n",
            "Epoch 87487: train loss: 0.0638311505317688\n",
            "Epoch 87488: train loss: 0.06382925808429718\n",
            "Epoch 87489: train loss: 0.06382735818624496\n",
            "Epoch 87490: train loss: 0.06382545828819275\n",
            "Epoch 87491: train loss: 0.06382355093955994\n",
            "Epoch 87492: train loss: 0.06382166594266891\n",
            "Epoch 87493: train loss: 0.0638197511434555\n",
            "Epoch 87494: train loss: 0.06381785869598389\n",
            "Epoch 87495: train loss: 0.06381595134735107\n",
            "Epoch 87496: train loss: 0.06381404399871826\n",
            "Epoch 87497: train loss: 0.06381214410066605\n",
            "Epoch 87498: train loss: 0.06381023675203323\n",
            "Epoch 87499: train loss: 0.06380835175514221\n",
            "Epoch 87500: train loss: 0.0638064369559288\n",
            "Epoch 87501: train loss: 0.06380453705787659\n",
            "Epoch 87502: train loss: 0.06380264461040497\n",
            "Epoch 87503: train loss: 0.06380074471235275\n",
            "Epoch 87504: train loss: 0.06379884481430054\n",
            "Epoch 87505: train loss: 0.06379694491624832\n",
            "Epoch 87506: train loss: 0.0637950450181961\n",
            "Epoch 87507: train loss: 0.06379314512014389\n",
            "Epoch 87508: train loss: 0.06379125267267227\n",
            "Epoch 87509: train loss: 0.06378936022520065\n",
            "Epoch 87510: train loss: 0.06378746032714844\n",
            "Epoch 87511: train loss: 0.06378556042909622\n",
            "Epoch 87512: train loss: 0.063783660531044\n",
            "Epoch 87513: train loss: 0.06378176063299179\n",
            "Epoch 87514: train loss: 0.06377986818552017\n",
            "Epoch 87515: train loss: 0.06377796083688736\n",
            "Epoch 87516: train loss: 0.06377606838941574\n",
            "Epoch 87517: train loss: 0.06377417594194412\n",
            "Epoch 87518: train loss: 0.0637722760438919\n",
            "Epoch 87519: train loss: 0.06377037614583969\n",
            "Epoch 87520: train loss: 0.06376848369836807\n",
            "Epoch 87521: train loss: 0.06376659125089645\n",
            "Epoch 87522: train loss: 0.06376469135284424\n",
            "Epoch 87523: train loss: 0.06376279890537262\n",
            "Epoch 87524: train loss: 0.06376089155673981\n",
            "Epoch 87525: train loss: 0.06375899165868759\n",
            "Epoch 87526: train loss: 0.06375710666179657\n",
            "Epoch 87527: train loss: 0.06375521421432495\n",
            "Epoch 87528: train loss: 0.06375330686569214\n",
            "Epoch 87529: train loss: 0.06375142186880112\n",
            "Epoch 87530: train loss: 0.0637495145201683\n",
            "Epoch 87531: train loss: 0.06374762207269669\n",
            "Epoch 87532: train loss: 0.06374572962522507\n",
            "Epoch 87533: train loss: 0.06374383717775345\n",
            "Epoch 87534: train loss: 0.06374192982912064\n",
            "Epoch 87535: train loss: 0.06374004483222961\n",
            "Epoch 87536: train loss: 0.0637381449341774\n",
            "Epoch 87537: train loss: 0.06373624503612518\n",
            "Epoch 87538: train loss: 0.06373434513807297\n",
            "Epoch 87539: train loss: 0.06373246014118195\n",
            "Epoch 87540: train loss: 0.06373056769371033\n",
            "Epoch 87541: train loss: 0.06372865289449692\n",
            "Epoch 87542: train loss: 0.0637267604470253\n",
            "Epoch 87543: train loss: 0.06372487545013428\n",
            "Epoch 87544: train loss: 0.06372297555208206\n",
            "Epoch 87545: train loss: 0.06372106820344925\n",
            "Epoch 87546: train loss: 0.06371918320655823\n",
            "Epoch 87547: train loss: 0.06371728330850601\n",
            "Epoch 87548: train loss: 0.0637153908610344\n",
            "Epoch 87549: train loss: 0.06371351331472397\n",
            "Epoch 87550: train loss: 0.06371160596609116\n",
            "Epoch 87551: train loss: 0.06370970606803894\n",
            "Epoch 87552: train loss: 0.06370781362056732\n",
            "Epoch 87553: train loss: 0.0637059137225151\n",
            "Epoch 87554: train loss: 0.06370402872562408\n",
            "Epoch 87555: train loss: 0.06370213627815247\n",
            "Epoch 87556: train loss: 0.06370023638010025\n",
            "Epoch 87557: train loss: 0.06369833648204803\n",
            "Epoch 87558: train loss: 0.06369644403457642\n",
            "Epoch 87559: train loss: 0.0636945590376854\n",
            "Epoch 87560: train loss: 0.06369265168905258\n",
            "Epoch 87561: train loss: 0.06369075924158096\n",
            "Epoch 87562: train loss: 0.06368887424468994\n",
            "Epoch 87563: train loss: 0.06368696689605713\n",
            "Epoch 87564: train loss: 0.06368507444858551\n",
            "Epoch 87565: train loss: 0.06368318945169449\n",
            "Epoch 87566: train loss: 0.06368128210306168\n",
            "Epoch 87567: train loss: 0.06367939710617065\n",
            "Epoch 87568: train loss: 0.06367750465869904\n",
            "Epoch 87569: train loss: 0.06367560476064682\n",
            "Epoch 87570: train loss: 0.0636737123131752\n",
            "Epoch 87571: train loss: 0.06367181986570358\n",
            "Epoch 87572: train loss: 0.06366992741823196\n",
            "Epoch 87573: train loss: 0.06366804242134094\n",
            "Epoch 87574: train loss: 0.06366614252328873\n",
            "Epoch 87575: train loss: 0.06366424262523651\n",
            "Epoch 87576: train loss: 0.06366235017776489\n",
            "Epoch 87577: train loss: 0.06366045773029327\n",
            "Epoch 87578: train loss: 0.06365857273340225\n",
            "Epoch 87579: train loss: 0.06365668028593063\n",
            "Epoch 87580: train loss: 0.06365478038787842\n",
            "Epoch 87581: train loss: 0.0636528879404068\n",
            "Epoch 87582: train loss: 0.06365098804235458\n",
            "Epoch 87583: train loss: 0.06364910304546356\n",
            "Epoch 87584: train loss: 0.06364721804857254\n",
            "Epoch 87585: train loss: 0.06364531815052032\n",
            "Epoch 87586: train loss: 0.06364341825246811\n",
            "Epoch 87587: train loss: 0.06364152580499649\n",
            "Epoch 87588: train loss: 0.06363964080810547\n",
            "Epoch 87589: train loss: 0.06363774836063385\n",
            "Epoch 87590: train loss: 0.06363584846258163\n",
            "Epoch 87591: train loss: 0.06363395601511002\n",
            "Epoch 87592: train loss: 0.0636320635676384\n",
            "Epoch 87593: train loss: 0.06363017112016678\n",
            "Epoch 87594: train loss: 0.06362827867269516\n",
            "Epoch 87595: train loss: 0.06362639367580414\n",
            "Epoch 87596: train loss: 0.06362450122833252\n",
            "Epoch 87597: train loss: 0.0636226013302803\n",
            "Epoch 87598: train loss: 0.06362070888280869\n",
            "Epoch 87599: train loss: 0.06361882388591766\n",
            "Epoch 87600: train loss: 0.06361693888902664\n",
            "Epoch 87601: train loss: 0.06361503899097443\n",
            "Epoch 87602: train loss: 0.0636131539940834\n",
            "Epoch 87603: train loss: 0.06361125409603119\n",
            "Epoch 87604: train loss: 0.06360936909914017\n",
            "Epoch 87605: train loss: 0.06360747665166855\n",
            "Epoch 87606: train loss: 0.06360558420419693\n",
            "Epoch 87607: train loss: 0.06360368430614471\n",
            "Epoch 87608: train loss: 0.06360180675983429\n",
            "Epoch 87609: train loss: 0.06359991431236267\n",
            "Epoch 87610: train loss: 0.06359802186489105\n",
            "Epoch 87611: train loss: 0.06359612941741943\n",
            "Epoch 87612: train loss: 0.06359422951936722\n",
            "Epoch 87613: train loss: 0.0635923445224762\n",
            "Epoch 87614: train loss: 0.06359045207500458\n",
            "Epoch 87615: train loss: 0.06358855962753296\n",
            "Epoch 87616: train loss: 0.06358666718006134\n",
            "Epoch 87617: train loss: 0.06358477473258972\n",
            "Epoch 87618: train loss: 0.0635828897356987\n",
            "Epoch 87619: train loss: 0.06358099728822708\n",
            "Epoch 87620: train loss: 0.06357909739017487\n",
            "Epoch 87621: train loss: 0.06357721239328384\n",
            "Epoch 87622: train loss: 0.06357531249523163\n",
            "Epoch 87623: train loss: 0.0635734274983406\n",
            "Epoch 87624: train loss: 0.06357153505086899\n",
            "Epoch 87625: train loss: 0.06356965005397797\n",
            "Epoch 87626: train loss: 0.06356775760650635\n",
            "Epoch 87627: train loss: 0.06356586515903473\n",
            "Epoch 87628: train loss: 0.06356397271156311\n",
            "Epoch 87629: train loss: 0.06356208771467209\n",
            "Epoch 87630: train loss: 0.06356020271778107\n",
            "Epoch 87631: train loss: 0.06355830281972885\n",
            "Epoch 87632: train loss: 0.06355642527341843\n",
            "Epoch 87633: train loss: 0.06355452537536621\n",
            "Epoch 87634: train loss: 0.06355265527963638\n",
            "Epoch 87635: train loss: 0.06355074048042297\n",
            "Epoch 87636: train loss: 0.06354885548353195\n",
            "Epoch 87637: train loss: 0.06354697048664093\n",
            "Epoch 87638: train loss: 0.0635450929403305\n",
            "Epoch 87639: train loss: 0.06354320049285889\n",
            "Epoch 87640: train loss: 0.06354131549596786\n",
            "Epoch 87641: train loss: 0.06353943049907684\n",
            "Epoch 87642: train loss: 0.06353753805160522\n",
            "Epoch 87643: train loss: 0.0635356530547142\n",
            "Epoch 87644: train loss: 0.06353376805782318\n",
            "Epoch 87645: train loss: 0.06353188306093216\n",
            "Epoch 87646: train loss: 0.06352999061346054\n",
            "Epoch 87647: train loss: 0.06352810561656952\n",
            "Epoch 87648: train loss: 0.0635262206196785\n",
            "Epoch 87649: train loss: 0.06352434307336807\n",
            "Epoch 87650: train loss: 0.06352245807647705\n",
            "Epoch 87651: train loss: 0.06352057307958603\n",
            "Epoch 87652: train loss: 0.0635186955332756\n",
            "Epoch 87653: train loss: 0.06351681798696518\n",
            "Epoch 87654: train loss: 0.06351493299007416\n",
            "Epoch 87655: train loss: 0.06351306289434433\n",
            "Epoch 87656: train loss: 0.06351117789745331\n",
            "Epoch 87657: train loss: 0.06350929290056229\n",
            "Epoch 87658: train loss: 0.06350740790367126\n",
            "Epoch 87659: train loss: 0.06350553780794144\n",
            "Epoch 87660: train loss: 0.06350365281105042\n",
            "Epoch 87661: train loss: 0.06350178271532059\n",
            "Epoch 87662: train loss: 0.06349989026784897\n",
            "Epoch 87663: train loss: 0.06349802017211914\n",
            "Epoch 87664: train loss: 0.06349613517522812\n",
            "Epoch 87665: train loss: 0.0634942501783371\n",
            "Epoch 87666: train loss: 0.06349237263202667\n",
            "Epoch 87667: train loss: 0.06349048763513565\n",
            "Epoch 87668: train loss: 0.06348860263824463\n",
            "Epoch 87669: train loss: 0.0634867250919342\n",
            "Epoch 87670: train loss: 0.06348486244678497\n",
            "Epoch 87671: train loss: 0.06348298490047455\n",
            "Epoch 87672: train loss: 0.06348109245300293\n",
            "Epoch 87673: train loss: 0.06347920745611191\n",
            "Epoch 87674: train loss: 0.06347732990980148\n",
            "Epoch 87675: train loss: 0.06347545981407166\n",
            "Epoch 87676: train loss: 0.06347358226776123\n",
            "Epoch 87677: train loss: 0.06347169727087021\n",
            "Epoch 87678: train loss: 0.06346982717514038\n",
            "Epoch 87679: train loss: 0.06346794217824936\n",
            "Epoch 87680: train loss: 0.06346606463193893\n",
            "Epoch 87681: train loss: 0.06346418708562851\n",
            "Epoch 87682: train loss: 0.06346230953931808\n",
            "Epoch 87683: train loss: 0.06346043199300766\n",
            "Epoch 87684: train loss: 0.06345854699611664\n",
            "Epoch 87685: train loss: 0.06345667690038681\n",
            "Epoch 87686: train loss: 0.06345480680465698\n",
            "Epoch 87687: train loss: 0.06345291435718536\n",
            "Epoch 87688: train loss: 0.06345102936029434\n",
            "Epoch 87689: train loss: 0.06344916671514511\n",
            "Epoch 87690: train loss: 0.06344727426767349\n",
            "Epoch 87691: train loss: 0.06344539672136307\n",
            "Epoch 87692: train loss: 0.06344351917505264\n",
            "Epoch 87693: train loss: 0.06344164162874222\n",
            "Epoch 87694: train loss: 0.0634397566318512\n",
            "Epoch 87695: train loss: 0.06343788653612137\n",
            "Epoch 87696: train loss: 0.06343600153923035\n",
            "Epoch 87697: train loss: 0.06343412399291992\n",
            "Epoch 87698: train loss: 0.0634322464466095\n",
            "Epoch 87699: train loss: 0.06343036890029907\n",
            "Epoch 87700: train loss: 0.06342849135398865\n",
            "Epoch 87701: train loss: 0.06342660635709763\n",
            "Epoch 87702: train loss: 0.0634247362613678\n",
            "Epoch 87703: train loss: 0.06342285126447678\n",
            "Epoch 87704: train loss: 0.06342098861932755\n",
            "Epoch 87705: train loss: 0.06341909617185593\n",
            "Epoch 87706: train loss: 0.0634172260761261\n",
            "Epoch 87707: train loss: 0.06341534852981567\n",
            "Epoch 87708: train loss: 0.06341347843408585\n",
            "Epoch 87709: train loss: 0.06341160088777542\n",
            "Epoch 87710: train loss: 0.063409723341465\n",
            "Epoch 87711: train loss: 0.06340785324573517\n",
            "Epoch 87712: train loss: 0.06340596824884415\n",
            "Epoch 87713: train loss: 0.06340409070253372\n",
            "Epoch 87714: train loss: 0.0634022131562233\n",
            "Epoch 87715: train loss: 0.06340035051107407\n",
            "Epoch 87716: train loss: 0.06339847296476364\n",
            "Epoch 87717: train loss: 0.06339658796787262\n",
            "Epoch 87718: train loss: 0.06339473277330399\n",
            "Epoch 87719: train loss: 0.06339284032583237\n",
            "Epoch 87720: train loss: 0.06339097023010254\n",
            "Epoch 87721: train loss: 0.06338910013437271\n",
            "Epoch 87722: train loss: 0.06338721513748169\n",
            "Epoch 87723: train loss: 0.06338533759117126\n",
            "Epoch 87724: train loss: 0.06338346749544144\n",
            "Epoch 87725: train loss: 0.06338158994913101\n",
            "Epoch 87726: train loss: 0.06337972730398178\n",
            "Epoch 87727: train loss: 0.06337784230709076\n",
            "Epoch 87728: train loss: 0.06337597966194153\n",
            "Epoch 87729: train loss: 0.0633740946650505\n",
            "Epoch 87730: train loss: 0.06337221711874008\n",
            "Epoch 87731: train loss: 0.06337034702301025\n",
            "Epoch 87732: train loss: 0.06336846947669983\n",
            "Epoch 87733: train loss: 0.0633666068315506\n",
            "Epoch 87734: train loss: 0.06336472183465958\n",
            "Epoch 87735: train loss: 0.06336285173892975\n",
            "Epoch 87736: train loss: 0.06336097419261932\n",
            "Epoch 87737: train loss: 0.0633590966463089\n",
            "Epoch 87738: train loss: 0.06335723400115967\n",
            "Epoch 87739: train loss: 0.06335535645484924\n",
            "Epoch 87740: train loss: 0.06335347890853882\n",
            "Epoch 87741: train loss: 0.0633516013622284\n",
            "Epoch 87742: train loss: 0.06334973126649857\n",
            "Epoch 87743: train loss: 0.06334785372018814\n",
            "Epoch 87744: train loss: 0.06334598362445831\n",
            "Epoch 87745: train loss: 0.06334411352872849\n",
            "Epoch 87746: train loss: 0.06334223598241806\n",
            "Epoch 87747: train loss: 0.06334035843610764\n",
            "Epoch 87748: train loss: 0.06333848834037781\n",
            "Epoch 87749: train loss: 0.06333661824464798\n",
            "Epoch 87750: train loss: 0.06333474069833755\n",
            "Epoch 87751: train loss: 0.06333287060260773\n",
            "Epoch 87752: train loss: 0.0633309930562973\n",
            "Epoch 87753: train loss: 0.06332912296056747\n",
            "Epoch 87754: train loss: 0.06332724541425705\n",
            "Epoch 87755: train loss: 0.06332536786794662\n",
            "Epoch 87756: train loss: 0.0633235052227974\n",
            "Epoch 87757: train loss: 0.06332162767648697\n",
            "Epoch 87758: train loss: 0.06331975758075714\n",
            "Epoch 87759: train loss: 0.06331788748502731\n",
            "Epoch 87760: train loss: 0.06331600993871689\n",
            "Epoch 87761: train loss: 0.06331413984298706\n",
            "Epoch 87762: train loss: 0.06331226229667664\n",
            "Epoch 87763: train loss: 0.06331039220094681\n",
            "Epoch 87764: train loss: 0.06330851465463638\n",
            "Epoch 87765: train loss: 0.06330664455890656\n",
            "Epoch 87766: train loss: 0.06330478191375732\n",
            "Epoch 87767: train loss: 0.0633029118180275\n",
            "Epoch 87768: train loss: 0.06330104172229767\n",
            "Epoch 87769: train loss: 0.06329915672540665\n",
            "Epoch 87770: train loss: 0.06329728662967682\n",
            "Epoch 87771: train loss: 0.0632954090833664\n",
            "Epoch 87772: train loss: 0.06329353153705597\n",
            "Epoch 87773: train loss: 0.06329166889190674\n",
            "Epoch 87774: train loss: 0.06328979134559631\n",
            "Epoch 87775: train loss: 0.06328792870044708\n",
            "Epoch 87776: train loss: 0.06328605115413666\n",
            "Epoch 87777: train loss: 0.06328418850898743\n",
            "Epoch 87778: train loss: 0.063282310962677\n",
            "Epoch 87779: train loss: 0.06328044831752777\n",
            "Epoch 87780: train loss: 0.06327856332063675\n",
            "Epoch 87781: train loss: 0.06327670067548752\n",
            "Epoch 87782: train loss: 0.0632748231291771\n",
            "Epoch 87783: train loss: 0.06327296793460846\n",
            "Epoch 87784: train loss: 0.06327108293771744\n",
            "Epoch 87785: train loss: 0.06326920539140701\n",
            "Epoch 87786: train loss: 0.06326733529567719\n",
            "Epoch 87787: train loss: 0.06326545774936676\n",
            "Epoch 87788: train loss: 0.06326360255479813\n",
            "Epoch 87789: train loss: 0.0632617175579071\n",
            "Epoch 87790: train loss: 0.06325986981391907\n",
            "Epoch 87791: train loss: 0.06325797736644745\n",
            "Epoch 87792: train loss: 0.06325611472129822\n",
            "Epoch 87793: train loss: 0.06325423717498779\n",
            "Epoch 87794: train loss: 0.06325238198041916\n",
            "Epoch 87795: train loss: 0.06325049698352814\n",
            "Epoch 87796: train loss: 0.0632486343383789\n",
            "Epoch 87797: train loss: 0.06324676424264908\n",
            "Epoch 87798: train loss: 0.06324489414691925\n",
            "Epoch 87799: train loss: 0.06324301660060883\n",
            "Epoch 87800: train loss: 0.0632411539554596\n",
            "Epoch 87801: train loss: 0.06323927640914917\n",
            "Epoch 87802: train loss: 0.06323740631341934\n",
            "Epoch 87803: train loss: 0.06323553621768951\n",
            "Epoch 87804: train loss: 0.06323366612195969\n",
            "Epoch 87805: train loss: 0.06323179602622986\n",
            "Epoch 87806: train loss: 0.06322993338108063\n",
            "Epoch 87807: train loss: 0.0632280632853508\n",
            "Epoch 87808: train loss: 0.06322618573904037\n",
            "Epoch 87809: train loss: 0.06322431564331055\n",
            "Epoch 87810: train loss: 0.06322244554758072\n",
            "Epoch 87811: train loss: 0.06322058290243149\n",
            "Epoch 87812: train loss: 0.06321871280670166\n",
            "Epoch 87813: train loss: 0.06321684271097183\n",
            "Epoch 87814: train loss: 0.063214972615242\n",
            "Epoch 87815: train loss: 0.06321309506893158\n",
            "Epoch 87816: train loss: 0.06321123987436295\n",
            "Epoch 87817: train loss: 0.06320936232805252\n",
            "Epoch 87818: train loss: 0.06320749223232269\n",
            "Epoch 87819: train loss: 0.06320562213659286\n",
            "Epoch 87820: train loss: 0.06320375949144363\n",
            "Epoch 87821: train loss: 0.06320188194513321\n",
            "Epoch 87822: train loss: 0.06320001929998398\n",
            "Epoch 87823: train loss: 0.06319814920425415\n",
            "Epoch 87824: train loss: 0.06319627910852432\n",
            "Epoch 87825: train loss: 0.0631944090127945\n",
            "Epoch 87826: train loss: 0.06319254636764526\n",
            "Epoch 87827: train loss: 0.06319067627191544\n",
            "Epoch 87828: train loss: 0.06318880617618561\n",
            "Epoch 87829: train loss: 0.06318694353103638\n",
            "Epoch 87830: train loss: 0.06318508088588715\n",
            "Epoch 87831: train loss: 0.06318320333957672\n",
            "Epoch 87832: train loss: 0.06318134069442749\n",
            "Epoch 87833: train loss: 0.06317947059869766\n",
            "Epoch 87834: train loss: 0.06317760795354843\n",
            "Epoch 87835: train loss: 0.0631757378578186\n",
            "Epoch 87836: train loss: 0.06317387521266937\n",
            "Epoch 87837: train loss: 0.06317200511693954\n",
            "Epoch 87838: train loss: 0.06317014247179031\n",
            "Epoch 87839: train loss: 0.06316827237606049\n",
            "Epoch 87840: train loss: 0.06316640973091125\n",
            "Epoch 87841: train loss: 0.06316453963518143\n",
            "Epoch 87842: train loss: 0.0631626769900322\n",
            "Epoch 87843: train loss: 0.06316079944372177\n",
            "Epoch 87844: train loss: 0.06315894424915314\n",
            "Epoch 87845: train loss: 0.06315707415342331\n",
            "Epoch 87846: train loss: 0.06315520405769348\n",
            "Epoch 87847: train loss: 0.06315334141254425\n",
            "Epoch 87848: train loss: 0.06315147876739502\n",
            "Epoch 87849: train loss: 0.06314961612224579\n",
            "Epoch 87850: train loss: 0.06314775347709656\n",
            "Epoch 87851: train loss: 0.06314588338136673\n",
            "Epoch 87852: train loss: 0.0631440281867981\n",
            "Epoch 87853: train loss: 0.06314215809106827\n",
            "Epoch 87854: train loss: 0.06314030289649963\n",
            "Epoch 87855: train loss: 0.063138447701931\n",
            "Epoch 87856: train loss: 0.06313657760620117\n",
            "Epoch 87857: train loss: 0.06313470751047134\n",
            "Epoch 87858: train loss: 0.0631328597664833\n",
            "Epoch 87859: train loss: 0.06313097476959229\n",
            "Epoch 87860: train loss: 0.06312913447618484\n",
            "Epoch 87861: train loss: 0.06312727183103561\n",
            "Epoch 87862: train loss: 0.06312540918588638\n",
            "Epoch 87863: train loss: 0.06312354654073715\n",
            "Epoch 87864: train loss: 0.06312168389558792\n",
            "Epoch 87865: train loss: 0.06311982125043869\n",
            "Epoch 87866: train loss: 0.06311795115470886\n",
            "Epoch 87867: train loss: 0.06311610341072083\n",
            "Epoch 87868: train loss: 0.063114233314991\n",
            "Epoch 87869: train loss: 0.06311237066984177\n",
            "Epoch 87870: train loss: 0.06311053037643433\n",
            "Epoch 87871: train loss: 0.0631086453795433\n",
            "Epoch 87872: train loss: 0.06310679018497467\n",
            "Epoch 87873: train loss: 0.06310492008924484\n",
            "Epoch 87874: train loss: 0.06310306489467621\n",
            "Epoch 87875: train loss: 0.06310119479894638\n",
            "Epoch 87876: train loss: 0.06309934705495834\n",
            "Epoch 87877: train loss: 0.06309747695922852\n",
            "Epoch 87878: train loss: 0.06309562176465988\n",
            "Epoch 87879: train loss: 0.06309375166893005\n",
            "Epoch 87880: train loss: 0.06309190392494202\n",
            "Epoch 87881: train loss: 0.06309004873037338\n",
            "Epoch 87882: train loss: 0.06308818608522415\n",
            "Epoch 87883: train loss: 0.06308633089065552\n",
            "Epoch 87884: train loss: 0.06308446079492569\n",
            "Epoch 87885: train loss: 0.06308259814977646\n",
            "Epoch 87886: train loss: 0.06308075040578842\n",
            "Epoch 87887: train loss: 0.0630788803100586\n",
            "Epoch 87888: train loss: 0.06307703256607056\n",
            "Epoch 87889: train loss: 0.06307516992092133\n",
            "Epoch 87890: train loss: 0.0630732998251915\n",
            "Epoch 87891: train loss: 0.06307143718004227\n",
            "Epoch 87892: train loss: 0.06306958943605423\n",
            "Epoch 87893: train loss: 0.063067726790905\n",
            "Epoch 87894: train loss: 0.06306587904691696\n",
            "Epoch 87895: train loss: 0.06306401640176773\n",
            "Epoch 87896: train loss: 0.0630621537566185\n",
            "Epoch 87897: train loss: 0.06306030601263046\n",
            "Epoch 87898: train loss: 0.06305845081806183\n",
            "Epoch 87899: train loss: 0.0630565881729126\n",
            "Epoch 87900: train loss: 0.06305472552776337\n",
            "Epoch 87901: train loss: 0.06305286288261414\n",
            "Epoch 87902: train loss: 0.0630510076880455\n",
            "Epoch 87903: train loss: 0.06304915249347687\n",
            "Epoch 87904: train loss: 0.06304728984832764\n",
            "Epoch 87905: train loss: 0.0630454421043396\n",
            "Epoch 87906: train loss: 0.06304358690977097\n",
            "Epoch 87907: train loss: 0.06304173171520233\n",
            "Epoch 87908: train loss: 0.0630398765206337\n",
            "Epoch 87909: train loss: 0.06303802877664566\n",
            "Epoch 87910: train loss: 0.06303617358207703\n",
            "Epoch 87911: train loss: 0.06303432583808899\n",
            "Epoch 87912: train loss: 0.06303246319293976\n",
            "Epoch 87913: train loss: 0.06303060054779053\n",
            "Epoch 87914: train loss: 0.06302875280380249\n",
            "Epoch 87915: train loss: 0.06302690505981445\n",
            "Epoch 87916: train loss: 0.06302503496408463\n",
            "Epoch 87917: train loss: 0.06302319467067719\n",
            "Epoch 87918: train loss: 0.06302133947610855\n",
            "Epoch 87919: train loss: 0.06301948428153992\n",
            "Epoch 87920: train loss: 0.06301763653755188\n",
            "Epoch 87921: train loss: 0.06301577389240265\n",
            "Epoch 87922: train loss: 0.06301391124725342\n",
            "Epoch 87923: train loss: 0.06301207095384598\n",
            "Epoch 87924: train loss: 0.06301020830869675\n",
            "Epoch 87925: train loss: 0.06300834566354752\n",
            "Epoch 87926: train loss: 0.06300650537014008\n",
            "Epoch 87927: train loss: 0.06300465017557144\n",
            "Epoch 87928: train loss: 0.063002809882164\n",
            "Epoch 87929: train loss: 0.06300095468759537\n",
            "Epoch 87930: train loss: 0.06299910694360733\n",
            "Epoch 87931: train loss: 0.0629972591996193\n",
            "Epoch 87932: train loss: 0.06299540400505066\n",
            "Epoch 87933: train loss: 0.06299355626106262\n",
            "Epoch 87934: train loss: 0.06299169361591339\n",
            "Epoch 87935: train loss: 0.06298985332250595\n",
            "Epoch 87936: train loss: 0.06298800557851791\n",
            "Epoch 87937: train loss: 0.06298615783452988\n",
            "Epoch 87938: train loss: 0.06298430263996124\n",
            "Epoch 87939: train loss: 0.06298244744539261\n",
            "Epoch 87940: train loss: 0.06298060715198517\n",
            "Epoch 87941: train loss: 0.06297875195741653\n",
            "Epoch 87942: train loss: 0.0629769042134285\n",
            "Epoch 87943: train loss: 0.06297504901885986\n",
            "Epoch 87944: train loss: 0.06297321617603302\n",
            "Epoch 87945: train loss: 0.06297135353088379\n",
            "Epoch 87946: train loss: 0.06296949833631516\n",
            "Epoch 87947: train loss: 0.06296765059232712\n",
            "Epoch 87948: train loss: 0.06296580284833908\n",
            "Epoch 87949: train loss: 0.06296396255493164\n",
            "Epoch 87950: train loss: 0.062962107360363\n",
            "Epoch 87951: train loss: 0.06296026706695557\n",
            "Epoch 87952: train loss: 0.06295841187238693\n",
            "Epoch 87953: train loss: 0.0629565566778183\n",
            "Epoch 87954: train loss: 0.06295470893383026\n",
            "Epoch 87955: train loss: 0.06295286118984222\n",
            "Epoch 87956: train loss: 0.06295102089643478\n",
            "Epoch 87957: train loss: 0.06294916570186615\n",
            "Epoch 87958: train loss: 0.06294731050729752\n",
            "Epoch 87959: train loss: 0.06294547766447067\n",
            "Epoch 87960: train loss: 0.06294362992048264\n",
            "Epoch 87961: train loss: 0.06294175982475281\n",
            "Epoch 87962: train loss: 0.06293992698192596\n",
            "Epoch 87963: train loss: 0.06293808668851852\n",
            "Epoch 87964: train loss: 0.06293623894453049\n",
            "Epoch 87965: train loss: 0.06293437629938126\n",
            "Epoch 87966: train loss: 0.06293254345655441\n",
            "Epoch 87967: train loss: 0.06293069571256638\n",
            "Epoch 87968: train loss: 0.06292884796857834\n",
            "Epoch 87969: train loss: 0.0629269927740097\n",
            "Epoch 87970: train loss: 0.06292514503002167\n",
            "Epoch 87971: train loss: 0.06292331218719482\n",
            "Epoch 87972: train loss: 0.06292145699262619\n",
            "Epoch 87973: train loss: 0.06291960179805756\n",
            "Epoch 87974: train loss: 0.06291776150465012\n",
            "Epoch 87975: train loss: 0.06291592121124268\n",
            "Epoch 87976: train loss: 0.06291406601667404\n",
            "Epoch 87977: train loss: 0.062912218272686\n",
            "Epoch 87978: train loss: 0.06291037052869797\n",
            "Epoch 87979: train loss: 0.06290853023529053\n",
            "Epoch 87980: train loss: 0.0629066750407219\n",
            "Epoch 87981: train loss: 0.06290483474731445\n",
            "Epoch 87982: train loss: 0.06290298700332642\n",
            "Epoch 87983: train loss: 0.06290113925933838\n",
            "Epoch 87984: train loss: 0.06289929896593094\n",
            "Epoch 87985: train loss: 0.0628974512219429\n",
            "Epoch 87986: train loss: 0.06289559602737427\n",
            "Epoch 87987: train loss: 0.06289374828338623\n",
            "Epoch 87988: train loss: 0.06289190798997879\n",
            "Epoch 87989: train loss: 0.06289006769657135\n",
            "Epoch 87990: train loss: 0.06288821250200272\n",
            "Epoch 87991: train loss: 0.06288636475801468\n",
            "Epoch 87992: train loss: 0.06288452446460724\n",
            "Epoch 87993: train loss: 0.0628826692700386\n",
            "Epoch 87994: train loss: 0.06288082897663116\n",
            "Epoch 87995: train loss: 0.06287898123264313\n",
            "Epoch 87996: train loss: 0.06287713348865509\n",
            "Epoch 87997: train loss: 0.06287528574466705\n",
            "Epoch 87998: train loss: 0.06287344545125961\n",
            "Epoch 87999: train loss: 0.06287160515785217\n",
            "Epoch 88000: train loss: 0.06286975741386414\n",
            "Epoch 88001: train loss: 0.0628679171204567\n",
            "Epoch 88002: train loss: 0.06286607682704926\n",
            "Epoch 88003: train loss: 0.06286422908306122\n",
            "Epoch 88004: train loss: 0.06286238878965378\n",
            "Epoch 88005: train loss: 0.06286054849624634\n",
            "Epoch 88006: train loss: 0.0628587082028389\n",
            "Epoch 88007: train loss: 0.06285686045885086\n",
            "Epoch 88008: train loss: 0.06285502016544342\n",
            "Epoch 88009: train loss: 0.06285317242145538\n",
            "Epoch 88010: train loss: 0.06285133957862854\n",
            "Epoch 88011: train loss: 0.0628494843840599\n",
            "Epoch 88012: train loss: 0.06284765154123306\n",
            "Epoch 88013: train loss: 0.06284581869840622\n",
            "Epoch 88014: train loss: 0.06284397095441818\n",
            "Epoch 88015: train loss: 0.06284212321043015\n",
            "Epoch 88016: train loss: 0.0628402829170227\n",
            "Epoch 88017: train loss: 0.06283843517303467\n",
            "Epoch 88018: train loss: 0.06283659487962723\n",
            "Epoch 88019: train loss: 0.06283475458621979\n",
            "Epoch 88020: train loss: 0.06283290684223175\n",
            "Epoch 88021: train loss: 0.06283106654882431\n",
            "Epoch 88022: train loss: 0.06282922625541687\n",
            "Epoch 88023: train loss: 0.06282738596200943\n",
            "Epoch 88024: train loss: 0.06282554566860199\n",
            "Epoch 88025: train loss: 0.06282369792461395\n",
            "Epoch 88026: train loss: 0.06282185018062592\n",
            "Epoch 88027: train loss: 0.06282000988721848\n",
            "Epoch 88028: train loss: 0.06281816959381104\n",
            "Epoch 88029: train loss: 0.06281633675098419\n",
            "Epoch 88030: train loss: 0.06281449645757675\n",
            "Epoch 88031: train loss: 0.06281265616416931\n",
            "Epoch 88032: train loss: 0.06281080096960068\n",
            "Epoch 88033: train loss: 0.06280897557735443\n",
            "Epoch 88034: train loss: 0.0628071203827858\n",
            "Epoch 88035: train loss: 0.06280528753995895\n",
            "Epoch 88036: train loss: 0.06280343979597092\n",
            "Epoch 88037: train loss: 0.06280160695314407\n",
            "Epoch 88038: train loss: 0.06279976665973663\n",
            "Epoch 88039: train loss: 0.0627979263663292\n",
            "Epoch 88040: train loss: 0.06279608607292175\n",
            "Epoch 88041: train loss: 0.06279424577951431\n",
            "Epoch 88042: train loss: 0.06279240548610687\n",
            "Epoch 88043: train loss: 0.06279055774211884\n",
            "Epoch 88044: train loss: 0.0627887174487114\n",
            "Epoch 88045: train loss: 0.06278686970472336\n",
            "Epoch 88046: train loss: 0.06278503686189651\n",
            "Epoch 88047: train loss: 0.06278319656848907\n",
            "Epoch 88048: train loss: 0.06278135627508163\n",
            "Epoch 88049: train loss: 0.0627795085310936\n",
            "Epoch 88050: train loss: 0.06277767568826675\n",
            "Epoch 88051: train loss: 0.06277583539485931\n",
            "Epoch 88052: train loss: 0.06277400255203247\n",
            "Epoch 88053: train loss: 0.06277216225862503\n",
            "Epoch 88054: train loss: 0.06277032196521759\n",
            "Epoch 88055: train loss: 0.06276848167181015\n",
            "Epoch 88056: train loss: 0.0627666488289833\n",
            "Epoch 88057: train loss: 0.06276480853557587\n",
            "Epoch 88058: train loss: 0.06276297569274902\n",
            "Epoch 88059: train loss: 0.06276112049818039\n",
            "Epoch 88060: train loss: 0.06275928765535355\n",
            "Epoch 88061: train loss: 0.0627574548125267\n",
            "Epoch 88062: train loss: 0.06275561451911926\n",
            "Epoch 88063: train loss: 0.06275376677513123\n",
            "Epoch 88064: train loss: 0.06275193393230438\n",
            "Epoch 88065: train loss: 0.06275010108947754\n",
            "Epoch 88066: train loss: 0.0627482607960701\n",
            "Epoch 88067: train loss: 0.06274642050266266\n",
            "Epoch 88068: train loss: 0.06274459511041641\n",
            "Epoch 88069: train loss: 0.06274274736642838\n",
            "Epoch 88070: train loss: 0.06274092197418213\n",
            "Epoch 88071: train loss: 0.06273908913135529\n",
            "Epoch 88072: train loss: 0.06273724883794785\n",
            "Epoch 88073: train loss: 0.062735415995121\n",
            "Epoch 88074: train loss: 0.06273357570171356\n",
            "Epoch 88075: train loss: 0.06273175030946732\n",
            "Epoch 88076: train loss: 0.06272991001605988\n",
            "Epoch 88077: train loss: 0.06272807717323303\n",
            "Epoch 88078: train loss: 0.06272623687982559\n",
            "Epoch 88079: train loss: 0.06272440403699875\n",
            "Epoch 88080: train loss: 0.0627225786447525\n",
            "Epoch 88081: train loss: 0.06272073835134506\n",
            "Epoch 88082: train loss: 0.06271889805793762\n",
            "Epoch 88083: train loss: 0.06271706521511078\n",
            "Epoch 88084: train loss: 0.06271523237228394\n",
            "Epoch 88085: train loss: 0.06271340698003769\n",
            "Epoch 88086: train loss: 0.06271156668663025\n",
            "Epoch 88087: train loss: 0.062709741294384\n",
            "Epoch 88088: train loss: 0.06270790100097656\n",
            "Epoch 88089: train loss: 0.06270606070756912\n",
            "Epoch 88090: train loss: 0.06270422786474228\n",
            "Epoch 88091: train loss: 0.06270240247249603\n",
            "Epoch 88092: train loss: 0.06270056217908859\n",
            "Epoch 88093: train loss: 0.06269872933626175\n",
            "Epoch 88094: train loss: 0.06269688904285431\n",
            "Epoch 88095: train loss: 0.06269505620002747\n",
            "Epoch 88096: train loss: 0.06269323080778122\n",
            "Epoch 88097: train loss: 0.06269139796495438\n",
            "Epoch 88098: train loss: 0.06268955767154694\n",
            "Epoch 88099: train loss: 0.06268773227930069\n",
            "Epoch 88100: train loss: 0.06268589943647385\n",
            "Epoch 88101: train loss: 0.062684066593647\n",
            "Epoch 88102: train loss: 0.06268222630023956\n",
            "Epoch 88103: train loss: 0.06268039345741272\n",
            "Epoch 88104: train loss: 0.06267856061458588\n",
            "Epoch 88105: train loss: 0.06267672777175903\n",
            "Epoch 88106: train loss: 0.06267489492893219\n",
            "Epoch 88107: train loss: 0.06267305463552475\n",
            "Epoch 88108: train loss: 0.0626712366938591\n",
            "Epoch 88109: train loss: 0.06266939640045166\n",
            "Epoch 88110: train loss: 0.06266756355762482\n",
            "Epoch 88111: train loss: 0.06266573816537857\n",
            "Epoch 88112: train loss: 0.06266390532255173\n",
            "Epoch 88113: train loss: 0.06266207247972488\n",
            "Epoch 88114: train loss: 0.06266023218631744\n",
            "Epoch 88115: train loss: 0.0626584067940712\n",
            "Epoch 88116: train loss: 0.06265656650066376\n",
            "Epoch 88117: train loss: 0.06265474110841751\n",
            "Epoch 88118: train loss: 0.06265290081501007\n",
            "Epoch 88119: train loss: 0.06265106797218323\n",
            "Epoch 88120: train loss: 0.06264924257993698\n",
            "Epoch 88121: train loss: 0.06264740973711014\n",
            "Epoch 88122: train loss: 0.0626455694437027\n",
            "Epoch 88123: train loss: 0.06264375150203705\n",
            "Epoch 88124: train loss: 0.06264190375804901\n",
            "Epoch 88125: train loss: 0.06264007091522217\n",
            "Epoch 88126: train loss: 0.06263825297355652\n",
            "Epoch 88127: train loss: 0.06263641268014908\n",
            "Epoch 88128: train loss: 0.06263458728790283\n",
            "Epoch 88129: train loss: 0.06263274699449539\n",
            "Epoch 88130: train loss: 0.06263092160224915\n",
            "Epoch 88131: train loss: 0.0626290887594223\n",
            "Epoch 88132: train loss: 0.06262726336717606\n",
            "Epoch 88133: train loss: 0.06262542307376862\n",
            "Epoch 88134: train loss: 0.06262360513210297\n",
            "Epoch 88135: train loss: 0.06262177228927612\n",
            "Epoch 88136: train loss: 0.06261993944644928\n",
            "Epoch 88137: train loss: 0.06261810660362244\n",
            "Epoch 88138: train loss: 0.062616266310215\n",
            "Epoch 88139: train loss: 0.06261444091796875\n",
            "Epoch 88140: train loss: 0.06261260062456131\n",
            "Epoch 88141: train loss: 0.06261077523231506\n",
            "Epoch 88142: train loss: 0.06260894238948822\n",
            "Epoch 88143: train loss: 0.06260710954666138\n",
            "Epoch 88144: train loss: 0.06260528415441513\n",
            "Epoch 88145: train loss: 0.06260345131158829\n",
            "Epoch 88146: train loss: 0.06260161846876144\n",
            "Epoch 88147: train loss: 0.0625997856259346\n",
            "Epoch 88148: train loss: 0.06259795278310776\n",
            "Epoch 88149: train loss: 0.06259612739086151\n",
            "Epoch 88150: train loss: 0.06259429454803467\n",
            "Epoch 88151: train loss: 0.06259246915578842\n",
            "Epoch 88152: train loss: 0.06259063631296158\n",
            "Epoch 88153: train loss: 0.06258881092071533\n",
            "Epoch 88154: train loss: 0.06258698552846909\n",
            "Epoch 88155: train loss: 0.06258515268564224\n",
            "Epoch 88156: train loss: 0.0625833198428154\n",
            "Epoch 88157: train loss: 0.06258149445056915\n",
            "Epoch 88158: train loss: 0.06257966160774231\n",
            "Epoch 88159: train loss: 0.06257783621549606\n",
            "Epoch 88160: train loss: 0.06257600337266922\n",
            "Epoch 88161: train loss: 0.06257417798042297\n",
            "Epoch 88162: train loss: 0.06257234513759613\n",
            "Epoch 88163: train loss: 0.06257051229476929\n",
            "Epoch 88164: train loss: 0.06256868690252304\n",
            "Epoch 88165: train loss: 0.0625668615102768\n",
            "Epoch 88166: train loss: 0.06256502121686935\n",
            "Epoch 88167: train loss: 0.0625632107257843\n",
            "Epoch 88168: train loss: 0.06256136298179626\n",
            "Epoch 88169: train loss: 0.06255953758955002\n",
            "Epoch 88170: train loss: 0.06255770474672318\n",
            "Epoch 88171: train loss: 0.06255588680505753\n",
            "Epoch 88172: train loss: 0.06255406141281128\n",
            "Epoch 88173: train loss: 0.06255222111940384\n",
            "Epoch 88174: train loss: 0.06255039572715759\n",
            "Epoch 88175: train loss: 0.06254857033491135\n",
            "Epoch 88176: train loss: 0.0625467374920845\n",
            "Epoch 88177: train loss: 0.06254490464925766\n",
            "Epoch 88178: train loss: 0.06254308670759201\n",
            "Epoch 88179: train loss: 0.06254126131534576\n",
            "Epoch 88180: train loss: 0.06253942847251892\n",
            "Epoch 88181: train loss: 0.06253758817911148\n",
            "Epoch 88182: train loss: 0.06253576278686523\n",
            "Epoch 88183: train loss: 0.06253392994403839\n",
            "Epoch 88184: train loss: 0.06253211200237274\n",
            "Epoch 88185: train loss: 0.06253029406070709\n",
            "Epoch 88186: train loss: 0.06252846121788025\n",
            "Epoch 88187: train loss: 0.0625266283750534\n",
            "Epoch 88188: train loss: 0.06252480298280716\n",
            "Epoch 88189: train loss: 0.06252297759056091\n",
            "Epoch 88190: train loss: 0.06252115219831467\n",
            "Epoch 88191: train loss: 0.06251933425664902\n",
            "Epoch 88192: train loss: 0.06251748651266098\n",
            "Epoch 88193: train loss: 0.06251566857099533\n",
            "Epoch 88194: train loss: 0.06251384317874908\n",
            "Epoch 88195: train loss: 0.06251201778650284\n",
            "Epoch 88196: train loss: 0.062510184943676\n",
            "Epoch 88197: train loss: 0.06250835955142975\n",
            "Epoch 88198: train loss: 0.0625065341591835\n",
            "Epoch 88199: train loss: 0.06250470131635666\n",
            "Epoch 88200: train loss: 0.06250288337469101\n",
            "Epoch 88201: train loss: 0.06250104308128357\n",
            "Epoch 88202: train loss: 0.06249922513961792\n",
            "Epoch 88203: train loss: 0.06249741092324257\n",
            "Epoch 88204: train loss: 0.06249556690454483\n",
            "Epoch 88205: train loss: 0.06249375268816948\n",
            "Epoch 88206: train loss: 0.062491919845342636\n",
            "Epoch 88207: train loss: 0.06249009445309639\n",
            "Epoch 88208: train loss: 0.062488261610269547\n",
            "Epoch 88209: train loss: 0.0624864436686039\n",
            "Epoch 88210: train loss: 0.06248461827635765\n",
            "Epoch 88211: train loss: 0.062482800334692\n",
            "Epoch 88212: train loss: 0.06248098239302635\n",
            "Epoch 88213: train loss: 0.06247914955019951\n",
            "Epoch 88214: train loss: 0.06247733160853386\n",
            "Epoch 88215: train loss: 0.06247550994157791\n",
            "Epoch 88216: train loss: 0.062473684549331665\n",
            "Epoch 88217: train loss: 0.062471866607666016\n",
            "Epoch 88218: train loss: 0.06247004121541977\n",
            "Epoch 88219: train loss: 0.06246821582317352\n",
            "Epoch 88220: train loss: 0.062466397881507874\n",
            "Epoch 88221: train loss: 0.06246457248926163\n",
            "Epoch 88222: train loss: 0.06246274709701538\n",
            "Epoch 88223: train loss: 0.06246092543005943\n",
            "Epoch 88224: train loss: 0.06245910003781319\n",
            "Epoch 88225: train loss: 0.06245728209614754\n",
            "Epoch 88226: train loss: 0.06245546415448189\n",
            "Epoch 88227: train loss: 0.062453631311655045\n",
            "Epoch 88228: train loss: 0.062451813369989395\n",
            "Epoch 88229: train loss: 0.06244999170303345\n",
            "Epoch 88230: train loss: 0.0624481737613678\n",
            "Epoch 88231: train loss: 0.062446340918540955\n",
            "Epoch 88232: train loss: 0.062444522976875305\n",
            "Epoch 88233: train loss: 0.06244269758462906\n",
            "Epoch 88234: train loss: 0.06244087964296341\n",
            "Epoch 88235: train loss: 0.06243906170129776\n",
            "Epoch 88236: train loss: 0.062437236309051514\n",
            "Epoch 88237: train loss: 0.06243542209267616\n",
            "Epoch 88238: train loss: 0.062433596700429916\n",
            "Epoch 88239: train loss: 0.06243177130818367\n",
            "Epoch 88240: train loss: 0.06242995336651802\n",
            "Epoch 88241: train loss: 0.062428127974271774\n",
            "Epoch 88242: train loss: 0.06242630258202553\n",
            "Epoch 88243: train loss: 0.06242448091506958\n",
            "Epoch 88244: train loss: 0.062422655522823334\n",
            "Epoch 88245: train loss: 0.062420837581157684\n",
            "Epoch 88246: train loss: 0.06241901218891144\n",
            "Epoch 88247: train loss: 0.06241719424724579\n",
            "Epoch 88248: train loss: 0.06241537630558014\n",
            "Epoch 88249: train loss: 0.06241355463862419\n",
            "Epoch 88250: train loss: 0.06241173669695854\n",
            "Epoch 88251: train loss: 0.062409911304712296\n",
            "Epoch 88252: train loss: 0.062408093363046646\n",
            "Epoch 88253: train loss: 0.062406275421381\n",
            "Epoch 88254: train loss: 0.06240445375442505\n",
            "Epoch 88255: train loss: 0.0624026283621788\n",
            "Epoch 88256: train loss: 0.06240081787109375\n",
            "Epoch 88257: train loss: 0.0623989999294281\n",
            "Epoch 88258: train loss: 0.062397174537181854\n",
            "Epoch 88259: train loss: 0.062395352870225906\n",
            "Epoch 88260: train loss: 0.062393542379140854\n",
            "Epoch 88261: train loss: 0.06239171698689461\n",
            "Epoch 88262: train loss: 0.062389906495809555\n",
            "Epoch 88263: train loss: 0.06238808110356331\n",
            "Epoch 88264: train loss: 0.062386274337768555\n",
            "Epoch 88265: train loss: 0.062384456396102905\n",
            "Epoch 88266: train loss: 0.06238263100385666\n",
            "Epoch 88267: train loss: 0.06238080561161041\n",
            "Epoch 88268: train loss: 0.06237899139523506\n",
            "Epoch 88269: train loss: 0.06237717345356941\n",
            "Epoch 88270: train loss: 0.062375348061323166\n",
            "Epoch 88271: train loss: 0.062373530119657516\n",
            "Epoch 88272: train loss: 0.062371715903282166\n",
            "Epoch 88273: train loss: 0.06236989051103592\n",
            "Epoch 88274: train loss: 0.06236808001995087\n",
            "Epoch 88275: train loss: 0.06236626207828522\n",
            "Epoch 88276: train loss: 0.06236444041132927\n",
            "Epoch 88277: train loss: 0.06236261501908302\n",
            "Epoch 88278: train loss: 0.06236080452799797\n",
            "Epoch 88279: train loss: 0.06235899403691292\n",
            "Epoch 88280: train loss: 0.06235716864466667\n",
            "Epoch 88281: train loss: 0.062355346977710724\n",
            "Epoch 88282: train loss: 0.062353529036045074\n",
            "Epoch 88283: train loss: 0.06235171854496002\n",
            "Epoch 88284: train loss: 0.06234990432858467\n",
            "Epoch 88285: train loss: 0.062348078936338425\n",
            "Epoch 88286: train loss: 0.06234626844525337\n",
            "Epoch 88287: train loss: 0.06234445422887802\n",
            "Epoch 88288: train loss: 0.06234264373779297\n",
            "Epoch 88289: train loss: 0.062340833246707916\n",
            "Epoch 88290: train loss: 0.06233902648091316\n",
            "Epoch 88291: train loss: 0.06233720853924751\n",
            "Epoch 88292: train loss: 0.06233539432287216\n",
            "Epoch 88293: train loss: 0.062333591282367706\n",
            "Epoch 88294: train loss: 0.06233177334070206\n",
            "Epoch 88295: train loss: 0.0623299665749073\n",
            "Epoch 88296: train loss: 0.06232815608382225\n",
            "Epoch 88297: train loss: 0.0623263344168663\n",
            "Epoch 88298: train loss: 0.06232452392578125\n",
            "Epoch 88299: train loss: 0.0623227134346962\n",
            "Epoch 88300: train loss: 0.06232089921832085\n",
            "Epoch 88301: train loss: 0.06231909617781639\n",
            "Epoch 88302: train loss: 0.06231728196144104\n",
            "Epoch 88303: train loss: 0.06231546401977539\n",
            "Epoch 88304: train loss: 0.06231365352869034\n",
            "Epoch 88305: train loss: 0.06231183931231499\n",
            "Epoch 88306: train loss: 0.06231003627181053\n",
            "Epoch 88307: train loss: 0.06230822205543518\n",
            "Epoch 88308: train loss: 0.062306419014930725\n",
            "Epoch 88309: train loss: 0.062304604798555374\n",
            "Epoch 88310: train loss: 0.06230279430747032\n",
            "Epoch 88311: train loss: 0.06230097636580467\n",
            "Epoch 88312: train loss: 0.06229916214942932\n",
            "Epoch 88313: train loss: 0.06229735165834427\n",
            "Epoch 88314: train loss: 0.06229553744196892\n",
            "Epoch 88315: train loss: 0.06229371950030327\n",
            "Epoch 88316: train loss: 0.06229191645979881\n",
            "Epoch 88317: train loss: 0.06229010969400406\n",
            "Epoch 88318: train loss: 0.062288299202919006\n",
            "Epoch 88319: train loss: 0.062286484986543655\n",
            "Epoch 88320: train loss: 0.0622846744954586\n",
            "Epoch 88321: train loss: 0.06228286772966385\n",
            "Epoch 88322: train loss: 0.062281057238578796\n",
            "Epoch 88323: train loss: 0.06227925047278404\n",
            "Epoch 88324: train loss: 0.06227743253111839\n",
            "Epoch 88325: train loss: 0.062275633215904236\n",
            "Epoch 88326: train loss: 0.062273815274238586\n",
            "Epoch 88327: train loss: 0.06227201223373413\n",
            "Epoch 88328: train loss: 0.06227019801735878\n",
            "Epoch 88329: train loss: 0.062268394976854324\n",
            "Epoch 88330: train loss: 0.06226656958460808\n",
            "Epoch 88331: train loss: 0.06226475536823273\n",
            "Epoch 88332: train loss: 0.06226295605301857\n",
            "Epoch 88333: train loss: 0.06226114556193352\n",
            "Epoch 88334: train loss: 0.06225933879613876\n",
            "Epoch 88335: train loss: 0.06225752830505371\n",
            "Epoch 88336: train loss: 0.06225571781396866\n",
            "Epoch 88337: train loss: 0.06225390359759331\n",
            "Epoch 88338: train loss: 0.06225210055708885\n",
            "Epoch 88339: train loss: 0.062250278890132904\n",
            "Epoch 88340: train loss: 0.062248483300209045\n",
            "Epoch 88341: train loss: 0.062246669083833694\n",
            "Epoch 88342: train loss: 0.062244851142168045\n",
            "Epoch 88343: train loss: 0.06224305182695389\n",
            "Epoch 88344: train loss: 0.06224124878644943\n",
            "Epoch 88345: train loss: 0.06223943457007408\n",
            "Epoch 88346: train loss: 0.062237631529569626\n",
            "Epoch 88347: train loss: 0.062235817313194275\n",
            "Epoch 88348: train loss: 0.062234021723270416\n",
            "Epoch 88349: train loss: 0.06223221495747566\n",
            "Epoch 88350: train loss: 0.06223040446639061\n",
            "Epoch 88351: train loss: 0.062228597700595856\n",
            "Epoch 88352: train loss: 0.0622267909348011\n",
            "Epoch 88353: train loss: 0.062224987894296646\n",
            "Epoch 88354: train loss: 0.062223177403211594\n",
            "Epoch 88355: train loss: 0.062221378087997437\n",
            "Epoch 88356: train loss: 0.06221957877278328\n",
            "Epoch 88357: train loss: 0.06221776828169823\n",
            "Epoch 88358: train loss: 0.062215954065322876\n",
            "Epoch 88359: train loss: 0.06221415102481842\n",
            "Epoch 88360: train loss: 0.06221235916018486\n",
            "Epoch 88361: train loss: 0.06221054494380951\n",
            "Epoch 88362: train loss: 0.062208741903305054\n",
            "Epoch 88363: train loss: 0.0622069351375103\n",
            "Epoch 88364: train loss: 0.06220512464642525\n",
            "Epoch 88365: train loss: 0.06220331788063049\n",
            "Epoch 88366: train loss: 0.06220151484012604\n",
            "Epoch 88367: train loss: 0.062199708074331284\n",
            "Epoch 88368: train loss: 0.06219791620969772\n",
            "Epoch 88369: train loss: 0.062196098268032074\n",
            "Epoch 88370: train loss: 0.06219429895281792\n",
            "Epoch 88371: train loss: 0.06219249591231346\n",
            "Epoch 88372: train loss: 0.06219068914651871\n",
            "Epoch 88373: train loss: 0.06218888238072395\n",
            "Epoch 88374: train loss: 0.0621870793402195\n",
            "Epoch 88375: train loss: 0.06218528002500534\n",
            "Epoch 88376: train loss: 0.06218346208333969\n",
            "Epoch 88377: train loss: 0.06218167021870613\n",
            "Epoch 88378: train loss: 0.06217985972762108\n",
            "Epoch 88379: train loss: 0.06217804551124573\n",
            "Epoch 88380: train loss: 0.06217625364661217\n",
            "Epoch 88381: train loss: 0.06217445060610771\n",
            "Epoch 88382: train loss: 0.06217264384031296\n",
            "Epoch 88383: train loss: 0.0621708407998085\n",
            "Epoch 88384: train loss: 0.06216903403401375\n",
            "Epoch 88385: train loss: 0.06216724216938019\n",
            "Epoch 88386: train loss: 0.062165431678295135\n",
            "Epoch 88387: train loss: 0.06216362491250038\n",
            "Epoch 88388: train loss: 0.062161825597286224\n",
            "Epoch 88389: train loss: 0.062160030007362366\n",
            "Epoch 88390: train loss: 0.06215822324156761\n",
            "Epoch 88391: train loss: 0.06215641275048256\n",
            "Epoch 88392: train loss: 0.062154605984687805\n",
            "Epoch 88393: train loss: 0.062152814120054245\n",
            "Epoch 88394: train loss: 0.06215101107954979\n",
            "Epoch 88395: train loss: 0.06214919686317444\n",
            "Epoch 88396: train loss: 0.06214739754796028\n",
            "Epoch 88397: train loss: 0.06214560195803642\n",
            "Epoch 88398: train loss: 0.062143802642822266\n",
            "Epoch 88399: train loss: 0.06214199215173721\n",
            "Epoch 88400: train loss: 0.062140192836523056\n",
            "Epoch 88401: train loss: 0.062138378620147705\n",
            "Epoch 88402: train loss: 0.062136583030223846\n",
            "Epoch 88403: train loss: 0.062134791165590286\n",
            "Epoch 88404: train loss: 0.062132976949214935\n",
            "Epoch 88405: train loss: 0.06213117390871048\n",
            "Epoch 88406: train loss: 0.06212937459349632\n",
            "Epoch 88407: train loss: 0.062127575278282166\n",
            "Epoch 88408: train loss: 0.06212577223777771\n",
            "Epoch 88409: train loss: 0.06212397292256355\n",
            "Epoch 88410: train loss: 0.062122173607349396\n",
            "Epoch 88411: train loss: 0.06212037056684494\n",
            "Epoch 88412: train loss: 0.06211857125163078\n",
            "Epoch 88413: train loss: 0.06211676448583603\n",
            "Epoch 88414: train loss: 0.06211497262120247\n",
            "Epoch 88415: train loss: 0.06211316958069801\n",
            "Epoch 88416: train loss: 0.062111370265483856\n",
            "Epoch 88417: train loss: 0.062109578400850296\n",
            "Epoch 88418: train loss: 0.062107767909765244\n",
            "Epoch 88419: train loss: 0.06210597604513168\n",
            "Epoch 88420: train loss: 0.062104176729917526\n",
            "Epoch 88421: train loss: 0.06210237741470337\n",
            "Epoch 88422: train loss: 0.062100574374198914\n",
            "Epoch 88423: train loss: 0.062098775058984756\n",
            "Epoch 88424: train loss: 0.0620969757437706\n",
            "Epoch 88425: train loss: 0.06209518387913704\n",
            "Epoch 88426: train loss: 0.062093380838632584\n",
            "Epoch 88427: train loss: 0.062091581523418427\n",
            "Epoch 88428: train loss: 0.06208978220820427\n",
            "Epoch 88429: train loss: 0.06208798661828041\n",
            "Epoch 88430: train loss: 0.062086187303066254\n",
            "Epoch 88431: train loss: 0.0620843805372715\n",
            "Epoch 88432: train loss: 0.06208258494734764\n",
            "Epoch 88433: train loss: 0.06208077818155289\n",
            "Epoch 88434: train loss: 0.06207898631691933\n",
            "Epoch 88435: train loss: 0.06207718700170517\n",
            "Epoch 88436: train loss: 0.06207539141178131\n",
            "Epoch 88437: train loss: 0.062073592096567154\n",
            "Epoch 88438: train loss: 0.062071792781353\n",
            "Epoch 88439: train loss: 0.06206999346613884\n",
            "Epoch 88440: train loss: 0.06206819787621498\n",
            "Epoch 88441: train loss: 0.062066398561000824\n",
            "Epoch 88442: train loss: 0.062064606696367264\n",
            "Epoch 88443: train loss: 0.06206280738115311\n",
            "Epoch 88444: train loss: 0.06206100434064865\n",
            "Epoch 88445: train loss: 0.06205921247601509\n",
            "Epoch 88446: train loss: 0.06205740571022034\n",
            "Epoch 88447: train loss: 0.06205561384558678\n",
            "Epoch 88448: train loss: 0.06205382198095322\n",
            "Epoch 88449: train loss: 0.062052011489868164\n",
            "Epoch 88450: train loss: 0.062050219625234604\n",
            "Epoch 88451: train loss: 0.06204842031002045\n",
            "Epoch 88452: train loss: 0.06204662472009659\n",
            "Epoch 88453: train loss: 0.06204482540488243\n",
            "Epoch 88454: train loss: 0.062043026089668274\n",
            "Epoch 88455: train loss: 0.06204122677445412\n",
            "Epoch 88456: train loss: 0.06203943118453026\n",
            "Epoch 88457: train loss: 0.062037646770477295\n",
            "Epoch 88458: train loss: 0.06203584000468254\n",
            "Epoch 88459: train loss: 0.06203404814004898\n",
            "Epoch 88460: train loss: 0.06203224137425423\n",
            "Epoch 88461: train loss: 0.062030453234910965\n",
            "Epoch 88462: train loss: 0.062028661370277405\n",
            "Epoch 88463: train loss: 0.06202686205506325\n",
            "Epoch 88464: train loss: 0.062025055289268494\n",
            "Epoch 88465: train loss: 0.06202327087521553\n",
            "Epoch 88466: train loss: 0.06202147528529167\n",
            "Epoch 88467: train loss: 0.06201968342065811\n",
            "Epoch 88468: train loss: 0.06201787665486336\n",
            "Epoch 88469: train loss: 0.0620160847902298\n",
            "Epoch 88470: train loss: 0.06201429292559624\n",
            "Epoch 88471: train loss: 0.06201249361038208\n",
            "Epoch 88472: train loss: 0.062010690569877625\n",
            "Epoch 88473: train loss: 0.062008898705244064\n",
            "Epoch 88474: train loss: 0.062007106840610504\n",
            "Epoch 88475: train loss: 0.06200530752539635\n",
            "Epoch 88476: train loss: 0.06200351193547249\n",
            "Epoch 88477: train loss: 0.06200172007083893\n",
            "Epoch 88478: train loss: 0.06199992820620537\n",
            "Epoch 88479: train loss: 0.06199813634157181\n",
            "Epoch 88480: train loss: 0.06199633702635765\n",
            "Epoch 88481: train loss: 0.061994537711143494\n",
            "Epoch 88482: train loss: 0.061992742121219635\n",
            "Epoch 88483: train loss: 0.061990950256586075\n",
            "Epoch 88484: train loss: 0.061989158391952515\n",
            "Epoch 88485: train loss: 0.06198735907673836\n",
            "Epoch 88486: train loss: 0.0619855672121048\n",
            "Epoch 88487: train loss: 0.06198377534747124\n",
            "Epoch 88488: train loss: 0.06198197603225708\n",
            "Epoch 88489: train loss: 0.06198018044233322\n",
            "Epoch 88490: train loss: 0.06197839602828026\n",
            "Epoch 88491: train loss: 0.0619765967130661\n",
            "Epoch 88492: train loss: 0.061974797397851944\n",
            "Epoch 88493: train loss: 0.06197299808263779\n",
            "Epoch 88494: train loss: 0.06197120249271393\n",
            "Epoch 88495: train loss: 0.06196941062808037\n",
            "Epoch 88496: train loss: 0.061967626214027405\n",
            "Epoch 88497: train loss: 0.06196582689881325\n",
            "Epoch 88498: train loss: 0.06196403503417969\n",
            "Epoch 88499: train loss: 0.06196224316954613\n",
            "Epoch 88500: train loss: 0.061960458755493164\n",
            "Epoch 88501: train loss: 0.06195865944027901\n",
            "Epoch 88502: train loss: 0.06195686757564545\n",
            "Epoch 88503: train loss: 0.06195507198572159\n",
            "Epoch 88504: train loss: 0.061953287571668625\n",
            "Epoch 88505: train loss: 0.061951495707035065\n",
            "Epoch 88506: train loss: 0.061949703842401505\n",
            "Epoch 88507: train loss: 0.06194792315363884\n",
            "Epoch 88508: train loss: 0.06194612756371498\n",
            "Epoch 88509: train loss: 0.061944346874952316\n",
            "Epoch 88510: train loss: 0.06194255128502846\n",
            "Epoch 88511: train loss: 0.061940766870975494\n",
            "Epoch 88512: train loss: 0.06193898245692253\n",
            "Epoch 88513: train loss: 0.06193719431757927\n",
            "Epoch 88514: train loss: 0.061935409903526306\n",
            "Epoch 88515: train loss: 0.06193361431360245\n",
            "Epoch 88516: train loss: 0.06193182244896889\n",
            "Epoch 88517: train loss: 0.06193004176020622\n",
            "Epoch 88518: train loss: 0.061928246170282364\n",
            "Epoch 88519: train loss: 0.061926472932100296\n",
            "Epoch 88520: train loss: 0.06192467734217644\n",
            "Epoch 88521: train loss: 0.061922889202833176\n",
            "Epoch 88522: train loss: 0.061921101063489914\n",
            "Epoch 88523: train loss: 0.061919327825307846\n",
            "Epoch 88524: train loss: 0.06191752851009369\n",
            "Epoch 88525: train loss: 0.061915744096040726\n",
            "Epoch 88526: train loss: 0.06191395968198776\n",
            "Epoch 88527: train loss: 0.0619121752679348\n",
            "Epoch 88528: train loss: 0.06191038340330124\n",
            "Epoch 88529: train loss: 0.061908598989248276\n",
            "Epoch 88530: train loss: 0.06190681457519531\n",
            "Epoch 88531: train loss: 0.06190502271056175\n",
            "Epoch 88532: train loss: 0.06190323084592819\n",
            "Epoch 88533: train loss: 0.06190144643187523\n",
            "Epoch 88534: train loss: 0.06189967691898346\n",
            "Epoch 88535: train loss: 0.0618978776037693\n",
            "Epoch 88536: train loss: 0.06189609318971634\n",
            "Epoch 88537: train loss: 0.061894308775663376\n",
            "Epoch 88538: train loss: 0.06189252436161041\n",
            "Epoch 88539: train loss: 0.06189073249697685\n",
            "Epoch 88540: train loss: 0.06188895180821419\n",
            "Epoch 88541: train loss: 0.06188715621829033\n",
            "Epoch 88542: train loss: 0.061885375529527664\n",
            "Epoch 88543: train loss: 0.061883579939603806\n",
            "Epoch 88544: train loss: 0.06188180670142174\n",
            "Epoch 88545: train loss: 0.06188001111149788\n",
            "Epoch 88546: train loss: 0.061878230422735214\n",
            "Epoch 88547: train loss: 0.061876438558101654\n",
            "Epoch 88548: train loss: 0.06187465414404869\n",
            "Epoch 88549: train loss: 0.061872877180576324\n",
            "Epoch 88550: train loss: 0.06187107786536217\n",
            "Epoch 88551: train loss: 0.0618693083524704\n",
            "Epoch 88552: train loss: 0.06186750903725624\n",
            "Epoch 88553: train loss: 0.06186571717262268\n",
            "Epoch 88554: train loss: 0.061863940209150314\n",
            "Epoch 88555: train loss: 0.06186215579509735\n",
            "Epoch 88556: train loss: 0.06186036393046379\n",
            "Epoch 88557: train loss: 0.061858586966991425\n",
            "Epoch 88558: train loss: 0.06185680255293846\n",
            "Epoch 88559: train loss: 0.0618550106883049\n",
            "Epoch 88560: train loss: 0.061853229999542236\n",
            "Epoch 88561: train loss: 0.061851441860198975\n",
            "Epoch 88562: train loss: 0.06184965372085571\n",
            "Epoch 88563: train loss: 0.06184786558151245\n",
            "Epoch 88564: train loss: 0.061846084892749786\n",
            "Epoch 88565: train loss: 0.061844293028116226\n",
            "Epoch 88566: train loss: 0.06184250861406326\n",
            "Epoch 88567: train loss: 0.0618407242000103\n",
            "Epoch 88568: train loss: 0.061838939785957336\n",
            "Epoch 88569: train loss: 0.06183716282248497\n",
            "Epoch 88570: train loss: 0.06183537840843201\n",
            "Epoch 88571: train loss: 0.061833593994379044\n",
            "Epoch 88572: train loss: 0.06183180958032608\n",
            "Epoch 88573: train loss: 0.06183002516627312\n",
            "Epoch 88574: train loss: 0.061828237026929855\n",
            "Epoch 88575: train loss: 0.06182645261287689\n",
            "Epoch 88576: train loss: 0.06182466819882393\n",
            "Epoch 88577: train loss: 0.061822883784770966\n",
            "Epoch 88578: train loss: 0.061821099370718\n",
            "Epoch 88579: train loss: 0.06181931495666504\n",
            "Epoch 88580: train loss: 0.06181753799319267\n",
            "Epoch 88581: train loss: 0.06181574612855911\n",
            "Epoch 88582: train loss: 0.06181396171450615\n",
            "Epoch 88583: train loss: 0.061812177300453186\n",
            "Epoch 88584: train loss: 0.06181039661169052\n",
            "Epoch 88585: train loss: 0.06180861219763756\n",
            "Epoch 88586: train loss: 0.061806827783584595\n",
            "Epoch 88587: train loss: 0.06180504336953163\n",
            "Epoch 88588: train loss: 0.06180325895547867\n",
            "Epoch 88589: train loss: 0.061801474541425705\n",
            "Epoch 88590: train loss: 0.06179969757795334\n",
            "Epoch 88591: train loss: 0.06179790571331978\n",
            "Epoch 88592: train loss: 0.061796121299266815\n",
            "Epoch 88593: train loss: 0.06179434061050415\n",
            "Epoch 88594: train loss: 0.06179255619645119\n",
            "Epoch 88595: train loss: 0.06179077923297882\n",
            "Epoch 88596: train loss: 0.06178899481892586\n",
            "Epoch 88597: train loss: 0.061787210404872894\n",
            "Epoch 88598: train loss: 0.06178542599081993\n",
            "Epoch 88599: train loss: 0.061783626675605774\n",
            "Epoch 88600: train loss: 0.061781857162714005\n",
            "Epoch 88601: train loss: 0.06178007647395134\n",
            "Epoch 88602: train loss: 0.061778292059898376\n",
            "Epoch 88603: train loss: 0.06177650764584541\n",
            "Epoch 88604: train loss: 0.061774738132953644\n",
            "Epoch 88605: train loss: 0.061772946268320084\n",
            "Epoch 88606: train loss: 0.06177116930484772\n",
            "Epoch 88607: train loss: 0.06176937744021416\n",
            "Epoch 88608: train loss: 0.061767593026161194\n",
            "Epoch 88609: train loss: 0.06176580861210823\n",
            "Epoch 88610: train loss: 0.06176402047276497\n",
            "Epoch 88611: train loss: 0.0617622435092926\n",
            "Epoch 88612: train loss: 0.061760466545820236\n",
            "Epoch 88613: train loss: 0.06175868213176727\n",
            "Epoch 88614: train loss: 0.06175690516829491\n",
            "Epoch 88615: train loss: 0.06175512075424194\n",
            "Epoch 88616: train loss: 0.06175334006547928\n",
            "Epoch 88617: train loss: 0.061751555651426315\n",
            "Epoch 88618: train loss: 0.061749786138534546\n",
            "Epoch 88619: train loss: 0.06174800172448158\n",
            "Epoch 88620: train loss: 0.06174621731042862\n",
            "Epoch 88621: train loss: 0.06174444407224655\n",
            "Epoch 88622: train loss: 0.06174265220761299\n",
            "Epoch 88623: train loss: 0.061740875244140625\n",
            "Epoch 88624: train loss: 0.06173909083008766\n",
            "Epoch 88625: train loss: 0.061737313866615295\n",
            "Epoch 88626: train loss: 0.06173552945256233\n",
            "Epoch 88627: train loss: 0.06173374876379967\n",
            "Epoch 88628: train loss: 0.06173195689916611\n",
            "Epoch 88629: train loss: 0.06173018738627434\n",
            "Epoch 88630: train loss: 0.061728402972221375\n",
            "Epoch 88631: train loss: 0.06172662600874901\n",
            "Epoch 88632: train loss: 0.06172484531998634\n",
            "Epoch 88633: train loss: 0.06172306090593338\n",
            "Epoch 88634: train loss: 0.06172129139304161\n",
            "Epoch 88635: train loss: 0.06171950697898865\n",
            "Epoch 88636: train loss: 0.061717722564935684\n",
            "Epoch 88637: train loss: 0.06171594187617302\n",
            "Epoch 88638: train loss: 0.06171416491270065\n",
            "Epoch 88639: train loss: 0.06171238049864769\n",
            "Epoch 88640: train loss: 0.061710603535175323\n",
            "Epoch 88641: train loss: 0.06170881912112236\n",
            "Epoch 88642: train loss: 0.061707038432359695\n",
            "Epoch 88643: train loss: 0.06170526146888733\n",
            "Epoch 88644: train loss: 0.06170349195599556\n",
            "Epoch 88645: train loss: 0.061701700091362\n",
            "Epoch 88646: train loss: 0.06169992685317993\n",
            "Epoch 88647: train loss: 0.06169813871383667\n",
            "Epoch 88648: train loss: 0.061696358025074005\n",
            "Epoch 88649: train loss: 0.06169458106160164\n",
            "Epoch 88650: train loss: 0.061692796647548676\n",
            "Epoch 88651: train loss: 0.061691027134656906\n",
            "Epoch 88652: train loss: 0.061689238995313644\n",
            "Epoch 88653: train loss: 0.061687469482421875\n",
            "Epoch 88654: train loss: 0.06168569251894951\n",
            "Epoch 88655: train loss: 0.061683908104896545\n",
            "Epoch 88656: train loss: 0.06168212741613388\n",
            "Epoch 88657: train loss: 0.061680350452661514\n",
            "Epoch 88658: train loss: 0.06167856603860855\n",
            "Epoch 88659: train loss: 0.06167679652571678\n",
            "Epoch 88660: train loss: 0.06167501211166382\n",
            "Epoch 88661: train loss: 0.061673231422901154\n",
            "Epoch 88662: train loss: 0.06167144700884819\n",
            "Epoch 88663: train loss: 0.06166967749595642\n",
            "Epoch 88664: train loss: 0.06166788563132286\n",
            "Epoch 88665: train loss: 0.06166611984372139\n",
            "Epoch 88666: train loss: 0.06166434288024902\n",
            "Epoch 88667: train loss: 0.06166255846619606\n",
            "Epoch 88668: train loss: 0.06166078522801399\n",
            "Epoch 88669: train loss: 0.06165900081396103\n",
            "Epoch 88670: train loss: 0.06165722385048866\n",
            "Epoch 88671: train loss: 0.06165545433759689\n",
            "Epoch 88672: train loss: 0.06165366992354393\n",
            "Epoch 88673: train loss: 0.06165188550949097\n",
            "Epoch 88674: train loss: 0.0616501122713089\n",
            "Epoch 88675: train loss: 0.061648327857255936\n",
            "Epoch 88676: train loss: 0.061646558344364166\n",
            "Epoch 88677: train loss: 0.0616447776556015\n",
            "Epoch 88678: train loss: 0.06164299324154854\n",
            "Epoch 88679: train loss: 0.06164122372865677\n",
            "Epoch 88680: train loss: 0.061639439314603806\n",
            "Epoch 88681: train loss: 0.06163765862584114\n",
            "Epoch 88682: train loss: 0.06163588911294937\n",
            "Epoch 88683: train loss: 0.061634112149477005\n",
            "Epoch 88684: train loss: 0.06163233518600464\n",
            "Epoch 88685: train loss: 0.06163056194782257\n",
            "Epoch 88686: train loss: 0.06162877753376961\n",
            "Epoch 88687: train loss: 0.06162700802087784\n",
            "Epoch 88688: train loss: 0.06162523478269577\n",
            "Epoch 88689: train loss: 0.06162345036864281\n",
            "Epoch 88690: train loss: 0.06162168085575104\n",
            "Epoch 88691: train loss: 0.06161990761756897\n",
            "Epoch 88692: train loss: 0.0616181306540966\n",
            "Epoch 88693: train loss: 0.06161636486649513\n",
            "Epoch 88694: train loss: 0.061614587903022766\n",
            "Epoch 88695: train loss: 0.0616128109395504\n",
            "Epoch 88696: train loss: 0.06161103770136833\n",
            "Epoch 88697: train loss: 0.061609260737895966\n",
            "Epoch 88698: train loss: 0.061607491225004196\n",
            "Epoch 88699: train loss: 0.06160571798682213\n",
            "Epoch 88700: train loss: 0.06160394847393036\n",
            "Epoch 88701: train loss: 0.06160217151045799\n",
            "Epoch 88702: train loss: 0.061600398272275925\n",
            "Epoch 88703: train loss: 0.061598628759384155\n",
            "Epoch 88704: train loss: 0.06159684434533119\n",
            "Epoch 88705: train loss: 0.061595071107149124\n",
            "Epoch 88706: train loss: 0.061593301594257355\n",
            "Epoch 88707: train loss: 0.06159152835607529\n",
            "Epoch 88708: train loss: 0.06158975884318352\n",
            "Epoch 88709: train loss: 0.061587993055582047\n",
            "Epoch 88710: train loss: 0.06158621609210968\n",
            "Epoch 88711: train loss: 0.06158444657921791\n",
            "Epoch 88712: train loss: 0.06158267334103584\n",
            "Epoch 88713: train loss: 0.06158091127872467\n",
            "Epoch 88714: train loss: 0.061579130589962006\n",
            "Epoch 88715: train loss: 0.061577361077070236\n",
            "Epoch 88716: train loss: 0.061575595289468765\n",
            "Epoch 88717: train loss: 0.061573825776576996\n",
            "Epoch 88718: train loss: 0.061572056263685226\n",
            "Epoch 88719: train loss: 0.06157028302550316\n",
            "Epoch 88720: train loss: 0.06156851723790169\n",
            "Epoch 88721: train loss: 0.06156674772500992\n",
            "Epoch 88722: train loss: 0.06156497821211815\n",
            "Epoch 88723: train loss: 0.06156320497393608\n",
            "Epoch 88724: train loss: 0.06156144663691521\n",
            "Epoch 88725: train loss: 0.06155966967344284\n",
            "Epoch 88726: train loss: 0.06155791133642197\n",
            "Epoch 88727: train loss: 0.0615561418235302\n",
            "Epoch 88728: train loss: 0.06155436486005783\n",
            "Epoch 88729: train loss: 0.06155259907245636\n",
            "Epoch 88730: train loss: 0.06155082955956459\n",
            "Epoch 88731: train loss: 0.06154907867312431\n",
            "Epoch 88732: train loss: 0.06154729798436165\n",
            "Epoch 88733: train loss: 0.06154554337263107\n",
            "Epoch 88734: train loss: 0.0615437775850296\n",
            "Epoch 88735: train loss: 0.06154200807213783\n",
            "Epoch 88736: train loss: 0.06154024973511696\n",
            "Epoch 88737: train loss: 0.06153848394751549\n",
            "Epoch 88738: train loss: 0.06153671443462372\n",
            "Epoch 88739: train loss: 0.06153494864702225\n",
            "Epoch 88740: train loss: 0.061533186584711075\n",
            "Epoch 88741: train loss: 0.061531420797109604\n",
            "Epoch 88742: train loss: 0.06152966246008873\n",
            "Epoch 88743: train loss: 0.06152789294719696\n",
            "Epoch 88744: train loss: 0.06152612715959549\n",
            "Epoch 88745: train loss: 0.06152437627315521\n",
            "Epoch 88746: train loss: 0.06152260676026344\n",
            "Epoch 88747: train loss: 0.06152084097266197\n",
            "Epoch 88748: train loss: 0.0615190789103508\n",
            "Epoch 88749: train loss: 0.06151731312274933\n",
            "Epoch 88750: train loss: 0.061515551060438156\n",
            "Epoch 88751: train loss: 0.061513785272836685\n",
            "Epoch 88752: train loss: 0.061512019485235214\n",
            "Epoch 88753: train loss: 0.06151026487350464\n",
            "Epoch 88754: train loss: 0.06150849908590317\n",
            "Epoch 88755: train loss: 0.0615067332983017\n",
            "Epoch 88756: train loss: 0.061504971235990524\n",
            "Epoch 88757: train loss: 0.06150321289896965\n",
            "Epoch 88758: train loss: 0.06150144711136818\n",
            "Epoch 88759: train loss: 0.06149967014789581\n",
            "Epoch 88760: train loss: 0.06149791181087494\n",
            "Epoch 88761: train loss: 0.06149615719914436\n",
            "Epoch 88762: train loss: 0.06149439886212349\n",
            "Epoch 88763: train loss: 0.06149263307452202\n",
            "Epoch 88764: train loss: 0.06149086356163025\n",
            "Epoch 88765: train loss: 0.06148909777402878\n",
            "Epoch 88766: train loss: 0.0614873468875885\n",
            "Epoch 88767: train loss: 0.06148557737469673\n",
            "Epoch 88768: train loss: 0.06148381903767586\n",
            "Epoch 88769: train loss: 0.061482060700654984\n",
            "Epoch 88770: train loss: 0.061480291187763214\n",
            "Epoch 88771: train loss: 0.061478517949581146\n",
            "Epoch 88772: train loss: 0.06147676706314087\n",
            "Epoch 88773: train loss: 0.0614750050008297\n",
            "Epoch 88774: train loss: 0.061473239213228226\n",
            "Epoch 88775: train loss: 0.06147148087620735\n",
            "Epoch 88776: train loss: 0.06146971136331558\n",
            "Epoch 88777: train loss: 0.06146795302629471\n",
            "Epoch 88778: train loss: 0.061466190963983536\n",
            "Epoch 88779: train loss: 0.06146443262696266\n",
            "Epoch 88780: train loss: 0.06146266683936119\n",
            "Epoch 88781: train loss: 0.06146090477705002\n",
            "Epoch 88782: train loss: 0.061459146440029144\n",
            "Epoch 88783: train loss: 0.06145738065242767\n",
            "Epoch 88784: train loss: 0.0614556185901165\n",
            "Epoch 88785: train loss: 0.06145386025309563\n",
            "Epoch 88786: train loss: 0.061452094465494156\n",
            "Epoch 88787: train loss: 0.06145033240318298\n",
            "Epoch 88788: train loss: 0.06144856661558151\n",
            "Epoch 88789: train loss: 0.061446815729141235\n",
            "Epoch 88790: train loss: 0.06144505366683006\n",
            "Epoch 88791: train loss: 0.06144328787922859\n",
            "Epoch 88792: train loss: 0.06144152954220772\n",
            "Epoch 88793: train loss: 0.061439771205186844\n",
            "Epoch 88794: train loss: 0.06143801659345627\n",
            "Epoch 88795: train loss: 0.06143626570701599\n",
            "Epoch 88796: train loss: 0.06143449246883392\n",
            "Epoch 88797: train loss: 0.061432741582393646\n",
            "Epoch 88798: train loss: 0.061430979520082474\n",
            "Epoch 88799: train loss: 0.0614292286336422\n",
            "Epoch 88800: train loss: 0.06142745539546013\n",
            "Epoch 88801: train loss: 0.061425693333148956\n",
            "Epoch 88802: train loss: 0.06142393499612808\n",
            "Epoch 88803: train loss: 0.061422184109687805\n",
            "Epoch 88804: train loss: 0.06142042577266693\n",
            "Epoch 88805: train loss: 0.06141866371035576\n",
            "Epoch 88806: train loss: 0.06141691282391548\n",
            "Epoch 88807: train loss: 0.06141514703631401\n",
            "Epoch 88808: train loss: 0.06141338869929314\n",
            "Epoch 88809: train loss: 0.06141163781285286\n",
            "Epoch 88810: train loss: 0.06140986829996109\n",
            "Epoch 88811: train loss: 0.06140811741352081\n",
            "Epoch 88812: train loss: 0.06140635907649994\n",
            "Epoch 88813: train loss: 0.061404597014188766\n",
            "Epoch 88814: train loss: 0.06140283867716789\n",
            "Epoch 88815: train loss: 0.06140108034014702\n",
            "Epoch 88816: train loss: 0.061399322003126144\n",
            "Epoch 88817: train loss: 0.061397578567266464\n",
            "Epoch 88818: train loss: 0.0613958016037941\n",
            "Epoch 88819: train loss: 0.06139405071735382\n",
            "Epoch 88820: train loss: 0.06139229238033295\n",
            "Epoch 88821: train loss: 0.061390530318021774\n",
            "Epoch 88822: train loss: 0.0613887794315815\n",
            "Epoch 88823: train loss: 0.06138702109456062\n",
            "Epoch 88824: train loss: 0.06138525530695915\n",
            "Epoch 88825: train loss: 0.061383504420518875\n",
            "Epoch 88826: train loss: 0.0613817498087883\n",
            "Epoch 88827: train loss: 0.061379991471767426\n",
            "Epoch 88828: train loss: 0.061378225684165955\n",
            "Epoch 88829: train loss: 0.06137648969888687\n",
            "Epoch 88830: train loss: 0.0613747201859951\n",
            "Epoch 88831: train loss: 0.06137296184897423\n",
            "Epoch 88832: train loss: 0.061371203511953354\n",
            "Epoch 88833: train loss: 0.06136945262551308\n",
            "Epoch 88834: train loss: 0.061367690563201904\n",
            "Epoch 88835: train loss: 0.06136593222618103\n",
            "Epoch 88836: train loss: 0.061364173889160156\n",
            "Epoch 88837: train loss: 0.06136242300271988\n",
            "Epoch 88838: train loss: 0.06136065721511841\n",
            "Epoch 88839: train loss: 0.06135890260338783\n",
            "Epoch 88840: train loss: 0.06135714426636696\n",
            "Epoch 88841: train loss: 0.06135539337992668\n",
            "Epoch 88842: train loss: 0.06135362759232521\n",
            "Epoch 88843: train loss: 0.06135188415646553\n",
            "Epoch 88844: train loss: 0.06135012209415436\n",
            "Epoch 88845: train loss: 0.061348363757133484\n",
            "Epoch 88846: train loss: 0.06134661287069321\n",
            "Epoch 88847: train loss: 0.06134486198425293\n",
            "Epoch 88848: train loss: 0.061343103647232056\n",
            "Epoch 88849: train loss: 0.06134134531021118\n",
            "Epoch 88850: train loss: 0.061339590698480606\n",
            "Epoch 88851: train loss: 0.061337824910879135\n",
            "Epoch 88852: train loss: 0.06133607402443886\n",
            "Epoch 88853: train loss: 0.06133432313799858\n",
            "Epoch 88854: train loss: 0.061332572251558304\n",
            "Epoch 88855: train loss: 0.06133081018924713\n",
            "Epoch 88856: train loss: 0.06132905185222626\n",
            "Epoch 88857: train loss: 0.06132730841636658\n",
            "Epoch 88858: train loss: 0.061325542628765106\n",
            "Epoch 88859: train loss: 0.06132379174232483\n",
            "Epoch 88860: train loss: 0.061322033405303955\n",
            "Epoch 88861: train loss: 0.061320286244153976\n",
            "Epoch 88862: train loss: 0.0613185279071331\n",
            "Epoch 88863: train loss: 0.06131676211953163\n",
            "Epoch 88864: train loss: 0.06131502613425255\n",
            "Epoch 88865: train loss: 0.06131327524781227\n",
            "Epoch 88866: train loss: 0.0613115020096302\n",
            "Epoch 88867: train loss: 0.06130974739789963\n",
            "Epoch 88868: train loss: 0.061307989060878754\n",
            "Epoch 88869: train loss: 0.061306245625019073\n",
            "Epoch 88870: train loss: 0.0613044872879982\n",
            "Epoch 88871: train loss: 0.061302728950977325\n",
            "Epoch 88872: train loss: 0.06130097806453705\n",
            "Epoch 88873: train loss: 0.06129922345280647\n",
            "Epoch 88874: train loss: 0.061297472566366196\n",
            "Epoch 88875: train loss: 0.061295706778764725\n",
            "Epoch 88876: train loss: 0.061293963342905045\n",
            "Epoch 88877: train loss: 0.061292197555303574\n",
            "Epoch 88878: train loss: 0.06129045411944389\n",
            "Epoch 88879: train loss: 0.06128869578242302\n",
            "Epoch 88880: train loss: 0.061286941170692444\n",
            "Epoch 88881: train loss: 0.061285197734832764\n",
            "Epoch 88882: train loss: 0.06128343939781189\n",
            "Epoch 88883: train loss: 0.061281681060791016\n",
            "Epoch 88884: train loss: 0.06127992272377014\n",
            "Epoch 88885: train loss: 0.06127818673849106\n",
            "Epoch 88886: train loss: 0.061276428401470184\n",
            "Epoch 88887: train loss: 0.06127466633915901\n",
            "Epoch 88888: train loss: 0.06127291917800903\n",
            "Epoch 88889: train loss: 0.061271172016859055\n",
            "Epoch 88890: train loss: 0.06126941367983818\n",
            "Epoch 88891: train loss: 0.0612676627933979\n",
            "Epoch 88892: train loss: 0.06126589700579643\n",
            "Epoch 88893: train loss: 0.06126415356993675\n",
            "Epoch 88894: train loss: 0.06126238778233528\n",
            "Epoch 88895: train loss: 0.0612606443464756\n",
            "Epoch 88896: train loss: 0.061258893460035324\n",
            "Epoch 88897: train loss: 0.061257150024175644\n",
            "Epoch 88898: train loss: 0.06125538796186447\n",
            "Epoch 88899: train loss: 0.061253637075424194\n",
            "Epoch 88900: train loss: 0.06125187873840332\n",
            "Epoch 88901: train loss: 0.06125012785196304\n",
            "Epoch 88902: train loss: 0.061248376965522766\n",
            "Epoch 88903: train loss: 0.06124662607908249\n",
            "Epoch 88904: train loss: 0.061244867742061615\n",
            "Epoch 88905: train loss: 0.061243124306201935\n",
            "Epoch 88906: train loss: 0.06124136969447136\n",
            "Epoch 88907: train loss: 0.06123961880803108\n",
            "Epoch 88908: train loss: 0.061237867921590805\n",
            "Epoch 88909: train loss: 0.06123610958456993\n",
            "Epoch 88910: train loss: 0.06123436614871025\n",
            "Epoch 88911: train loss: 0.06123260781168938\n",
            "Epoch 88912: train loss: 0.0612308569252491\n",
            "Epoch 88913: train loss: 0.06122910603880882\n",
            "Epoch 88914: train loss: 0.06122734770178795\n",
            "Epoch 88915: train loss: 0.061225611716508865\n",
            "Epoch 88916: train loss: 0.061223845928907394\n",
            "Epoch 88917: train loss: 0.06122209131717682\n",
            "Epoch 88918: train loss: 0.06122035160660744\n",
            "Epoch 88919: train loss: 0.061218589544296265\n",
            "Epoch 88920: train loss: 0.06121683120727539\n",
            "Epoch 88921: train loss: 0.06121509522199631\n",
            "Epoch 88922: train loss: 0.06121334433555603\n",
            "Epoch 88923: train loss: 0.06121159344911575\n",
            "Epoch 88924: train loss: 0.061209842562675476\n",
            "Epoch 88925: train loss: 0.0612080842256546\n",
            "Epoch 88926: train loss: 0.06120634078979492\n",
            "Epoch 88927: train loss: 0.06120458245277405\n",
            "Epoch 88928: train loss: 0.061202846467494965\n",
            "Epoch 88929: train loss: 0.06120108813047409\n",
            "Epoch 88930: train loss: 0.061199337244033813\n",
            "Epoch 88931: train loss: 0.06119759380817413\n",
            "Epoch 88932: train loss: 0.06119583547115326\n",
            "Epoch 88933: train loss: 0.061194099485874176\n",
            "Epoch 88934: train loss: 0.0611923411488533\n",
            "Epoch 88935: train loss: 0.061190590262413025\n",
            "Epoch 88936: train loss: 0.061188846826553345\n",
            "Epoch 88937: train loss: 0.061187103390693665\n",
            "Epoch 88938: train loss: 0.06118535250425339\n",
            "Epoch 88939: train loss: 0.06118360906839371\n",
            "Epoch 88940: train loss: 0.06118185073137283\n",
            "Epoch 88941: train loss: 0.06118010729551315\n",
            "Epoch 88942: train loss: 0.061178356409072876\n",
            "Epoch 88943: train loss: 0.061176612973213196\n",
            "Epoch 88944: train loss: 0.06117486208677292\n",
            "Epoch 88945: train loss: 0.06117311865091324\n",
            "Epoch 88946: train loss: 0.06117136776447296\n",
            "Epoch 88947: train loss: 0.06116962432861328\n",
            "Epoch 88948: train loss: 0.0611678808927536\n",
            "Epoch 88949: train loss: 0.06116611510515213\n",
            "Epoch 88950: train loss: 0.06116437911987305\n",
            "Epoch 88951: train loss: 0.06116262078285217\n",
            "Epoch 88952: train loss: 0.06116087734699249\n",
            "Epoch 88953: train loss: 0.061159126460552216\n",
            "Epoch 88954: train loss: 0.06115737557411194\n",
            "Epoch 88955: train loss: 0.06115562468767166\n",
            "Epoch 88956: train loss: 0.06115388870239258\n",
            "Epoch 88957: train loss: 0.0611521378159523\n",
            "Epoch 88958: train loss: 0.061150386929512024\n",
            "Epoch 88959: train loss: 0.061148643493652344\n",
            "Epoch 88960: train loss: 0.06114690750837326\n",
            "Epoch 88961: train loss: 0.06114515662193298\n",
            "Epoch 88962: train loss: 0.0611434131860733\n",
            "Epoch 88963: train loss: 0.06114166975021362\n",
            "Epoch 88964: train loss: 0.06113993003964424\n",
            "Epoch 88965: train loss: 0.061138179153203964\n",
            "Epoch 88966: train loss: 0.061136435717344284\n",
            "Epoch 88967: train loss: 0.0611346997320652\n",
            "Epoch 88968: train loss: 0.061132948845624924\n",
            "Epoch 88969: train loss: 0.06113122031092644\n",
            "Epoch 88970: train loss: 0.06112946942448616\n",
            "Epoch 88971: train loss: 0.06112772226333618\n",
            "Epoch 88972: train loss: 0.0611259751021862\n",
            "Epoch 88973: train loss: 0.06112423539161682\n",
            "Epoch 88974: train loss: 0.061122484505176544\n",
            "Epoch 88975: train loss: 0.06112075597047806\n",
            "Epoch 88976: train loss: 0.06111901253461838\n",
            "Epoch 88977: train loss: 0.0611172690987587\n",
            "Epoch 88978: train loss: 0.06111552193760872\n",
            "Epoch 88979: train loss: 0.06111377850174904\n",
            "Epoch 88980: train loss: 0.061112042516469955\n",
            "Epoch 88981: train loss: 0.061110299080610275\n",
            "Epoch 88982: train loss: 0.06110854819417\n",
            "Epoch 88983: train loss: 0.061106812208890915\n",
            "Epoch 88984: train loss: 0.06110506132245064\n",
            "Epoch 88985: train loss: 0.06110332906246185\n",
            "Epoch 88986: train loss: 0.061101578176021576\n",
            "Epoch 88987: train loss: 0.06109984219074249\n",
            "Epoch 88988: train loss: 0.06109809875488281\n",
            "Epoch 88989: train loss: 0.061096370220184326\n",
            "Epoch 88990: train loss: 0.061094630509614944\n",
            "Epoch 88991: train loss: 0.06109287962317467\n",
            "Epoch 88992: train loss: 0.06109113618731499\n",
            "Epoch 88993: train loss: 0.061089400202035904\n",
            "Epoch 88994: train loss: 0.061087656766176224\n",
            "Epoch 88995: train loss: 0.06108592078089714\n",
            "Epoch 88996: train loss: 0.06108417361974716\n",
            "Epoch 88997: train loss: 0.06108243018388748\n",
            "Epoch 88998: train loss: 0.0610806941986084\n",
            "Epoch 88999: train loss: 0.061078958213329315\n",
            "Epoch 89000: train loss: 0.061077214777469635\n",
            "Epoch 89001: train loss: 0.061075467616319656\n",
            "Epoch 89002: train loss: 0.06107373163104057\n",
            "Epoch 89003: train loss: 0.06107198819518089\n",
            "Epoch 89004: train loss: 0.06107024475932121\n",
            "Epoch 89005: train loss: 0.06106850877404213\n",
            "Epoch 89006: train loss: 0.06106676533818245\n",
            "Epoch 89007: train loss: 0.06106502562761307\n",
            "Epoch 89008: train loss: 0.061063289642333984\n",
            "Epoch 89009: train loss: 0.0610615536570549\n",
            "Epoch 89010: train loss: 0.06105981022119522\n",
            "Epoch 89011: train loss: 0.06105806306004524\n",
            "Epoch 89012: train loss: 0.06105632707476616\n",
            "Epoch 89013: train loss: 0.06105457618832588\n",
            "Epoch 89014: train loss: 0.061052847653627396\n",
            "Epoch 89015: train loss: 0.06105111166834831\n",
            "Epoch 89016: train loss: 0.06104937940835953\n",
            "Epoch 89017: train loss: 0.06104762852191925\n",
            "Epoch 89018: train loss: 0.06104588508605957\n",
            "Epoch 89019: train loss: 0.06104414910078049\n",
            "Epoch 89020: train loss: 0.0610424168407917\n",
            "Epoch 89021: train loss: 0.061040665954351425\n",
            "Epoch 89022: train loss: 0.06103892996907234\n",
            "Epoch 89023: train loss: 0.06103719398379326\n",
            "Epoch 89024: train loss: 0.06103545427322388\n",
            "Epoch 89025: train loss: 0.0610337108373642\n",
            "Epoch 89026: train loss: 0.061031974852085114\n",
            "Epoch 89027: train loss: 0.06103023886680603\n",
            "Epoch 89028: train loss: 0.06102849543094635\n",
            "Epoch 89029: train loss: 0.061026763170957565\n",
            "Epoch 89030: train loss: 0.06102501228451729\n",
            "Epoch 89031: train loss: 0.061023276299238205\n",
            "Epoch 89032: train loss: 0.06102154031395912\n",
            "Epoch 89033: train loss: 0.06101980060338974\n",
            "Epoch 89034: train loss: 0.06101805716753006\n",
            "Epoch 89035: train loss: 0.061016328632831573\n",
            "Epoch 89036: train loss: 0.06101458519697189\n",
            "Epoch 89037: train loss: 0.06101285293698311\n",
            "Epoch 89038: train loss: 0.061011116951704025\n",
            "Epoch 89039: train loss: 0.06100936606526375\n",
            "Epoch 89040: train loss: 0.061007630079984665\n",
            "Epoch 89041: train loss: 0.06100589036941528\n",
            "Epoch 89042: train loss: 0.0610041618347168\n",
            "Epoch 89043: train loss: 0.06100241839885712\n",
            "Epoch 89044: train loss: 0.061000674962997437\n",
            "Epoch 89045: train loss: 0.060998935252428055\n",
            "Epoch 89046: train loss: 0.060997191816568375\n",
            "Epoch 89047: train loss: 0.06099545583128929\n",
            "Epoch 89048: train loss: 0.060993727296590805\n",
            "Epoch 89049: train loss: 0.06099198758602142\n",
            "Epoch 89050: train loss: 0.06099024415016174\n",
            "Epoch 89051: train loss: 0.060988523066043854\n",
            "Epoch 89052: train loss: 0.06098677217960358\n",
            "Epoch 89053: train loss: 0.06098503991961479\n",
            "Epoch 89054: train loss: 0.06098330393433571\n",
            "Epoch 89055: train loss: 0.06098156049847603\n",
            "Epoch 89056: train loss: 0.060979828238487244\n",
            "Epoch 89057: train loss: 0.06097809225320816\n",
            "Epoch 89058: train loss: 0.06097635626792908\n",
            "Epoch 89059: train loss: 0.0609746128320694\n",
            "Epoch 89060: train loss: 0.06097286939620972\n",
            "Epoch 89061: train loss: 0.060971129685640335\n",
            "Epoch 89062: train loss: 0.06096939370036125\n",
            "Epoch 89063: train loss: 0.06096765771508217\n",
            "Epoch 89064: train loss: 0.06096591800451279\n",
            "Epoch 89065: train loss: 0.0609641894698143\n",
            "Epoch 89066: train loss: 0.06096245348453522\n",
            "Epoch 89067: train loss: 0.060960713773965836\n",
            "Epoch 89068: train loss: 0.06095897778868675\n",
            "Epoch 89069: train loss: 0.06095724180340767\n",
            "Epoch 89070: train loss: 0.06095549836754799\n",
            "Epoch 89071: train loss: 0.0609537735581398\n",
            "Epoch 89072: train loss: 0.06095203757286072\n",
            "Epoch 89073: train loss: 0.060950297862291336\n",
            "Epoch 89074: train loss: 0.06094856187701225\n",
            "Epoch 89075: train loss: 0.060946833342313766\n",
            "Epoch 89076: train loss: 0.060945093631744385\n",
            "Epoch 89077: train loss: 0.0609433650970459\n",
            "Epoch 89078: train loss: 0.060941629111766815\n",
            "Epoch 89079: train loss: 0.060939889401197433\n",
            "Epoch 89080: train loss: 0.06093816086649895\n",
            "Epoch 89081: train loss: 0.06093643233180046\n",
            "Epoch 89082: train loss: 0.06093468517065048\n",
            "Epoch 89083: train loss: 0.060932956635951996\n",
            "Epoch 89084: train loss: 0.06093123182654381\n",
            "Epoch 89085: train loss: 0.06092950329184532\n",
            "Epoch 89086: train loss: 0.06092776730656624\n",
            "Epoch 89087: train loss: 0.06092602759599686\n",
            "Epoch 89088: train loss: 0.06092429161071777\n",
            "Epoch 89089: train loss: 0.06092255562543869\n",
            "Epoch 89090: train loss: 0.060920823365449905\n",
            "Epoch 89091: train loss: 0.06091909483075142\n",
            "Epoch 89092: train loss: 0.060917362570762634\n",
            "Epoch 89093: train loss: 0.06091563403606415\n",
            "Epoch 89094: train loss: 0.060913898050785065\n",
            "Epoch 89095: train loss: 0.06091217324137688\n",
            "Epoch 89096: train loss: 0.06091044470667839\n",
            "Epoch 89097: train loss: 0.060908712446689606\n",
            "Epoch 89098: train loss: 0.06090698763728142\n",
            "Epoch 89099: train loss: 0.06090524420142174\n",
            "Epoch 89100: train loss: 0.060903530567884445\n",
            "Epoch 89101: train loss: 0.06090179830789566\n",
            "Epoch 89102: train loss: 0.060900069773197174\n",
            "Epoch 89103: train loss: 0.06089833751320839\n",
            "Epoch 89104: train loss: 0.0608966164290905\n",
            "Epoch 89105: train loss: 0.06089489161968231\n",
            "Epoch 89106: train loss: 0.06089315563440323\n",
            "Epoch 89107: train loss: 0.060891423374414444\n",
            "Epoch 89108: train loss: 0.06088970601558685\n",
            "Epoch 89109: train loss: 0.06088797003030777\n",
            "Epoch 89110: train loss: 0.06088624522089958\n",
            "Epoch 89111: train loss: 0.060884516686201096\n",
            "Epoch 89112: train loss: 0.06088278815150261\n",
            "Epoch 89113: train loss: 0.06088107079267502\n",
            "Epoch 89114: train loss: 0.06087933108210564\n",
            "Epoch 89115: train loss: 0.06087760254740715\n",
            "Epoch 89116: train loss: 0.060875874012708664\n",
            "Epoch 89117: train loss: 0.060874149203300476\n",
            "Epoch 89118: train loss: 0.06087241694331169\n",
            "Epoch 89119: train loss: 0.060870688408613205\n",
            "Epoch 89120: train loss: 0.060868971049785614\n",
            "Epoch 89121: train loss: 0.06086724251508713\n",
            "Epoch 89122: train loss: 0.06086551025509834\n",
            "Epoch 89123: train loss: 0.06086378172039986\n",
            "Epoch 89124: train loss: 0.06086205691099167\n",
            "Epoch 89125: train loss: 0.06086033582687378\n",
            "Epoch 89126: train loss: 0.060858603566884995\n",
            "Epoch 89127: train loss: 0.06085687503218651\n",
            "Epoch 89128: train loss: 0.06085515022277832\n",
            "Epoch 89129: train loss: 0.060853421688079834\n",
            "Epoch 89130: train loss: 0.060851696878671646\n",
            "Epoch 89131: train loss: 0.06084996461868286\n",
            "Epoch 89132: train loss: 0.06084825098514557\n",
            "Epoch 89133: train loss: 0.06084651127457619\n",
            "Epoch 89134: train loss: 0.0608447901904583\n",
            "Epoch 89135: train loss: 0.06084305793046951\n",
            "Epoch 89136: train loss: 0.060841336846351624\n",
            "Epoch 89137: train loss: 0.060839612036943436\n",
            "Epoch 89138: train loss: 0.06083788350224495\n",
            "Epoch 89139: train loss: 0.06083615869283676\n",
            "Epoch 89140: train loss: 0.06083442643284798\n",
            "Epoch 89141: train loss: 0.06083270534873009\n",
            "Epoch 89142: train loss: 0.060830987989902496\n",
            "Epoch 89143: train loss: 0.06082926318049431\n",
            "Epoch 89144: train loss: 0.060827527195215225\n",
            "Epoch 89145: train loss: 0.060825809836387634\n",
            "Epoch 89146: train loss: 0.06082408130168915\n",
            "Epoch 89147: train loss: 0.06082236394286156\n",
            "Epoch 89148: train loss: 0.06082063540816307\n",
            "Epoch 89149: train loss: 0.060818903148174286\n",
            "Epoch 89150: train loss: 0.060817185789346695\n",
            "Epoch 89151: train loss: 0.060815464705228806\n",
            "Epoch 89152: train loss: 0.06081373244524002\n",
            "Epoch 89153: train loss: 0.06081201508641243\n",
            "Epoch 89154: train loss: 0.06081029400229454\n",
            "Epoch 89155: train loss: 0.060808561742305756\n",
            "Epoch 89156: train loss: 0.060806840658187866\n",
            "Epoch 89157: train loss: 0.06080511584877968\n",
            "Epoch 89158: train loss: 0.06080339476466179\n",
            "Epoch 89159: train loss: 0.0608016699552536\n",
            "Epoch 89160: train loss: 0.060799937695264816\n",
            "Epoch 89161: train loss: 0.060798224061727524\n",
            "Epoch 89162: train loss: 0.060796499252319336\n",
            "Epoch 89163: train loss: 0.06079477071762085\n",
            "Epoch 89164: train loss: 0.06079304590821266\n",
            "Epoch 89165: train loss: 0.06079133599996567\n",
            "Epoch 89166: train loss: 0.06078960373997688\n",
            "Epoch 89167: train loss: 0.060787882655858994\n",
            "Epoch 89168: train loss: 0.060786157846450806\n",
            "Epoch 89169: train loss: 0.060784436762332916\n",
            "Epoch 89170: train loss: 0.060782719403505325\n",
            "Epoch 89171: train loss: 0.06078098714351654\n",
            "Epoch 89172: train loss: 0.06077927350997925\n",
            "Epoch 89173: train loss: 0.06077754870057106\n",
            "Epoch 89174: train loss: 0.06077582389116287\n",
            "Epoch 89175: train loss: 0.060774095356464386\n",
            "Epoch 89176: train loss: 0.06077238544821739\n",
            "Epoch 89177: train loss: 0.060770660638809204\n",
            "Epoch 89178: train loss: 0.060768939554691315\n",
            "Epoch 89179: train loss: 0.06076721474528313\n",
            "Epoch 89180: train loss: 0.06076548993587494\n",
            "Epoch 89181: train loss: 0.06076376140117645\n",
            "Epoch 89182: train loss: 0.060762036591768265\n",
            "Epoch 89183: train loss: 0.060760319232940674\n",
            "Epoch 89184: train loss: 0.060758598148822784\n",
            "Epoch 89185: train loss: 0.0607568733394146\n",
            "Epoch 89186: train loss: 0.060755155980587006\n",
            "Epoch 89187: train loss: 0.060753434896469116\n",
            "Epoch 89188: train loss: 0.06075171008706093\n",
            "Epoch 89189: train loss: 0.06074999272823334\n",
            "Epoch 89190: train loss: 0.06074827164411545\n",
            "Epoch 89191: train loss: 0.06074655428528786\n",
            "Epoch 89192: train loss: 0.060744836926460266\n",
            "Epoch 89193: train loss: 0.060743119567632675\n",
            "Epoch 89194: train loss: 0.060741398483514786\n",
            "Epoch 89195: train loss: 0.060739681124687195\n",
            "Epoch 89196: train loss: 0.060737963765859604\n",
            "Epoch 89197: train loss: 0.06073624640703201\n",
            "Epoch 89198: train loss: 0.060734525322914124\n",
            "Epoch 89199: train loss: 0.06073280796408653\n",
            "Epoch 89200: train loss: 0.06073109805583954\n",
            "Epoch 89201: train loss: 0.060729365795850754\n",
            "Epoch 89202: train loss: 0.06072765216231346\n",
            "Epoch 89203: train loss: 0.06072594225406647\n",
            "Epoch 89204: train loss: 0.060724224895238876\n",
            "Epoch 89205: train loss: 0.06072250008583069\n",
            "Epoch 89206: train loss: 0.0607207827270031\n",
            "Epoch 89207: train loss: 0.06071906164288521\n",
            "Epoch 89208: train loss: 0.06071733683347702\n",
            "Epoch 89209: train loss: 0.060715626925230026\n",
            "Epoch 89210: train loss: 0.06071390584111214\n",
            "Epoch 89211: train loss: 0.06071219593286514\n",
            "Epoch 89212: train loss: 0.06071047857403755\n",
            "Epoch 89213: train loss: 0.060708753764629364\n",
            "Epoch 89214: train loss: 0.06070703640580177\n",
            "Epoch 89215: train loss: 0.06070532277226448\n",
            "Epoch 89216: train loss: 0.06070360541343689\n",
            "Epoch 89217: train loss: 0.0607018806040287\n",
            "Epoch 89218: train loss: 0.06070017069578171\n",
            "Epoch 89219: train loss: 0.06069845333695412\n",
            "Epoch 89220: train loss: 0.060696739703416824\n",
            "Epoch 89221: train loss: 0.060695014894008636\n",
            "Epoch 89222: train loss: 0.06069330498576164\n",
            "Epoch 89223: train loss: 0.06069158762693405\n",
            "Epoch 89224: train loss: 0.06068986654281616\n",
            "Epoch 89225: train loss: 0.06068814918398857\n",
            "Epoch 89226: train loss: 0.06068643927574158\n",
            "Epoch 89227: train loss: 0.06068472936749458\n",
            "Epoch 89228: train loss: 0.060683004558086395\n",
            "Epoch 89229: train loss: 0.060681283473968506\n",
            "Epoch 89230: train loss: 0.06067955866456032\n",
            "Epoch 89231: train loss: 0.060677848756313324\n",
            "Epoch 89232: train loss: 0.06067613139748573\n",
            "Epoch 89233: train loss: 0.06067441403865814\n",
            "Epoch 89234: train loss: 0.06067270413041115\n",
            "Epoch 89235: train loss: 0.06067098304629326\n",
            "Epoch 89236: train loss: 0.060669273138046265\n",
            "Epoch 89237: train loss: 0.060667555779218674\n",
            "Epoch 89238: train loss: 0.060665830969810486\n",
            "Epoch 89239: train loss: 0.06066412106156349\n",
            "Epoch 89240: train loss: 0.0606624074280262\n",
            "Epoch 89241: train loss: 0.06066069006919861\n",
            "Epoch 89242: train loss: 0.06065897271037102\n",
            "Epoch 89243: train loss: 0.060657255351543427\n",
            "Epoch 89244: train loss: 0.06065554544329643\n",
            "Epoch 89245: train loss: 0.06065382435917854\n",
            "Epoch 89246: train loss: 0.06065210700035095\n",
            "Epoch 89247: train loss: 0.06065038964152336\n",
            "Epoch 89248: train loss: 0.06064867973327637\n",
            "Epoch 89249: train loss: 0.06064695492386818\n",
            "Epoch 89250: train loss: 0.060645245015621185\n",
            "Epoch 89251: train loss: 0.06064353138208389\n",
            "Epoch 89252: train loss: 0.0606418214738369\n",
            "Epoch 89253: train loss: 0.06064009666442871\n",
            "Epoch 89254: train loss: 0.06063838675618172\n",
            "Epoch 89255: train loss: 0.060636669397354126\n",
            "Epoch 89256: train loss: 0.060634952038526535\n",
            "Epoch 89257: train loss: 0.06063324958086014\n",
            "Epoch 89258: train loss: 0.06063152849674225\n",
            "Epoch 89259: train loss: 0.060629818588495255\n",
            "Epoch 89260: train loss: 0.060628101229667664\n",
            "Epoch 89261: train loss: 0.06062638387084007\n",
            "Epoch 89262: train loss: 0.06062467396259308\n",
            "Epoch 89263: train loss: 0.06062295660376549\n",
            "Epoch 89264: train loss: 0.060621246695518494\n",
            "Epoch 89265: train loss: 0.060619525611400604\n",
            "Epoch 89266: train loss: 0.06061781570315361\n",
            "Epoch 89267: train loss: 0.06061609089374542\n",
            "Epoch 89268: train loss: 0.06061438098549843\n",
            "Epoch 89269: train loss: 0.06061267852783203\n",
            "Epoch 89270: train loss: 0.060610949993133545\n",
            "Epoch 89271: train loss: 0.060609232634305954\n",
            "Epoch 89272: train loss: 0.06060753017663956\n",
            "Epoch 89273: train loss: 0.060605812817811966\n",
            "Epoch 89274: train loss: 0.06060410290956497\n",
            "Epoch 89275: train loss: 0.06060239300131798\n",
            "Epoch 89276: train loss: 0.06060067564249039\n",
            "Epoch 89277: train loss: 0.060598958283662796\n",
            "Epoch 89278: train loss: 0.0605972483754158\n",
            "Epoch 89279: train loss: 0.06059552729129791\n",
            "Epoch 89280: train loss: 0.06059381738305092\n",
            "Epoch 89281: train loss: 0.060592107474803925\n",
            "Epoch 89282: train loss: 0.060590390115976334\n",
            "Epoch 89283: train loss: 0.06058868020772934\n",
            "Epoch 89284: train loss: 0.06058696284890175\n",
            "Epoch 89285: train loss: 0.060585252940654755\n",
            "Epoch 89286: train loss: 0.060583531856536865\n",
            "Epoch 89287: train loss: 0.06058182567358017\n",
            "Epoch 89288: train loss: 0.060580115765333176\n",
            "Epoch 89289: train loss: 0.06057840213179588\n",
            "Epoch 89290: train loss: 0.06057669222354889\n",
            "Epoch 89291: train loss: 0.0605749748647213\n",
            "Epoch 89292: train loss: 0.0605732724070549\n",
            "Epoch 89293: train loss: 0.06057154759764671\n",
            "Epoch 89294: train loss: 0.06056983023881912\n",
            "Epoch 89295: train loss: 0.06056812033057213\n",
            "Epoch 89296: train loss: 0.06056641787290573\n",
            "Epoch 89297: train loss: 0.06056470051407814\n",
            "Epoch 89298: train loss: 0.06056298688054085\n",
            "Epoch 89299: train loss: 0.060561276972293854\n",
            "Epoch 89300: train loss: 0.06055955961346626\n",
            "Epoch 89301: train loss: 0.06055784970521927\n",
            "Epoch 89302: train loss: 0.060556139796972275\n",
            "Epoch 89303: train loss: 0.060554422438144684\n",
            "Epoch 89304: train loss: 0.06055271252989769\n",
            "Epoch 89305: train loss: 0.0605509951710701\n",
            "Epoch 89306: train loss: 0.06054927781224251\n",
            "Epoch 89307: train loss: 0.06054757162928581\n",
            "Epoch 89308: train loss: 0.06054586172103882\n",
            "Epoch 89309: train loss: 0.06054415553808212\n",
            "Epoch 89310: train loss: 0.06054243445396423\n",
            "Epoch 89311: train loss: 0.06054072454571724\n",
            "Epoch 89312: train loss: 0.060539014637470245\n",
            "Epoch 89313: train loss: 0.06053730472922325\n",
            "Epoch 89314: train loss: 0.060535579919815063\n",
            "Epoch 89315: train loss: 0.060533877462148666\n",
            "Epoch 89316: train loss: 0.06053216755390167\n",
            "Epoch 89317: train loss: 0.06053045764565468\n",
            "Epoch 89318: train loss: 0.060528747737407684\n",
            "Epoch 89319: train loss: 0.06052703037858009\n",
            "Epoch 89320: train loss: 0.060525327920913696\n",
            "Epoch 89321: train loss: 0.0605236180126667\n",
            "Epoch 89322: train loss: 0.06052190810441971\n",
            "Epoch 89323: train loss: 0.06052018702030182\n",
            "Epoch 89324: train loss: 0.06051848456263542\n",
            "Epoch 89325: train loss: 0.06051676720380783\n",
            "Epoch 89326: train loss: 0.06051505729556084\n",
            "Epoch 89327: train loss: 0.06051334738731384\n",
            "Epoch 89328: train loss: 0.06051163747906685\n",
            "Epoch 89329: train loss: 0.060509927570819855\n",
            "Epoch 89330: train loss: 0.06050822511315346\n",
            "Epoch 89331: train loss: 0.06050650775432587\n",
            "Epoch 89332: train loss: 0.06050479784607887\n",
            "Epoch 89333: train loss: 0.06050310283899307\n",
            "Epoch 89334: train loss: 0.06050138548016548\n",
            "Epoch 89335: train loss: 0.06049966812133789\n",
            "Epoch 89336: train loss: 0.0604979507625103\n",
            "Epoch 89337: train loss: 0.0604962557554245\n",
            "Epoch 89338: train loss: 0.06049453839659691\n",
            "Epoch 89339: train loss: 0.060492828488349915\n",
            "Epoch 89340: train loss: 0.06049111858010292\n",
            "Epoch 89341: train loss: 0.060489408671855927\n",
            "Epoch 89342: train loss: 0.06048769876360893\n",
            "Epoch 89343: train loss: 0.06048598885536194\n",
            "Epoch 89344: train loss: 0.060484278947114944\n",
            "Epoch 89345: train loss: 0.06048257648944855\n",
            "Epoch 89346: train loss: 0.06048086658120155\n",
            "Epoch 89347: train loss: 0.06047915667295456\n",
            "Epoch 89348: train loss: 0.060477446764707565\n",
            "Epoch 89349: train loss: 0.06047573685646057\n",
            "Epoch 89350: train loss: 0.060474034398794174\n",
            "Epoch 89351: train loss: 0.06047232449054718\n",
            "Epoch 89352: train loss: 0.060470614582300186\n",
            "Epoch 89353: train loss: 0.06046890467405319\n",
            "Epoch 89354: train loss: 0.0604671947658062\n",
            "Epoch 89355: train loss: 0.060465484857559204\n",
            "Epoch 89356: train loss: 0.06046378239989281\n",
            "Epoch 89357: train loss: 0.060462065041065216\n",
            "Epoch 89358: train loss: 0.06046036258339882\n",
            "Epoch 89359: train loss: 0.060458652675151825\n",
            "Epoch 89360: train loss: 0.06045694276690483\n",
            "Epoch 89361: train loss: 0.060455240309238434\n",
            "Epoch 89362: train loss: 0.06045352295041084\n",
            "Epoch 89363: train loss: 0.060451820492744446\n",
            "Epoch 89364: train loss: 0.060450125485658646\n",
            "Epoch 89365: train loss: 0.060448408126831055\n",
            "Epoch 89366: train loss: 0.06044669821858406\n",
            "Epoch 89367: train loss: 0.060444995760917664\n",
            "Epoch 89368: train loss: 0.06044328585267067\n",
            "Epoch 89369: train loss: 0.060441575944423676\n",
            "Epoch 89370: train loss: 0.06043987348675728\n",
            "Epoch 89371: train loss: 0.06043815612792969\n",
            "Epoch 89372: train loss: 0.06043645367026329\n",
            "Epoch 89373: train loss: 0.06043475121259689\n",
            "Epoch 89374: train loss: 0.060433048754930496\n",
            "Epoch 89375: train loss: 0.0604313388466835\n",
            "Epoch 89376: train loss: 0.06042963266372681\n",
            "Epoch 89377: train loss: 0.06042792275547981\n",
            "Epoch 89378: train loss: 0.060426220297813416\n",
            "Epoch 89379: train loss: 0.06042450666427612\n",
            "Epoch 89380: train loss: 0.06042279675602913\n",
            "Epoch 89381: train loss: 0.060421090573072433\n",
            "Epoch 89382: train loss: 0.06041938066482544\n",
            "Epoch 89383: train loss: 0.060417670756578445\n",
            "Epoch 89384: train loss: 0.060415975749492645\n",
            "Epoch 89385: train loss: 0.060414258390665054\n",
            "Epoch 89386: train loss: 0.06041255593299866\n",
            "Epoch 89387: train loss: 0.06041085347533226\n",
            "Epoch 89388: train loss: 0.06040915101766586\n",
            "Epoch 89389: train loss: 0.06040744110941887\n",
            "Epoch 89390: train loss: 0.06040573865175247\n",
            "Epoch 89391: train loss: 0.06040402129292488\n",
            "Epoch 89392: train loss: 0.060402318835258484\n",
            "Epoch 89393: train loss: 0.06040061637759209\n",
            "Epoch 89394: train loss: 0.06039891391992569\n",
            "Epoch 89395: train loss: 0.060397204011678696\n",
            "Epoch 89396: train loss: 0.0603954941034317\n",
            "Epoch 89397: train loss: 0.060393791645765305\n",
            "Epoch 89398: train loss: 0.06039208546280861\n",
            "Epoch 89399: train loss: 0.060390375554561615\n",
            "Epoch 89400: train loss: 0.060388680547475815\n",
            "Epoch 89401: train loss: 0.06038697808980942\n",
            "Epoch 89402: train loss: 0.060385268181562424\n",
            "Epoch 89403: train loss: 0.06038355827331543\n",
            "Epoch 89404: train loss: 0.06038186326622963\n",
            "Epoch 89405: train loss: 0.06038016080856323\n",
            "Epoch 89406: train loss: 0.06037845462560654\n",
            "Epoch 89407: train loss: 0.06037675216794014\n",
            "Epoch 89408: train loss: 0.060375042259693146\n",
            "Epoch 89409: train loss: 0.06037333980202675\n",
            "Epoch 89410: train loss: 0.060371629893779755\n",
            "Epoch 89411: train loss: 0.060369934886693954\n",
            "Epoch 89412: train loss: 0.06036823242902756\n",
            "Epoch 89413: train loss: 0.06036653369665146\n",
            "Epoch 89414: train loss: 0.060364820063114166\n",
            "Epoch 89415: train loss: 0.060363128781318665\n",
            "Epoch 89416: train loss: 0.06036142632365227\n",
            "Epoch 89417: train loss: 0.06035972386598587\n",
            "Epoch 89418: train loss: 0.060358013957738876\n",
            "Epoch 89419: train loss: 0.06035631150007248\n",
            "Epoch 89420: train loss: 0.060354601591825485\n",
            "Epoch 89421: train loss: 0.06035290285944939\n",
            "Epoch 89422: train loss: 0.06035120040178299\n",
            "Epoch 89423: train loss: 0.06034949794411659\n",
            "Epoch 89424: train loss: 0.060347795486450195\n",
            "Epoch 89425: train loss: 0.06034610792994499\n",
            "Epoch 89426: train loss: 0.060344401746988297\n",
            "Epoch 89427: train loss: 0.0603426918387413\n",
            "Epoch 89428: train loss: 0.0603410042822361\n",
            "Epoch 89429: train loss: 0.0603393018245697\n",
            "Epoch 89430: train loss: 0.060337603092193604\n",
            "Epoch 89431: train loss: 0.060335900634527206\n",
            "Epoch 89432: train loss: 0.060334205627441406\n",
            "Epoch 89433: train loss: 0.060332510620355606\n",
            "Epoch 89434: train loss: 0.060330819338560104\n",
            "Epoch 89435: train loss: 0.06032910943031311\n",
            "Epoch 89436: train loss: 0.06032740697264671\n",
            "Epoch 89437: train loss: 0.06032571196556091\n",
            "Epoch 89438: train loss: 0.060324013233184814\n",
            "Epoch 89439: train loss: 0.06032231077551842\n",
            "Epoch 89440: train loss: 0.06032061576843262\n",
            "Epoch 89441: train loss: 0.06031892076134682\n",
            "Epoch 89442: train loss: 0.06031721085309982\n",
            "Epoch 89443: train loss: 0.060315512120723724\n",
            "Epoch 89444: train loss: 0.06031382456421852\n",
            "Epoch 89445: train loss: 0.06031211465597153\n",
            "Epoch 89446: train loss: 0.06031041964888573\n",
            "Epoch 89447: train loss: 0.06030872091650963\n",
            "Epoch 89448: train loss: 0.06030702590942383\n",
            "Epoch 89449: train loss: 0.06030533090233803\n",
            "Epoch 89450: train loss: 0.06030363216996193\n",
            "Epoch 89451: train loss: 0.060301944613456726\n",
            "Epoch 89452: train loss: 0.060300253331661224\n",
            "Epoch 89453: train loss: 0.06029855087399483\n",
            "Epoch 89454: train loss: 0.06029685586690903\n",
            "Epoch 89455: train loss: 0.060295164585113525\n",
            "Epoch 89456: train loss: 0.060293469578027725\n",
            "Epoch 89457: train loss: 0.06029178574681282\n",
            "Epoch 89458: train loss: 0.06029009073972702\n",
            "Epoch 89459: train loss: 0.06028838828206062\n",
            "Epoch 89460: train loss: 0.06028670445084572\n",
            "Epoch 89461: train loss: 0.06028500944375992\n",
            "Epoch 89462: train loss: 0.06028331443667412\n",
            "Epoch 89463: train loss: 0.06028163060545921\n",
            "Epoch 89464: train loss: 0.06027993559837341\n",
            "Epoch 89465: train loss: 0.060278236865997314\n",
            "Epoch 89466: train loss: 0.060276541858911514\n",
            "Epoch 89467: train loss: 0.06027485802769661\n",
            "Epoch 89468: train loss: 0.06027315557003021\n",
            "Epoch 89469: train loss: 0.06027146801352501\n",
            "Epoch 89470: train loss: 0.06026977300643921\n",
            "Epoch 89471: train loss: 0.06026808172464371\n",
            "Epoch 89472: train loss: 0.060266394168138504\n",
            "Epoch 89473: train loss: 0.060264695435762405\n",
            "Epoch 89474: train loss: 0.0602630153298378\n",
            "Epoch 89475: train loss: 0.0602613165974617\n",
            "Epoch 89476: train loss: 0.0602596290409565\n",
            "Epoch 89477: train loss: 0.0602579303085804\n",
            "Epoch 89478: train loss: 0.0602562353014946\n",
            "Epoch 89479: train loss: 0.060254547744989395\n",
            "Epoch 89480: train loss: 0.06025285646319389\n",
            "Epoch 89481: train loss: 0.06025116145610809\n",
            "Epoch 89482: train loss: 0.06024947017431259\n",
            "Epoch 89483: train loss: 0.06024777516722679\n",
            "Epoch 89484: train loss: 0.06024608016014099\n",
            "Epoch 89485: train loss: 0.060244396328926086\n",
            "Epoch 89486: train loss: 0.06024270877242088\n",
            "Epoch 89487: train loss: 0.06024101749062538\n",
            "Epoch 89488: train loss: 0.06023932248353958\n",
            "Epoch 89489: train loss: 0.06023762747645378\n",
            "Epoch 89490: train loss: 0.060235943645238876\n",
            "Epoch 89491: train loss: 0.060234248638153076\n",
            "Epoch 89492: train loss: 0.06023256480693817\n",
            "Epoch 89493: train loss: 0.06023087352514267\n",
            "Epoch 89494: train loss: 0.060229163616895676\n",
            "Epoch 89495: train loss: 0.06022748351097107\n",
            "Epoch 89496: train loss: 0.06022579222917557\n",
            "Epoch 89497: train loss: 0.060224104672670364\n",
            "Epoch 89498: train loss: 0.06022241339087486\n",
            "Epoch 89499: train loss: 0.06022071838378906\n",
            "Epoch 89500: train loss: 0.06021903082728386\n",
            "Epoch 89501: train loss: 0.06021733209490776\n",
            "Epoch 89502: train loss: 0.06021563708782196\n",
            "Epoch 89503: train loss: 0.06021396070718765\n",
            "Epoch 89504: train loss: 0.06021226570010185\n",
            "Epoch 89505: train loss: 0.06021058186888695\n",
            "Epoch 89506: train loss: 0.060208894312381744\n",
            "Epoch 89507: train loss: 0.060207195580005646\n",
            "Epoch 89508: train loss: 0.06020551547408104\n",
            "Epoch 89509: train loss: 0.06020381674170494\n",
            "Epoch 89510: train loss: 0.06020212173461914\n",
            "Epoch 89511: train loss: 0.06020044535398483\n",
            "Epoch 89512: train loss: 0.06019875779747963\n",
            "Epoch 89513: train loss: 0.06019705533981323\n",
            "Epoch 89514: train loss: 0.060195378959178925\n",
            "Epoch 89515: train loss: 0.06019367650151253\n",
            "Epoch 89516: train loss: 0.06019199267029762\n",
            "Epoch 89517: train loss: 0.06019029766321182\n",
            "Epoch 89518: train loss: 0.06018861383199692\n",
            "Epoch 89519: train loss: 0.060186922550201416\n",
            "Epoch 89520: train loss: 0.06018523499369621\n",
            "Epoch 89521: train loss: 0.06018355116248131\n",
            "Epoch 89522: train loss: 0.0601818710565567\n",
            "Epoch 89523: train loss: 0.0601801797747612\n",
            "Epoch 89524: train loss: 0.0601784847676754\n",
            "Epoch 89525: train loss: 0.060176800936460495\n",
            "Epoch 89526: train loss: 0.06017512083053589\n",
            "Epoch 89527: train loss: 0.060173436999320984\n",
            "Epoch 89528: train loss: 0.06017174571752548\n",
            "Epoch 89529: train loss: 0.06017005071043968\n",
            "Epoch 89530: train loss: 0.060168374329805374\n",
            "Epoch 89531: train loss: 0.06016668677330017\n",
            "Epoch 89532: train loss: 0.06016499549150467\n",
            "Epoch 89533: train loss: 0.060163307934999466\n",
            "Epoch 89534: train loss: 0.060161616653203964\n",
            "Epoch 89535: train loss: 0.060159940272569656\n",
            "Epoch 89536: train loss: 0.06015825271606445\n",
            "Epoch 89537: train loss: 0.06015656515955925\n",
            "Epoch 89538: train loss: 0.060154881328344345\n",
            "Epoch 89539: train loss: 0.060153186321258545\n",
            "Epoch 89540: train loss: 0.06015150249004364\n",
            "Epoch 89541: train loss: 0.060149818658828735\n",
            "Epoch 89542: train loss: 0.06014813110232353\n",
            "Epoch 89543: train loss: 0.060146454721689224\n",
            "Epoch 89544: train loss: 0.060144759714603424\n",
            "Epoch 89545: train loss: 0.06014307588338852\n",
            "Epoch 89546: train loss: 0.06014138460159302\n",
            "Epoch 89547: train loss: 0.06013970449566841\n",
            "Epoch 89548: train loss: 0.06013801321387291\n",
            "Epoch 89549: train loss: 0.0601363331079483\n",
            "Epoch 89550: train loss: 0.0601346418261528\n",
            "Epoch 89551: train loss: 0.060132961720228195\n",
            "Epoch 89552: train loss: 0.060131270438432693\n",
            "Epoch 89553: train loss: 0.06012958660721779\n",
            "Epoch 89554: train loss: 0.06012790650129318\n",
            "Epoch 89555: train loss: 0.06012622267007828\n",
            "Epoch 89556: train loss: 0.06012452766299248\n",
            "Epoch 89557: train loss: 0.06012284383177757\n",
            "Epoch 89558: train loss: 0.06012115627527237\n",
            "Epoch 89559: train loss: 0.060119472444057465\n",
            "Epoch 89560: train loss: 0.06011778861284256\n",
            "Epoch 89561: train loss: 0.06011610105633736\n",
            "Epoch 89562: train loss: 0.06011441722512245\n",
            "Epoch 89563: train loss: 0.060112740844488144\n",
            "Epoch 89564: train loss: 0.06011105328798294\n",
            "Epoch 89565: train loss: 0.06010935828089714\n",
            "Epoch 89566: train loss: 0.060107674449682236\n",
            "Epoch 89567: train loss: 0.060105983167886734\n",
            "Epoch 89568: train loss: 0.06010431423783302\n",
            "Epoch 89569: train loss: 0.06010261923074722\n",
            "Epoch 89570: train loss: 0.060100942850112915\n",
            "Epoch 89571: train loss: 0.06009925529360771\n",
            "Epoch 89572: train loss: 0.06009756401181221\n",
            "Epoch 89573: train loss: 0.0600958876311779\n",
            "Epoch 89574: train loss: 0.0600941926240921\n",
            "Epoch 89575: train loss: 0.060092516243457794\n",
            "Epoch 89576: train loss: 0.06009082868695259\n",
            "Epoch 89577: train loss: 0.06008915230631828\n",
            "Epoch 89578: train loss: 0.06008746102452278\n",
            "Epoch 89579: train loss: 0.06008577346801758\n",
            "Epoch 89580: train loss: 0.06008409708738327\n",
            "Epoch 89581: train loss: 0.060082416981458664\n",
            "Epoch 89582: train loss: 0.06008073315024376\n",
            "Epoch 89583: train loss: 0.06007904186844826\n",
            "Epoch 89584: train loss: 0.06007736176252365\n",
            "Epoch 89585: train loss: 0.060075677931308746\n",
            "Epoch 89586: train loss: 0.06007400155067444\n",
            "Epoch 89587: train loss: 0.060072313994169235\n",
            "Epoch 89588: train loss: 0.06007063761353493\n",
            "Epoch 89589: train loss: 0.06006894260644913\n",
            "Epoch 89590: train loss: 0.060067273676395416\n",
            "Epoch 89591: train loss: 0.06006558984518051\n",
            "Epoch 89592: train loss: 0.06006389483809471\n",
            "Epoch 89593: train loss: 0.060062211006879807\n",
            "Epoch 89594: train loss: 0.0600605271756649\n",
            "Epoch 89595: train loss: 0.060058850795030594\n",
            "Epoch 89596: train loss: 0.06005717068910599\n",
            "Epoch 89597: train loss: 0.060055479407310486\n",
            "Epoch 89598: train loss: 0.060053806751966476\n",
            "Epoch 89599: train loss: 0.060052115470170975\n",
            "Epoch 89600: train loss: 0.06005043163895607\n",
            "Epoch 89601: train loss: 0.06004874408245087\n",
            "Epoch 89602: train loss: 0.06004706770181656\n",
            "Epoch 89603: train loss: 0.06004539877176285\n",
            "Epoch 89604: train loss: 0.060043711215257645\n",
            "Epoch 89605: train loss: 0.06004203483462334\n",
            "Epoch 89606: train loss: 0.06004035100340843\n",
            "Epoch 89607: train loss: 0.06003866717219353\n",
            "Epoch 89608: train loss: 0.06003698706626892\n",
            "Epoch 89609: train loss: 0.060035303235054016\n",
            "Epoch 89610: train loss: 0.06003361940383911\n",
            "Epoch 89611: train loss: 0.0600319467484951\n",
            "Epoch 89612: train loss: 0.060030270367860794\n",
            "Epoch 89613: train loss: 0.06002858653664589\n",
            "Epoch 89614: train loss: 0.060026902705430984\n",
            "Epoch 89615: train loss: 0.06002522259950638\n",
            "Epoch 89616: train loss: 0.06002354621887207\n",
            "Epoch 89617: train loss: 0.060021862387657166\n",
            "Epoch 89618: train loss: 0.060020189732313156\n",
            "Epoch 89619: train loss: 0.06001850590109825\n",
            "Epoch 89620: train loss: 0.060016829520463943\n",
            "Epoch 89621: train loss: 0.06001514568924904\n",
            "Epoch 89622: train loss: 0.06001346558332443\n",
            "Epoch 89623: train loss: 0.060011789202690125\n",
            "Epoch 89624: train loss: 0.06001010537147522\n",
            "Epoch 89625: train loss: 0.06000842899084091\n",
            "Epoch 89626: train loss: 0.060006748884916306\n",
            "Epoch 89627: train loss: 0.0600050650537014\n",
            "Epoch 89628: train loss: 0.060003381222486496\n",
            "Epoch 89629: train loss: 0.060001712292432785\n",
            "Epoch 89630: train loss: 0.06000003218650818\n",
            "Epoch 89631: train loss: 0.05999834090471268\n",
            "Epoch 89632: train loss: 0.05999666452407837\n",
            "Epoch 89633: train loss: 0.05999499186873436\n",
            "Epoch 89634: train loss: 0.05999331548810005\n",
            "Epoch 89635: train loss: 0.059991639107465744\n",
            "Epoch 89636: train loss: 0.05998995527625084\n",
            "Epoch 89637: train loss: 0.05998827889561653\n",
            "Epoch 89638: train loss: 0.05998659133911133\n",
            "Epoch 89639: train loss: 0.05998490750789642\n",
            "Epoch 89640: train loss: 0.05998323857784271\n",
            "Epoch 89641: train loss: 0.0599815659224987\n",
            "Epoch 89642: train loss: 0.0599798820912838\n",
            "Epoch 89643: train loss: 0.059978190809488297\n",
            "Epoch 89644: train loss: 0.059976521879434586\n",
            "Epoch 89645: train loss: 0.059974849224090576\n",
            "Epoch 89646: train loss: 0.059973157942295074\n",
            "Epoch 89647: train loss: 0.059971489012241364\n",
            "Epoch 89648: train loss: 0.05996979400515556\n",
            "Epoch 89649: train loss: 0.05996813252568245\n",
            "Epoch 89650: train loss: 0.059966448694467545\n",
            "Epoch 89651: train loss: 0.05996476486325264\n",
            "Epoch 89652: train loss: 0.05996308848261833\n",
            "Epoch 89653: train loss: 0.059961408376693726\n",
            "Epoch 89654: train loss: 0.05995973199605942\n",
            "Epoch 89655: train loss: 0.05995805561542511\n",
            "Epoch 89656: train loss: 0.0599563866853714\n",
            "Epoch 89657: train loss: 0.05995471030473709\n",
            "Epoch 89658: train loss: 0.059953030198812485\n",
            "Epoch 89659: train loss: 0.059951361268758774\n",
            "Epoch 89660: train loss: 0.05994966998696327\n",
            "Epoch 89661: train loss: 0.05994800850749016\n",
            "Epoch 89662: train loss: 0.05994633212685585\n",
            "Epoch 89663: train loss: 0.05994464457035065\n",
            "Epoch 89664: train loss: 0.059942975640296936\n",
            "Epoch 89665: train loss: 0.05994129925966263\n",
            "Epoch 89666: train loss: 0.05993963032960892\n",
            "Epoch 89667: train loss: 0.059937961399555206\n",
            "Epoch 89668: train loss: 0.0599362775683403\n",
            "Epoch 89669: train loss: 0.059934601187705994\n",
            "Epoch 89670: train loss: 0.05993293970823288\n",
            "Epoch 89671: train loss: 0.05993125960230827\n",
            "Epoch 89672: train loss: 0.059929583221673965\n",
            "Epoch 89673: train loss: 0.059927914291620255\n",
            "Epoch 89674: train loss: 0.05992623046040535\n",
            "Epoch 89675: train loss: 0.05992456153035164\n",
            "Epoch 89676: train loss: 0.05992288514971733\n",
            "Epoch 89677: train loss: 0.05992121621966362\n",
            "Epoch 89678: train loss: 0.05991953983902931\n",
            "Epoch 89679: train loss: 0.0599178671836853\n",
            "Epoch 89680: train loss: 0.05991619825363159\n",
            "Epoch 89681: train loss: 0.059914521872997284\n",
            "Epoch 89682: train loss: 0.059912845492362976\n",
            "Epoch 89683: train loss: 0.05991116911172867\n",
            "Epoch 89684: train loss: 0.05990950018167496\n",
            "Epoch 89685: train loss: 0.05990782380104065\n",
            "Epoch 89686: train loss: 0.05990615487098694\n",
            "Epoch 89687: train loss: 0.05990448594093323\n",
            "Epoch 89688: train loss: 0.05990280583500862\n",
            "Epoch 89689: train loss: 0.05990114063024521\n",
            "Epoch 89690: train loss: 0.0598994605243206\n",
            "Epoch 89691: train loss: 0.059897784143686295\n",
            "Epoch 89692: train loss: 0.059896115213632584\n",
            "Epoch 89693: train loss: 0.059894438832998276\n",
            "Epoch 89694: train loss: 0.05989277735352516\n",
            "Epoch 89695: train loss: 0.05989109352231026\n",
            "Epoch 89696: train loss: 0.059889424592256546\n",
            "Epoch 89697: train loss: 0.05988774821162224\n",
            "Epoch 89698: train loss: 0.05988607928156853\n",
            "Epoch 89699: train loss: 0.05988440662622452\n",
            "Epoch 89700: train loss: 0.05988273024559021\n",
            "Epoch 89701: train loss: 0.0598810613155365\n",
            "Epoch 89702: train loss: 0.05987938493490219\n",
            "Epoch 89703: train loss: 0.05987771600484848\n",
            "Epoch 89704: train loss: 0.05987603962421417\n",
            "Epoch 89705: train loss: 0.05987437069416046\n",
            "Epoch 89706: train loss: 0.05987270176410675\n",
            "Epoch 89707: train loss: 0.05987102538347244\n",
            "Epoch 89708: train loss: 0.059869349002838135\n",
            "Epoch 89709: train loss: 0.05986768752336502\n",
            "Epoch 89710: train loss: 0.05986601114273071\n",
            "Epoch 89711: train loss: 0.059864338487386703\n",
            "Epoch 89712: train loss: 0.05986266955733299\n",
            "Epoch 89713: train loss: 0.059860993176698685\n",
            "Epoch 89714: train loss: 0.05985933169722557\n",
            "Epoch 89715: train loss: 0.05985766276717186\n",
            "Epoch 89716: train loss: 0.05985598638653755\n",
            "Epoch 89717: train loss: 0.059854310005903244\n",
            "Epoch 89718: train loss: 0.05985265597701073\n",
            "Epoch 89719: train loss: 0.05985097959637642\n",
            "Epoch 89720: train loss: 0.05984931066632271\n",
            "Epoch 89721: train loss: 0.059847641736269\n",
            "Epoch 89722: train loss: 0.05984595790505409\n",
            "Epoch 89723: train loss: 0.059844303876161575\n",
            "Epoch 89724: train loss: 0.05984262749552727\n",
            "Epoch 89725: train loss: 0.059840958565473557\n",
            "Epoch 89726: train loss: 0.059839289635419846\n",
            "Epoch 89727: train loss: 0.05983761325478554\n",
            "Epoch 89728: train loss: 0.059835951775312424\n",
            "Epoch 89729: train loss: 0.05983429029583931\n",
            "Epoch 89730: train loss: 0.059832613915205\n",
            "Epoch 89731: train loss: 0.059830937534570694\n",
            "Epoch 89732: train loss: 0.05982927605509758\n",
            "Epoch 89733: train loss: 0.05982759967446327\n",
            "Epoch 89734: train loss: 0.05982593074440956\n",
            "Epoch 89735: train loss: 0.05982426926493645\n",
            "Epoch 89736: train loss: 0.05982259288430214\n",
            "Epoch 89737: train loss: 0.05982092395424843\n",
            "Epoch 89738: train loss: 0.05981925502419472\n",
            "Epoch 89739: train loss: 0.059817586094141006\n",
            "Epoch 89740: train loss: 0.05981592461466789\n",
            "Epoch 89741: train loss: 0.05981425568461418\n",
            "Epoch 89742: train loss: 0.05981258675456047\n",
            "Epoch 89743: train loss: 0.05981091782450676\n",
            "Epoch 89744: train loss: 0.05980924889445305\n",
            "Epoch 89745: train loss: 0.05980757996439934\n",
            "Epoch 89746: train loss: 0.05980591103434563\n",
            "Epoch 89747: train loss: 0.059804242104291916\n",
            "Epoch 89748: train loss: 0.0598025806248188\n",
            "Epoch 89749: train loss: 0.059800904244184494\n",
            "Epoch 89750: train loss: 0.05979923531413078\n",
            "Epoch 89751: train loss: 0.05979756638407707\n",
            "Epoch 89752: train loss: 0.05979590490460396\n",
            "Epoch 89753: train loss: 0.05979423597455025\n",
            "Epoch 89754: train loss: 0.05979255959391594\n",
            "Epoch 89755: train loss: 0.059790898114442825\n",
            "Epoch 89756: train loss: 0.05978923663496971\n",
            "Epoch 89757: train loss: 0.0597875602543354\n",
            "Epoch 89758: train loss: 0.05978589877486229\n",
            "Epoch 89759: train loss: 0.05978422239422798\n",
            "Epoch 89760: train loss: 0.05978255346417427\n",
            "Epoch 89761: train loss: 0.05978088453412056\n",
            "Epoch 89762: train loss: 0.059779226779937744\n",
            "Epoch 89763: train loss: 0.059777554124593735\n",
            "Epoch 89764: train loss: 0.05977588891983032\n",
            "Epoch 89765: train loss: 0.05977421998977661\n",
            "Epoch 89766: train loss: 0.0597725510597229\n",
            "Epoch 89767: train loss: 0.05977087840437889\n",
            "Epoch 89768: train loss: 0.059769220650196075\n",
            "Epoch 89769: train loss: 0.059767551720142365\n",
            "Epoch 89770: train loss: 0.05976589024066925\n",
            "Epoch 89771: train loss: 0.059764206409454346\n",
            "Epoch 89772: train loss: 0.05976254492998123\n",
            "Epoch 89773: train loss: 0.05976087599992752\n",
            "Epoch 89774: train loss: 0.05975920706987381\n",
            "Epoch 89775: train loss: 0.059757545590400696\n",
            "Epoch 89776: train loss: 0.05975588411092758\n",
            "Epoch 89777: train loss: 0.059754207730293274\n",
            "Epoch 89778: train loss: 0.05975254625082016\n",
            "Epoch 89779: train loss: 0.059750884771347046\n",
            "Epoch 89780: train loss: 0.059749215841293335\n",
            "Epoch 89781: train loss: 0.05974755436182022\n",
            "Epoch 89782: train loss: 0.05974588543176651\n",
            "Epoch 89783: train loss: 0.0597442165017128\n",
            "Epoch 89784: train loss: 0.05974254757165909\n",
            "Epoch 89785: train loss: 0.05974087864160538\n",
            "Epoch 89786: train loss: 0.059739209711551666\n",
            "Epoch 89787: train loss: 0.05973755195736885\n",
            "Epoch 89788: train loss: 0.05973588302731514\n",
            "Epoch 89789: train loss: 0.05973421409726143\n",
            "Epoch 89790: train loss: 0.05973254516720772\n",
            "Epoch 89791: train loss: 0.05973087623715401\n",
            "Epoch 89792: train loss: 0.05972921475768089\n",
            "Epoch 89793: train loss: 0.059727560728788376\n",
            "Epoch 89794: train loss: 0.05972588434815407\n",
            "Epoch 89795: train loss: 0.059724222868680954\n",
            "Epoch 89796: train loss: 0.05972255393862724\n",
            "Epoch 89797: train loss: 0.05972089245915413\n",
            "Epoch 89798: train loss: 0.05971922352910042\n",
            "Epoch 89799: train loss: 0.059717558324337006\n",
            "Epoch 89800: train loss: 0.059715889394283295\n",
            "Epoch 89801: train loss: 0.059714220464229584\n",
            "Epoch 89802: train loss: 0.05971255898475647\n",
            "Epoch 89803: train loss: 0.059710897505283356\n",
            "Epoch 89804: train loss: 0.05970924347639084\n",
            "Epoch 89805: train loss: 0.05970756709575653\n",
            "Epoch 89806: train loss: 0.05970590561628342\n",
            "Epoch 89807: train loss: 0.0597042478621006\n",
            "Epoch 89808: train loss: 0.05970257893204689\n",
            "Epoch 89809: train loss: 0.05970091000199318\n",
            "Epoch 89810: train loss: 0.059699248522520065\n",
            "Epoch 89811: train loss: 0.05969758704304695\n",
            "Epoch 89812: train loss: 0.05969591811299324\n",
            "Epoch 89813: train loss: 0.05969424918293953\n",
            "Epoch 89814: train loss: 0.05969259515404701\n",
            "Epoch 89815: train loss: 0.0596909262239933\n",
            "Epoch 89816: train loss: 0.05968926474452019\n",
            "Epoch 89817: train loss: 0.059687599539756775\n",
            "Epoch 89818: train loss: 0.05968593806028366\n",
            "Epoch 89819: train loss: 0.05968426167964935\n",
            "Epoch 89820: train loss: 0.059682607650756836\n",
            "Epoch 89821: train loss: 0.059680938720703125\n",
            "Epoch 89822: train loss: 0.05967927724123001\n",
            "Epoch 89823: train loss: 0.05967762693762779\n",
            "Epoch 89824: train loss: 0.05967596545815468\n",
            "Epoch 89825: train loss: 0.05967429652810097\n",
            "Epoch 89826: train loss: 0.05967263504862785\n",
            "Epoch 89827: train loss: 0.059670981019735336\n",
            "Epoch 89828: train loss: 0.059669315814971924\n",
            "Epoch 89829: train loss: 0.05966766178607941\n",
            "Epoch 89830: train loss: 0.05966600030660629\n",
            "Epoch 89831: train loss: 0.05966433882713318\n",
            "Epoch 89832: train loss: 0.05966268107295036\n",
            "Epoch 89833: train loss: 0.05966101959347725\n",
            "Epoch 89834: train loss: 0.059659358114004135\n",
            "Epoch 89835: train loss: 0.05965769663453102\n",
            "Epoch 89836: train loss: 0.05965603515505791\n",
            "Epoch 89837: train loss: 0.05965437367558479\n",
            "Epoch 89838: train loss: 0.05965270847082138\n",
            "Epoch 89839: train loss: 0.05965106189250946\n",
            "Epoch 89840: train loss: 0.05964939296245575\n",
            "Epoch 89841: train loss: 0.05964773893356323\n",
            "Epoch 89842: train loss: 0.05964607372879982\n",
            "Epoch 89843: train loss: 0.059644412249326706\n",
            "Epoch 89844: train loss: 0.059642765671014786\n",
            "Epoch 89845: train loss: 0.059641096740961075\n",
            "Epoch 89846: train loss: 0.05963943526148796\n",
            "Epoch 89847: train loss: 0.059637777507305145\n",
            "Epoch 89848: train loss: 0.05963612347841263\n",
            "Epoch 89849: train loss: 0.059634461998939514\n",
            "Epoch 89850: train loss: 0.0596328042447567\n",
            "Epoch 89851: train loss: 0.059631142765283585\n",
            "Epoch 89852: train loss: 0.05962948128581047\n",
            "Epoch 89853: train loss: 0.059627827256917953\n",
            "Epoch 89854: train loss: 0.05962616950273514\n",
            "Epoch 89855: train loss: 0.05962450057268143\n",
            "Epoch 89856: train loss: 0.05962284654378891\n",
            "Epoch 89857: train loss: 0.059621185064315796\n",
            "Epoch 89858: train loss: 0.05961952731013298\n",
            "Epoch 89859: train loss: 0.05961787328124046\n",
            "Epoch 89860: train loss: 0.05961621180176735\n",
            "Epoch 89861: train loss: 0.059614550322294235\n",
            "Epoch 89862: train loss: 0.05961288884282112\n",
            "Epoch 89863: train loss: 0.059611231088638306\n",
            "Epoch 89864: train loss: 0.05960956960916519\n",
            "Epoch 89865: train loss: 0.059607915580272675\n",
            "Epoch 89866: train loss: 0.05960625782608986\n",
            "Epoch 89867: train loss: 0.059604596346616745\n",
            "Epoch 89868: train loss: 0.05960294231772423\n",
            "Epoch 89869: train loss: 0.059601280838251114\n",
            "Epoch 89870: train loss: 0.059599619358778\n",
            "Epoch 89871: train loss: 0.059597961604595184\n",
            "Epoch 89872: train loss: 0.059596315026283264\n",
            "Epoch 89873: train loss: 0.05959464982151985\n",
            "Epoch 89874: train loss: 0.05959298089146614\n",
            "Epoch 89875: train loss: 0.05959133431315422\n",
            "Epoch 89876: train loss: 0.05958967283368111\n",
            "Epoch 89877: train loss: 0.05958801135420799\n",
            "Epoch 89878: train loss: 0.059586361050605774\n",
            "Epoch 89879: train loss: 0.05958469957113266\n",
            "Epoch 89880: train loss: 0.05958304554224014\n",
            "Epoch 89881: train loss: 0.05958138033747673\n",
            "Epoch 89882: train loss: 0.05957972630858421\n",
            "Epoch 89883: train loss: 0.0595780648291111\n",
            "Epoch 89884: train loss: 0.059576403349637985\n",
            "Epoch 89885: train loss: 0.059574760496616364\n",
            "Epoch 89886: train loss: 0.05957309901714325\n",
            "Epoch 89887: train loss: 0.05957143008708954\n",
            "Epoch 89888: train loss: 0.05956977978348732\n",
            "Epoch 89889: train loss: 0.059568118304014206\n",
            "Epoch 89890: train loss: 0.059566471725702286\n",
            "Epoch 89891: train loss: 0.05956481024622917\n",
            "Epoch 89892: train loss: 0.05956314131617546\n",
            "Epoch 89893: train loss: 0.05956149101257324\n",
            "Epoch 89894: train loss: 0.059559836983680725\n",
            "Epoch 89895: train loss: 0.05955817177891731\n",
            "Epoch 89896: train loss: 0.05955652520060539\n",
            "Epoch 89897: train loss: 0.05955486372113228\n",
            "Epoch 89898: train loss: 0.05955321341753006\n",
            "Epoch 89899: train loss: 0.05955155938863754\n",
            "Epoch 89900: train loss: 0.059549909085035324\n",
            "Epoch 89901: train loss: 0.05954824015498161\n",
            "Epoch 89902: train loss: 0.05954659357666969\n",
            "Epoch 89903: train loss: 0.059544939547777176\n",
            "Epoch 89904: train loss: 0.05954328179359436\n",
            "Epoch 89905: train loss: 0.05954162776470184\n",
            "Epoch 89906: train loss: 0.059539973735809326\n",
            "Epoch 89907: train loss: 0.05953832343220711\n",
            "Epoch 89908: train loss: 0.05953667685389519\n",
            "Epoch 89909: train loss: 0.05953501909971237\n",
            "Epoch 89910: train loss: 0.059533365070819855\n",
            "Epoch 89911: train loss: 0.05953170731663704\n",
            "Epoch 89912: train loss: 0.05953006073832512\n",
            "Epoch 89913: train loss: 0.0595284067094326\n",
            "Epoch 89914: train loss: 0.05952674150466919\n",
            "Epoch 89915: train loss: 0.059525102376937866\n",
            "Epoch 89916: train loss: 0.05952344462275505\n",
            "Epoch 89917: train loss: 0.05952180549502373\n",
            "Epoch 89918: train loss: 0.05952014401555061\n",
            "Epoch 89919: train loss: 0.0595184862613678\n",
            "Epoch 89920: train loss: 0.05951683968305588\n",
            "Epoch 89921: train loss: 0.05951518192887306\n",
            "Epoch 89922: train loss: 0.05951353535056114\n",
            "Epoch 89923: train loss: 0.059511881321668625\n",
            "Epoch 89924: train loss: 0.059510231018066406\n",
            "Epoch 89925: train loss: 0.05950858071446419\n",
            "Epoch 89926: train loss: 0.059506919234991074\n",
            "Epoch 89927: train loss: 0.05950527265667915\n",
            "Epoch 89928: train loss: 0.059503622353076935\n",
            "Epoch 89929: train loss: 0.05950196087360382\n",
            "Epoch 89930: train loss: 0.0595003142952919\n",
            "Epoch 89931: train loss: 0.05949866399168968\n",
            "Epoch 89932: train loss: 0.059497009962797165\n",
            "Epoch 89933: train loss: 0.05949535220861435\n",
            "Epoch 89934: train loss: 0.05949369817972183\n",
            "Epoch 89935: train loss: 0.05949205532670021\n",
            "Epoch 89936: train loss: 0.0594903938472271\n",
            "Epoch 89937: train loss: 0.05948875471949577\n",
            "Epoch 89938: train loss: 0.05948708951473236\n",
            "Epoch 89939: train loss: 0.05948545038700104\n",
            "Epoch 89940: train loss: 0.05948380008339882\n",
            "Epoch 89941: train loss: 0.0594821460545063\n",
            "Epoch 89942: train loss: 0.05948049575090408\n",
            "Epoch 89943: train loss: 0.05947884917259216\n",
            "Epoch 89944: train loss: 0.05947719141840935\n",
            "Epoch 89945: train loss: 0.059475529938936234\n",
            "Epoch 89946: train loss: 0.05947389081120491\n",
            "Epoch 89947: train loss: 0.05947224050760269\n",
            "Epoch 89948: train loss: 0.05947059392929077\n",
            "Epoch 89949: train loss: 0.059468936175107956\n",
            "Epoch 89950: train loss: 0.059467289596796036\n",
            "Epoch 89951: train loss: 0.05946563929319382\n",
            "Epoch 89952: train loss: 0.059463996440172195\n",
            "Epoch 89953: train loss: 0.05946234241127968\n",
            "Epoch 89954: train loss: 0.05946068838238716\n",
            "Epoch 89955: train loss: 0.05945904552936554\n",
            "Epoch 89956: train loss: 0.05945739895105362\n",
            "Epoch 89957: train loss: 0.05945573374629021\n",
            "Epoch 89958: train loss: 0.059454094618558884\n",
            "Epoch 89959: train loss: 0.059452444314956665\n",
            "Epoch 89960: train loss: 0.05945080518722534\n",
            "Epoch 89961: train loss: 0.059449147433042526\n",
            "Epoch 89962: train loss: 0.059447500854730606\n",
            "Epoch 89963: train loss: 0.05944584310054779\n",
            "Epoch 89964: train loss: 0.059444207698106766\n",
            "Epoch 89965: train loss: 0.05944255366921425\n",
            "Epoch 89966: train loss: 0.05944089964032173\n",
            "Epoch 89967: train loss: 0.05943925678730011\n",
            "Epoch 89968: train loss: 0.05943760275840759\n",
            "Epoch 89969: train loss: 0.059435952454805374\n",
            "Epoch 89970: train loss: 0.059434305876493454\n",
            "Epoch 89971: train loss: 0.059432655572891235\n",
            "Epoch 89972: train loss: 0.05943101644515991\n",
            "Epoch 89973: train loss: 0.059429366141557693\n",
            "Epoch 89974: train loss: 0.059427712112665176\n",
            "Epoch 89975: train loss: 0.05942606180906296\n",
            "Epoch 89976: train loss: 0.05942441523075104\n",
            "Epoch 89977: train loss: 0.059422772377729416\n",
            "Epoch 89978: train loss: 0.0594211183488369\n",
            "Epoch 89979: train loss: 0.05941947549581528\n",
            "Epoch 89980: train loss: 0.05941782146692276\n",
            "Epoch 89981: train loss: 0.059416186064481735\n",
            "Epoch 89982: train loss: 0.05941452831029892\n",
            "Epoch 89983: train loss: 0.0594128742814064\n",
            "Epoch 89984: train loss: 0.05941123887896538\n",
            "Epoch 89985: train loss: 0.05940959230065346\n",
            "Epoch 89986: train loss: 0.059407949447631836\n",
            "Epoch 89987: train loss: 0.05940629541873932\n",
            "Epoch 89988: train loss: 0.059404660016298294\n",
            "Epoch 89989: train loss: 0.05940300226211548\n",
            "Epoch 89990: train loss: 0.05940135568380356\n",
            "Epoch 89991: train loss: 0.05939971283078194\n",
            "Epoch 89992: train loss: 0.05939807370305061\n",
            "Epoch 89993: train loss: 0.05939643085002899\n",
            "Epoch 89994: train loss: 0.05939478054642677\n",
            "Epoch 89995: train loss: 0.05939313396811485\n",
            "Epoch 89996: train loss: 0.05939149111509323\n",
            "Epoch 89997: train loss: 0.05938984453678131\n",
            "Epoch 89998: train loss: 0.05938819423317909\n",
            "Epoch 89999: train loss: 0.05938655883073807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.eval() #set model to eval mode\n",
        "\n",
        "#train\n",
        "y_pred1 = model1(x_train) #predict\n",
        "y_pred1=(y_pred1>0.5).int().flatten() #argmax class lable\n",
        "train_acc =torch.sum(y_pred1 == Y_train.int())/ Y_train.shape[0]\n",
        "print(\"train ACC: \",train_acc.float())"
      ],
      "metadata": {
        "id": "CfNnwTuB4Lpz",
        "outputId": "6184f25d-445f-4782-9335-53d2c60f55d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train ACC:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "y_pred = model1(x_test) #predict\n",
        "y_pred=(y_pred>0.5).int().flatten() #argmax class lable\n",
        "test_acc = torch.sum(y_pred == Y_test.int()) / Y_test.shape[0]\n",
        "print(\"test ACC: \",test_acc.float())"
      ],
      "metadata": {
        "id": "mU_doCX64y_T",
        "outputId": "c7669ef6-d9f3-4f3d-b187-d6e2d55e9bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test ACC:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_test[:,0],x_test[:,1],c=y_pred)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "KM7EjPEu49Xm",
        "outputId": "66f361c5-5795-4249-8c13-c9d82411b4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7ed255baa050>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment_Basic_MLP_in_Pytorch.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}