{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8UVLjZF9diq"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg_yzvDy9dit"
      },
      "source": [
        "\n",
        "Using a pre-trained Vision Transformer Model\n",
        "===========================\n",
        "\n",
        "\n",
        "Vision Transformer models apply the cutting-edge attention-based\n",
        "transformer models, introduced in Natural Language Processing to achieve\n",
        "all kinds of the state of the art (SOTA) results, to Computer Vision\n",
        "tasks. Facebook Data-efficient Image Transformers `DeiT`\n",
        "\n",
        "https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification>\n",
        "\n",
        "is a Vision Transformer model trained on ImageNet for image\n",
        "classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyoBimta9div"
      },
      "source": [
        "What is DeiT\n",
        "---------------------\n",
        "\n",
        "Convolutional Neural Networks (CNNs) have been the main models for image\n",
        "classification since deep learning took off in 2012, but CNNs typically\n",
        "require hundreds of millions of images for training to achieve the\n",
        "SOTAresults. DeiT is a vision transformer model that requires a lot less\n",
        "data and computing resources for training to compete with the leading\n",
        "CNNs in performing image classification, which is made possible by two\n",
        "key components of of DeiT:\n",
        "\n",
        "-  Data augmentation that simulates training on a much larger dataset;\n",
        "-  Native distillation that allows the transformer network to learn from\n",
        "   a CNN’s output.\n",
        "\n",
        "DeiT shows that Transformers can be successfully applied to computer\n",
        "vision tasks, with limited access to data and resources. For more\n",
        "details on DeiT, see\n",
        "\n",
        "https://github.com/facebookresearch/deit\n",
        "\n",
        "and paper https://arxiv.org/abs/2012.12877\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPWET0ti9diw"
      },
      "source": [
        "Classifying Images with DeiT\n",
        "-------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PQE6N5Cn9dix",
        "outputId": "4e882566-57ee-49cc-ca39-1c574b054c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        " !pip install -q timm pandas requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now use DeiT to classify this image:\n",
        "<img src=\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\">"
      ],
      "metadata": {
        "id": "Tl1_tbjT_X3l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TdC6TGgB9diz",
        "outputId": "e955bbd6-0db2-496d-c056-f2480226be88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_deit_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timber wolf, grey wolf, gray wolf, Canis lupus\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import timm\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "\n",
        "print(torch.__version__)\n",
        "# should be 1.8.0\n",
        "\n",
        "\n",
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=3),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
        "])\n",
        "\n",
        "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
        "img = transform(img)[None,]\n",
        "out = model(img)\n",
        "clsidx = torch.argmax(out)\n",
        "\n",
        "import requests\n",
        "import ast\n",
        "\n",
        "url = \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/imagenet1000_clsidx_to_labels.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Safely evaluate the dictionary string into a Python dict\n",
        "imagenet_dict = ast.literal_eval(response.text)\n",
        "\n",
        "# Now you can access it like a normal dictionary\n",
        "print(imagenet_dict[clsidx.item()])  # Example output: 'tench, Tinca tinca'\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ1_CrMN9diz"
      },
      "source": [
        "The output should be 269, which, according to the ImageNet list of class\n",
        "index to\n",
        "\n",
        "(see https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n",
        "\n",
        "maps to ‘timber wolf, grey wolf, gray wolf, Canis lupus’.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-d5jhdz_JXM",
        "outputId": "8f2e85de-8b66-4e71-a938-99ba60fa50b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tench, Tinca tinca\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "vt_tutorial.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}